#+TITLE: Category Theory and Lambda Calculus
#+AUTHOR: Mario Román
#+OPTIONS: broken-links:mark toc:t tasks:nil num:3
#+SETUPFILE: ctlc.setup
#+LATEX_HEADER_EXTRA: %\input{titlepage}


** Abstract
:PROPERTIES:
:UNNUMBERED: t
:END:

This is the abstract.

** Acknowlegments                                                 :noexport:
# David Charte and Ignacio Cordón, testing the first versions of
# Mikrokosmos.

# Alejandro García, Adrián Ranea and David Charte, template usage.

This document has been written with Emacs26 and org-mode 9, using the
=org= file format and LaTeX as intermediate format. The document follows
the =classicthesis= [[http://www.latextemplates.com/templates/theses/2/thesis_2.pdf][template]] by André Miede. The =minted= package
has been used for code listings and the =tikzcd= package has been used
for commutative diagrams.
* Category theory (abstract)                                         :ignore:
* Category theory
** Categories
*** Definition of category
We will think of a category as the algebraic structure that captures
the notion of composition. A category will be built from some sort
of objects linked by composable arrows; to which some associativity and
identity laws will apply.

**** Categories, objects and morphisms                            :ignore:
#+attr_latex: :options [Category]
#+begin_definition
A *category* ${\cal C}$, as defined in cite:maclane78, is given by

 * ${\cal C}_0$, a collection[fn:collection] whose elements are called
   *objects*[fn:objectnotation], and
 * ${\cal C}_1$, a collection whose elements are called *morphisms*.

Every morphism $f \in {\cal C}_1$ has two objects assigned: a
*domain*, written as $\mathrm{dom}(f) \in {\cal C}_0$, and a
*codomain*, written as $\mathrm{cod}(f) \in {\cal C}_0$; a common
notation for such morphism is
\[
f \colon \mathrm{dom}(f) \to \mathrm{cod}(f).
\]

Given two morphisms $f \colon A \to B$ and $g \colon B \to C$ there
exists a *composition morphism*, written as $g \circ f \colon A \to C$.
Morphism composition is a binary associative operation with
identity elements $\id_{A}\colon A \to A$, that is
\[
h \circ (g \circ f) = (h \circ g) \circ f
\quad\text{ and }\quad
f \circ \id_A = f = \id_B \circ f.
\]
#+end_definition

[fn:objectnotation]: We will sometimes write this class of objects of
a category ${\cal C}$ as $\mathrm{obj}({\cal C})$, but it is common to simply use ${\cal C}$ to denote it.

[fn:collection]: We use the term /collection/ to denote
some unspecified formal notion of compilation of "things" that could
be given by sets or proper classes. We will want to define categories
whose objects are all the possible sets and we will need the objects
to form a proper class in order to avoid inconsistent results such as
the Russell's paradox.

**** Definition of hom-sets and small categories                  :ignore:
#+attr_latex: :options [Hom-sets]
#+begin_definition
The *hom-set* of two objects $A,B$ on a category is the collection of morphisms 
between them. It is written as $\hom(A,B)$. The set of *endomorphisms*
of an object $A$ is the hom-set $\mathrm{end}(A) = \hom(A,A)$.
#+end_definition

Sometimes, when considering a hom-set, it will be useful to explicitly
specify the category on which we are working as $\hom_{{\cal C}}(A,B)$.

#+attr_latex: :options [Small and locally small categories]
#+begin_definition
A category is said to be *small* if the collections ${\cal C}_0,{\cal C}_1$ of objects and morphisms
are both sets (instead of proper classes). It is said to be *locally small* if every
hom-set is actually a set.
#+end_definition

*** Morphisms
**** Isomorphisms                                                 :ignore:
#+attr_latex: :options [Isomorphisms]
#+begin_definition
A morphism $f : A \to B$ is an *isomorphism* if an inverse morphism $f^{-1} : B \to A$
such that

  * $f^{-1} \circ f = \id_{A}$,
  * $f \circ f^{-1} = \id_{B}$;

exists.
#+end_definition

We call *automorphisms* to the endomorphisms which are also isomorphisms.

#+attr_latex: :options [Unicity of inverses]
#+begin_proposition
<<prop-unicityinverse>>
If the inverse of a morphism exists, it is unique. In fact, if a
morphism has a left-side inverse and a right-side inverse, they are
both-side inverses and they are equal.
#+end_proposition
#+begin_proof
Given $f : A \to B$ with inverses $g_1,g_2 : B \to A$; we have that
\[
g_1 = g_1 \circ \id_A = g_1 \circ (f \circ g_2) = 
(g_1 \circ f) \circ g_2 =
\id \circ g_2 = g_2.
\]
We have used associativity of composition, neutrality of the identity 
and the fact that $g_1$ is a left-side inverse and $g_2$ is a 
right-side inverse.
#+end_proof

#+begin_definition
Two objects are *isomorphic* if an isomorphism between them exists.
We write $A \cong B$ when $A$ and $B$ are isomorphic.
#+end_definition

#+attr_latex: :options [Isomorphy is an equivalence relation]
#+begin_proposition
The relation of being isomorphic is an equivalence relation. In
particular,

 * the identity, $\id = \id^{-1}$;
 * the inverse of an isomorphism, $(f^{-1})^{-1} = f$;
 * and the composition of isomorphisms, $(f \circ g)^{-1} = g^{-1} \circ f^{-1}$;

are all isomorphisms.
#+end_proposition

**** Monomorphisms and epimorphisms                               :ignore:
#+attr_latex: :options [Monomorphisms and epimorphisms]
#+begin_definition
A *monomorphism* is a left-cancellable morphism, that is, $f : A \to B$ is
a monomorphism if, for every $g,h : B \to A$, 
\[ f \circ g = f \circ h  \implies g = h.
\]
An *epimorphism* is a right-cancellable morphism, that is, $f : A \to B$ is
an epimorphism if, for every $g,h : B \to A$,
\[ g \circ f = h \circ f \implies g = h.
\]
A morphism which is a monomorphism and an epimorphism at the same time is
called a *bimorphism*.
#+end_definition

#+begin_remark
A morphism can be a bimorphism without being an isomorphism.
#+end_remark

**** Retractions and sections                                     :ignore:
#+attr_latex: :options [Retractions and sections]
#+begin_definition
A *retraction* is a left inverse, that is, a morphism which has a right inverse;
conversely, a *section* is a right inverse, a morphism which has a left inverse.
#+end_definition

By virtue of Proposition [[prop-unicityinverse]], a morphism which is both a
retraction and a section is an isomorphism.

*** Terminal objects, products and coproducts
#+attr_latex: :options [Initial object]
#+begin_definition
An object $I$ is an *initial object* if every object is the domain of exactly
one morphism to it. That is, for every object $A$ exists an unique morphism $I \to A$.
#+end_definition

#+attr_latex: :options [Terminal object]
#+begin_definition
An object $T$ is a *terminal object* if every object is the codomain of exactly
one morphism from it. That is, for every object $A$ exists an unique $A \to T$.
#+end_definition

#+attr_latex: :options [Zero object]
#+begin_definition
A *zero object* is an object which is both initial and terminal at the
same time.
#+end_definition

#+attr_latex: :options [Initial and final objects are essentially unique]
#+begin_proposition
<<prop-initialfinalunique>>
Initial and final objects in a category are essentially unique; that
is, any two initial objects are isomorphic and any two final objects
are isomorphic.
#+end_proposition
#+begin_proof
If $A,B$ were initial objects, by definition, there would be only one
morphism $f : A \to B$ and only one morphism $g : B \to A$. Moreover, there
would be only an endomorphism in $\mathrm{End}(A)$ and $\mathrm{End}(B)$ which should be
the identity. That implies,

  * $f \circ g = \id$,
  * $g \circ f = \id$.

As a consequence, $A \cong B$. A similar proof can be written for the terminal
object.
#+end_proof

# The definition of product was given by MacLane on 1949. (Awodey on CTF2.0)

#+attr_latex: :options [Product object]
#+begin_definition
An object $C$ is the *product* of two objects $A,B$ on a category if there
are two morphisms
\[\begin{tikzcd}
A & C \rar{\pi_B}\lar[swap]{\pi_A} & B
\end{tikzcd}\]
such that, for any other object $D$ with two morphisms $f_1 : D \to A$ and
$f_2 : D \to B$, an unique morphism $h : D \to C$, such that $f_1 = \pi_A \circ h$
and $f_2 = \pi_B \circ h$. Diagramatically,
\[\begin{tikzcd}[column sep=tiny]
& D \dar[dashed]{\exists! h} \ar[bend left]{ddr}{f_2}\ar[bend right,swap]{ddl}{f_1} & \\
& C \drar{\pi_B}\dlar[swap]{\pi_A} & \\
A && B &.
\end{tikzcd}\]
#+end_definition

Note that the product of two objects does not have to exist on a category;
but when it exists, it is essentially unique. In fact, we will be able later
to construct a category in which the product object is the final object of
the category and Proposition [[prop-initialfinalunique]] can be applied. We will
write /the/ product object of $A,B$ as $A \times B$.


#+attr_latex: :options [Coproduct object]
#+begin_definition
An object $C$ is the *coproduct* of two objects $A,B$ on a category if there
are two morphisms
\[\begin{tikzcd}
A \rar{i_A} & C & B \lar[swap]{i_B}
\end{tikzcd}\]
such that, for any other object $D$ with two morphisms $f_1 : D \to A$ and
$f_2 : D \to B$, an unique morphism $h : D \to C$, such that $f_1 = i_A \circ h$
and $f_2 = i_B \circ h$. Diagramatically,
\[\begin{tikzcd}[column sep=tiny]
& D & \\
& C \uar[dashed]{\exists! h}  & \\
A \ar[bend left]{uur}{f_1}\urar{i_A} && B \ular[swap]{i_B} \ar[bend right,swap]{uul}{f_2} &.
\end{tikzcd}\]
#+end_definition

The same discussion we had earlier for the product can be rewritten here for
the coproduct only reversing the direction of the arrows. We will write /the/
coproduct of $A,B$ as $A \amalg B$. As we will see later, the notion of a coproduct
is dual to the notion of product; and the same proofs can be applied on
both cases, only by reversing the arrows.

*** Examples of categories
**** Discrete categories                                          :ignore:
#+attr_latex: :options [Discrete categories]
#+begin_exampleth
A category is *discrete* if it has no other morphisms than the identities.
A discrete category is uniquely defined by its class of objects and
every class of objects defines a category.
#+end_exampleth

**** Monoids, groups                                              :ignore:
#+attr_latex: :options [Monoids, groups]
#+begin_exampleth
A single-object category is a *monoid*. [fn:monoid] A monoid in which
every morphism is an isomorphism is a *group*. A *groupoid* is a
category (of any number of objects) where all the morphisms are
isomorphisms.
#+end_exampleth

[fn:monoid]: This definition is equivalent to the usual definition of monoid
if we take the morphisms as elements of the monoid and composition of morphisms
as the monoid operation.

**** Posets                                                       :ignore:
#+attr_latex: :options [Partially ordered sets]
#+begin_exampleth
Every partial ordering defines a category in which the elements are the
objects and an only morphism between two objects $\rho_{a,b} : a \to b$ exists 

In particular, every ordinal can be seen as a partially ordered set
and defines a category.
#+end_exampleth

**** Category of Sets                                             :ignore:
#+attr_latex: :options [The category of sets]
#+begin_exampleth
The category $\Sets$ is defined as the category with all the
possible sets as objects and functions between them as morphisms. It
is trivial to check associativity of composition and the existence of
the identity function for any set.
#+end_exampleth

**** Category of Groups                                           :ignore:
#+attr_latex: :options [The category of groups]
#+begin_exampleth
The category $\mathtt{Grp}$ is defined as the category with groups as objects
and group homomorphisms between them as morphisms.
#+end_exampleth

**** Category of R-modules                                        :ignore:
#+attr_latex: :options [The category of R-modules]
#+begin_exampleth
The category $R\text{-Mod}$ is defined as the category with $R\text{-modules}$ as
objects and module homomorphisms between them as morphisms. We know
that the composition of module homomorphisms and the identity are
also module homomorphisms.

In particular, abelian groups form a category as $\mathbb{Z}\text{-modules}$.
#+end_exampleth

**** Category of Topological spaces                               :ignore:
#+attr_latex: :options [The category of topological spaces]
#+begin_exampleth
The category $\mathtt{Top}$ is defined as the category with topologicaal spaces as
objects and continuous functions between them as morphisms.
#+end_exampleth

** Functors and natural transformations
#+begin_quote
"Category" has been defined in order to define "functor" and "functor"
has been defined in order to define "natural transformation".

      -- *Saunders MacLane*, /Categories for the working mathematician/.
#+end_quote

Functors and natural transformations were defined for the first time
by Eilenberg and MacLane in cite:maclane42 while studying Čech
cohomology. While initially they were devised mainly as a language for
studying homology, they have proven its foundational value with the
passage of time.

*** Functors
**** Definition of functor                                        :ignore:
#+attr_latex: :options [Functor]
#+begin_definition
A *functor* will be interpreted as a morphism of categories.
Given two categories ${\cal C}$ and ${\cal D}$, a functor between them $F : {\cal C} \to {\cal D}$ is given by

  * an *object function*, $F : \mathrm{obj}({\cal C}) \to \mathrm{obj}({\cal D})$;
  * and an *arrow function*, $F : (A \to B) \to (FA \to FB)$ for any two
    objects $A,B$ of the category;

such that

  * $F(\id_A) = \id_{FA}$, identities are preserved;
  * $F(f \circ g) = Ff \circ Fg$, the functor respects composition.
#+end_definition

**** Composition of functors                                      :ignore:
Functors can be composed as we did with morphisms. In fact, a category of
categories can be defined; having functors as morphisms.

#+attr_latex: :options [Composition of functors]
#+begin_definition
Given two functors $F \colon {\cal C} \to {\cal B}$ and $G \colon {\cal B} \to {\cal A}$, their composite functor
$G \circ F : {\cal C} \to {\cal A}$ is given by the composition of object and arrow functions
of the functors. This composition is trivially associative.
#+end_definition

#+attr_latex: :options [Identity functor]
#+begin_definition
The identity functor on a category $I_{{\cal C}}\colon {\cal C} \to {\cal C}$ is given by identity object
and arrow functions. It is trivially neutral with respect to composition.
#+end_definition

**** Full and faithfull functors                                  :ignore:
#+attr_latex: :options [Full functor]
#+begin_definition
A functor $F$ is *full* if the arrow map is surjective. That is, if every
$g : FA \to FB$ is of the form $Ff$ for some morphism $f \colon A \to B$.
#+end_definition
#+attr_latex: :options [Faithful functor]
#+begin_definition
A functor $F$ is *faithful* if the arrow map is injective. That is,
if, for every two arrows $f_1,f_2 : A \to B$, $Ff_1 = Ff_2$ implies
$f_1 = f_2$.
#+end_definition

It is easy to notice that the composition of faithful (respectively,
full) functors is again a faithful functor (respectively, full).

**** Isomorphisms of categories                                   :ignore:
#+attr_latex: :options [Isomorphism of categories]
#+begin_definition
An *isomorphism of categories* is a functor $T$ whose object and arrow functions
are bijections. Equivalently, it is a functor $T$ such that there exists an /inverse/
functor $S$ such that $T \circ S$ and $S \circ T$ are identity functors.
#+end_definition

# Subcategories
# Category of categories
*** Natural transformations
#+attr_latex: :options [Natural transformation]
#+begin_definition
A *natural transformation* between two functors with the same domain
and codomain, $\alpha\colon F \todot G$, is a family of morphisms parameterized by 
the objects of the domain category, $\alpha_C\colon FC \to GC$ such that the
following diagram commutes

\[\begin{tikzcd}
C \dar{f} & & SC \rar{\tau_C}\dar{Sf} & TC \dar{Tf} \\
C' & & SC' \rar{\tau_{C'}} & TC'
\end{tikzcd}\]

for every arrow $f : C \to C'$.
#+end_definition

It is also said that the family of morphisms is /natural/ in its
parameter. This naturality property is what allows us to translate a
commutative diagram from a functor to another.

\[\begin{tikzcd}
A\arrow{dd}{h}\drar{f} &   & & F A\arrow{dd}{F h}\drar{F f} \arrow{rrr}{\tau A} &     & & G A\arrow{dd}{}\drar{G f} &     \\
  & B \dlar{g} & &     & F B \dlar{F g} \arrow{rrr}{\tau B} & &     & G B \dlar{G g} \\
C &   & & F C \arrow{rrr}{\tau C} &     & & G C &     \\
\end{tikzcd}\]

#+attr_latex: :options [Natural isomorphism]
#+begin_definition
A *natural isomorphism* is a natural transformation in which every component,
every morphism of the parameterized family, is invertible.
#+end_definition

The inverses of a natural transformation form another natural
transformation, whose naturality follows from the naturality of the
original transformation.

**** TODO Equivalence of categories
*** Composition of natural transformations
**** Vertical composition                                         :ignore:
#+attr_latex: :options [Vertical composition of natural transformations]
#+begin_definition
The *vertical composition* of two natural transformations $\tau : S\to T$
and $\sigma : R \to S$, denoted by $\tau \cdot \sigma$ is the family of morphisms defined by the objectwise
composition of the components of the two natural transformations, i.e.

\[\begin{tikzcd}
Rc \rar{Rf}\dar{\sigma_c}\arrow[swap,bend right=90]{dd}{(\tau \circ \sigma)_c} &
Rc' \dar{\sigma_{c'}} \arrow[bend left=90]{dd}{(\tau \circ \sigma)_{c'}} \\
Sc \rar{Sf}\dar{\tau_c}  &
Sc' \dar{\tau_{c'}} \\
Tc \rar{Tf}  &  Tc' 
\end{tikzcd}\]
#+end_definition

#+attr_latex: :options [Vertical composition is a natural transformation]
#+begin_proposition
The vertical composition of two natural transformations is in fact
a natural transformation.
#+end_proposition
#+begin_proof
Naturality of the composition follows from the naturality of its two
factors. In other words, the commutativity of the external square on
the above diagram follows from the commutativity of the two internal
squares.
#+end_proof

**** Horizontal composition                                       :ignore:
#+attr_latex: :options [Horizontal composition of natural transformations]
#+begin_definition
The *horizontal composition* of two natural transformations $\tau \colon S \to T$ and
$\tau' \colon S' \to T'$, with domains and codomains as in the following diagram
\[\begin{tikzcd}
C 
\arrow[bend left=50]{r}[name=U,below]{}{S}
\arrow[bend right=50]{r}[name=D]{}[swap]{T}
& 
B \arrow[Rightarrow,from=U,to=D]{}{\tau}
\arrow[bend left=50]{r}[name=UU,below]{}{S'}
\arrow[bend right=50]{r}[name=DD]{}[swap]{T'}
&
C \arrow[Rightarrow,from=UU,to=DD]{}{\sigma}
\end{tikzcd}\]

is denoted by $\tau' \circ \tau \colon S'S \to T'T$ and is defined as the family of morphisms
given by $\tau' \circ \tau = T' \tau \circ \tau' = \tau' \circ S'\tau$, that is, by the diagonal of the 
following commutative square
\[\begin{tikzcd}
S'Sc\rar{\tau'_{Sc}} \drar{\scriptsize{(\tau' \circ \tau)_c}} \dar[swap]{S'\tau_c} & T'Sc \dar{T' \tau_c} \\
S'Tc\rar{\tau'_{Tc}} & T'Tc
\end{tikzcd}\]
#+end_definition

#+attr_latex: :options [Horizontal composition is a natural transformation]
#+begin_proposition
The horizontal composition of two natural transformations is in fact
a natural transformation.
#+end_proposition
#+begin_proof
It is natural as the following diagram is the composition of two
naturality squares
\[\begin{tikzcd}
S'Sc \rar{S'\tau} \dar{S'Sf} &
S'Tc \rar{\tau'}  \dar{S'Tf} &
T'Tc \dar{T'Tf} \\
S'Sb \rar{S'\tau} &
S'Tb \rar{\tau'} &
T'Tb
\end{tikzcd}\]

defined respectively by the naturality of $S'\tau$ and $\tau'$.
#+end_proof

** Constructions on categories
*** Opposite categories and contravariant functors
#+attr_latex: :options [Opposite category]
#+begin_definition
The *opposite category* ${\cal C}^{op}$ of a category ${\cal C}$ is a category with the
same objects as ${\cal C}$ but with all its arrows reversed. That is, for each
morphism $f : A \to B$, there exists a morphism $f^{op} : B \to A$ in ${\cal C}^{op}$.
Composition is defined as
\[
f^{op} \circ g^{op} = (g\circ f)^{op},
\]
exactly when the composite $g \circ f$ is defined in ${\cal C}$.
#+end_definition

Reversing all the arrows is a process that directly translates every
property of the category into a /dual/ property. A morphism $f$ is a
monomorphism if and only if $f^{op}$ is an epimorphism; a terminal object
in ${\cal C}$ is an initial object in ${\cal C}^{op}$ and a right inverse becomes a left
inverse on the opposite category. This process is also an /involution/,
where $(f^{op})^{op}$ can be seen as $f$ and $({\cal C}^{op})^{op}$ is trivially isomorphic to ${\cal C}$.

#+attr_latex: :options [Contravariant functor]
#+begin_definition
A *contravariant* functor from ${\cal C}$ to ${\cal D}$ is a functor from the opposite category,
that is, $F \colon {\cal C}^{op}\to {\cal D}$. Non-contravariant functors are often called *covariant*
functors, to emphasize the difference.
#+end_definition

# Hom functors as an example of covariant and contravariant functors
*** TODO The category of all categories
*** Product categories
#+attr_latex: :options [Product category]
#+begin_definition
The *product category* of two categories ${\cal C}$ and ${\cal D}$, denoted by ${\cal C} \times {\cal D}$ is a
category having

  * pairs $\left\langle c,d \right\rangle$ as objects, where $c \in {\cal C}$ and $d \in {\cal D}$;
  * and pairs $\pair{f,g} : \pair{c,d} \to \pair{c',d'}$ as morphisms, where $f \colon c \to c'$ and
    $g \colon d \to d'$ are morphisms in their respective categories.

The identity morphism of any object $\pair{c,d}$ is $\pair{\id_c, \id_d}$, and composition is
defined componentwise as
\[
(f',g') \circ (f,g) = (f' \circ f,g' \circ g).
\]
#+end_definition

We also define *projection functors* $P\colon {\cal C} \times {\cal D} \to {\cal C}$ and $Q : {\cal C} \times {\cal D} \to {\cal D}$
on arrows as $P\pair{f,g} = f$ and $Q\pair{f,g} = g$. Note that this definition of
product, using these projections, would be the product of two categories on a
category of categories with functors as morphisms.

#+attr_latex: :options [Product of functors]
#+begin_definition
The *product functor* of two functors $F\colon {\cal C} \to {\cal C}'$ and $G \colon {\cal D} \to {\cal D}'$ is a
functor $F \times G \colon {\cal C} \times {\cal D} \to {\cal C}' \times {\cal D}'$ which can be defined

  * on objects as $(F \times G)\pair{c,d} = \pair{Fc,Gd}$;
  * and on arrows as $(F \times G)\pair{f,g} = \pair{Ff,Gg}$.
#+end_definition

It can be seen as the unique functor making the following diagram
commute

\[\begin{tikzcd}
{\cal C} \dar{F} &
{\cal C} \times {\cal D}  \rar{Q}\lar[swap]{P} \dar[dashed]{F \times G}&
{\cal D} \dar{G} \\
{\cal C}' &
{\cal C}' \times {\cal D}' \rar[swap]{Q'}\lar{P'}&
{\cal D}'
\end{tikzcd}\]

In this sense, the $\times$ operation is itself a functor acting on objects
and morphisms of the $\mathtt{Cat}$ category of all categories.

*** TODO Bifunctors
# Definition of Bifunctors
# Bifunctors are defined by its two parts, obtained by fixing an element
# Proposition 1 on page 37 Maclane

*** TODO Comma categories
** Universality
*** Universal arrows
#+attr_latex: :options [Universal arrow]
#+begin_definition
A *universal arrow* from $c$ to $S$ is an arrow $u \colon c \to Sr$ such that
for every $c \to Sd$ exists a unique $r \to d$ making this diagram commute

\[\begin{tikzcd}
& Sd & d \\
c \rar[swap]{u}\urar{g} & Sr \uar[swap,dashed]{Sf} & r \uar[dashed]{\exists! f} &.
\end{tikzcd}\]
#+end_definition

#+attr_latex: :options [Universality in terms of hom-sets]
#+begin_proposition
The arrow $u \colon c \to Sr$ is universal iff $f \mapsto Sf \circ u$ is a bijection
$\hom(r,d) \cong \hom(c,Sd)$ natural in $d$. Any natural bijection of this
kind is determined by a unique universal arrow.
#+end_proposition
#+begin_proof
Bijection follows from the definition of universal arrow, and
naturality follows from $S(gf)\circ u = Sg \circ Sf \circ u$.

Given a bijection $\varphi$, we define $u = \varphi(\id_r)$.
By naturality we have the bijection $\varphi(f) = Sf \circ u$, every arrow
is written in this way.
#+end_proof

# TODO Universal from a functor to another

*** Representability
#+attr_latex: :options [Representation of a functor]
#+begin_definition
A *representation* of $K \colon D \to \Sets$ is a natural isomorphism
\[
\psi\colon \hom_{D}(r,-) \cong K.
\]

A functor is /representable/ if it has a representation. An object $r$ is
called a /representing object/. $D$ must have small hom-sets.
#+end_definition

#+attr_latex: :options [Representations in terms of universal arrows]
#+begin_proposition
If $u \colon \ast \to Kr$ is a universal arrow for a functor $K\colon D \to \Sets$, then
$f \mapsto K(f)(u\ast)$ is a representation. Every representation is obtained
in this way.
#+end_proposition
#+begin_proof
We know that $\hom(\ast, X) \xrightarrow{.} X$ is a natural isomorphism in $X$; in particular
$\hom(\ast, K-) \xrightarrow{.} K-$. Every representation is built then as

\[ \hom_{D}(r,-) \cong \hom(\ast,K-) \cong K, \]

for every natural isomorphism $D(r,-) \cong \Sets(\ast,K-)$. But every natural
isomorphism of this kind is a [[*Universal arrows as natural bijections][universal arrow]].
#+end_proof

*** Yoneda Lemma
#+attr_latex: :options [Yoneda Lemma]
#+begin_lemma
For any $K\colon D \to \Sets$ and $r \in D$, there is a bijection
\[
y \colon \mathrm{Nat}(\hom_{D}(r,-), K) \cong Kr
\]

sending the natural transformation $\alpha \colon \hom_{D}(r,-) \xrightarrow{.} K$
to the image of the identity, $\alpha_r1_r$.
#+end_lemma
#+begin_proof

#+end_proof

#+attr_latex: :options [Characterization of natural transformations between representable functors]
#+begin_corollary
Given $r,s \in D$, any natural transformation $\hom(r,-) \xrightarrow{.} \hom(s,-)$ has
the form $h_{\ast}$ for a unique $h\colon s \to r$.
#+end_corollary
#+begin_proof
Using Yoneda Lemma, we know that
\[
\mathrm{Nat}(\hom_D(r,-), \hom_D(s,-)) \cong \hom_D(s,r),
\]

sending the natural transformation to a morphism $\alpha(id_r) = h \colon s \to r$. The
rest of the natural transformation is determined as $h_{\ast}$ by naturality.
#+end_proof

#+attr_latex: :options [Addendum to the Yoneda Lemma]
#+begin_proposition
The bijection on the [[*Yoneda Lemma][Yoneda Lemma]] is a natural isomorphism between
two $\Sets^D \times D \to \Sets$ functors.
#+end_proposition
#+begin_proof
# TODO
#+end_proof

#+begin_definition
In the conditions of [[*Yoneda Lemma][Yoneda Lemma]], the *Yoneda functor*,
$Y \colon D^{op} \to \Sets^{D}$, is defined with the arrow function

\[
\left(f \colon s \to r\right) \mapsto 
\Big(D(f,-) \colon D(r,-) \to D(s,-)\Big).
\]
#+end_definition

#+begin_proposition
The Yoneda functor is full and faithful.
#+end_proposition
#+begin_proof
# TODO
#+end_proof

# The Yoneda functor is a currying of the hom functor.

*** TODO Products and limits
**** TODO Pullbacks
**** TODO Example: p-adic numbers
*** TODO Coproducts and colimits
**** TODO Coproducts as universal arrows
**** TODO Pushouts
**** Colimits
#+attr_latex: :options [Colimits]
#+begin_definition
A *colimit* of a functor from a given /index category/ $F \colon J \to {\cal C}$ is an object
$r \in {\cal C}$ such that there exists a universal arrow $u \colon F \todot \Delta r$ from $F$ to $\Delta$.
It is usually written as $r = \varinjlim F$.
#+end_definition

** Adjoints
*** Adjunctions
#+attr_latex: :options [Adjunction]
#+begin_definition
An *adjunction* from categories $X$ to $A$ is a pair of functors
$F\colon X \to A$, $G\colon A \to X$ with a natural bijection
\[
\varphi \colon \hom(Fx,a) \cong \hom(x,Ga),
\]

natural in both $x\in X$ and $a \in A$. We write it as $F \dashv G$.
#+end_definition

#+attr_latex: :options [Unit and counit of an adjunction]
#+begin_definition
An adjunction determines a *unit* and a *counit*;

 1) the *unit* is natural transformation made with universal arrows $\eta\colon I \todot GF$, where
    the right adjoint of each $f \colon Fx \to a$ is
    \[
    \varphi f = Gf \circ \eta_x \colon x \to Ga.
    \]
 2) the *counit* is natural transformation made with universal arrows $\varepsilon \colon FG \todot I$, where
    the left adjoint of each $g \colon x \to Ga$ is
    \[
    \varphi^{-1}g = \varepsilon \circ Fg \colon Fx \to a.
    \]

that follow the /triangle identities/ $\eta G \circ G \varepsilon = \id$ and $F\eta \circ \varepsilon F = \id$.
#+end_definition

#+attr_latex: :options [Characterization of adjunctions]
#+begin_proposition
Each adjunction is completely determined by any of

 1) functors $F,G$ and $\eta\colon 1 \todot GF$ where $\eta_x\colon x \to GFx$ is universal to $G$.
 2) functor $G$ and universals $\eta_x \colon x \to GF_0 x$, creating a functor $F$.
 3) functors $F,G$ and $\varepsilon\colon FG \todot 1$ where $\varepsilon_a\colon FGa \to a$ is universal from $F$.
 4) functor $F$ and universals $\varepsilon_a\colon FG_0a \to a$, creating a functor $G$.
 5) functors $F,G$, with units and counits satisfiying the triangle
    identities $\eta G \circ G \varepsilon = \id$ and $F\eta \circ \varepsilon F = \id$.
#+end_proposition
#+begin_proof
# TODO
#+end_proof

** Monads and algebras
*** Monads
**** Definition of Monads                                         :ignore:
#+attr_latex: :options [Monad]
#+begin_definition
A *monad* is a functor $T\colon X \to X$ with natural transformations

 * $\eta\colon I \todot T$, called /unit/
 * $\mu \colon T^2 \todot T$, called /multiplication/

such that
\[\begin{tikzcd}
T^3 \rar{T\mu}\dar{\mu T} & T^2\dar{\mu} \\
T^2 \rar{\mu} & T
\end{tikzcd}
\qquad
\begin{tikzcd}
IT \rar{\eta T}\drar[swap]{\cong} & T^2\dar{\mu} & \lar[swap]{T\eta}\dlar{\cong} TI \\
& T & &.
\end{tikzcd}\]
#+end_definition

**** Each adjunction gives rise to a monad                        :ignore:
#+attr_latex: :options [Each adjunction gives rise to a monad]
#+begin_proposition
Given $F \dashv G$, $GF$ is a monad.
#+end_proposition
#+begin_proof
We take the unit of the adjunction as the monad unit. We define the
product as $\mu = G\varepsilon F$. Associativity follows from these diagrams
\[\begin{tikzcd}
FGFG\rar{FG\varepsilon} \dar[swap]{\varepsilon FG} & FG \dar{\varepsilon} \\
FG\rar{\varepsilon} & I
\end{tikzcd}
\qquad
\begin{tikzcd}
GFGFGF\rar{GFG\varepsilon F} \dar[swap]{G\varepsilon FGF} & GFGF \dar{G\varepsilon F} \\
GFGF\rar{G\varepsilon F} & GF &,
\end{tikzcd}\]

where the first is commutative by the [[*Interchange law][interchange law]] and the second
is obtained by applying functors $G$ and $F$. Unit laws follow from
the [[*Unit and counit][triangular identities]] after applying $F$ and $G$.
#+end_proof

*** Comonads
#+attr_latex: :options [Comonad]
#+begin_definition
A *comonad* is a functor $L\colon X \to X$ with natural transformations

 * $\varepsilon\colon L\to I$, called /counit/
 * $\delta\colon L \to L^2$, called /comultiplication/

such that
\[\begin{tikzcd}
L\rar{\delta} \dar[swap]{\delta} & L^{2} \dar{L\delta} \\
L^{2}\rar{\delta L} & L^{3}
\end{tikzcd}
\qquad
\begin{tikzcd}
& L \dar{\delta} \dlar[swap]{\cong} \drar{\cong} & \\
IL & 
L^2 \lar{\varepsilon L}\rar[swap]{L \varepsilon} & 
LI
&.
\end{tikzcd}\]
#+end_definition

*** Algebras for a monad
#+attr_latex: :options [T-algebra]
#+begin_definition
For a monad $T$, a $T\textbf{-algebra}$ is an object $x$ with an arrow $h \colon Tx \to x$ called 
/structure map/ making these diagrams commute
\[\begin{tikzcd}
T^{2}x \rar{Th}\dar[swap]{\mu} & Tx \dar{h} \\
Tx\rar{h} & x &.
\end{tikzcd}
\]
#+end_definition

#+attr_latex: :options [Morphism of T-algebras]
#+begin_definition
A *morphism of T-algebras* is an arrow $f\colon x \to x'$ making the following square
commute
\[\begin{tikzcd}
Tx \dar[swap]{Tf}\rar{h} & Tx \dar{f} \\
Tx' \rar[swap]{h'} & Tx' &.
\end{tikzcd}\]
#+end_definition

#+attr_latex: :options [Category of T-algebras]
#+begin_proposition
The set of all $T\text{-algebras}$ and their morphisms form a category $X^{T}$.
#+end_proposition
#+begin_proof
Given $f\colon x \to x'$ and $g\colon x'\to x''$, $T\text{-algebra}$ morphisms, their composition
is also a $T\text{-algebra}$ morphism, due to the fact that this diagram

\[\begin{tikzcd}
Tx \rar{h}\dar[swap]{Tf} & 
x \dar{f}\\
Tx' \dar[swap]{Tg} \rar{h'} &
x' \dar{g}\\
Tx'' \rar{h''}&
x''
\end{tikzcd}\]

commutes.
#+end_proof
* Lambda calculus (abstract)                                         :ignore:
#+LATEX: \ctparttext{\color{black}\begin{center}
#+LATEX: \end{center}}
* Lambda calculus
** Untyped \lambda-calculus
The *\lambda-calculus* is a collection of formal systems, 
all of them based on the lambda notation discovered by Alonzo 
Church in the 1930s while trying to develop a foundational notion 
of function on mathematics.

The *untyped* or *pure lambda calculus* is, syntactically, the
simplest of those formal systems.
This presentation of the untyped lambda calculus will follow
cite:Hindley08 and cite:selinger13.

*** Definition
#+attr_latex: :options [Lambda terms]
#+begin_definition
The *\lambda-terms* are defined inductively as

  * every /variable/, taken from an infinite and numerable set ${\cal V}$ of
    variables, and usually written as lowercase single letters
    (x,y,z,...), is a \lambda-term.

  * given two \lambda-terms $M,N$; its /application/, $MN$ is a \lambda-term.

  * given a \lambda-term $M$ and a variable $x$, its /abstraction/, $\lambda x.M$
    is a lambda term.

They can be also defined by the following BNF
\[ \mathtt{Exp} ::= x \mid (\mathtt{Exp}\ \mathtt{Exp}) \mid (\lambda x.\mathtt{Exp})
\]
where $x \in {\cal V}$ is any variable.
#+end_definition

By convention, we omit outermost parentheses and assume
left-associativity, i.e., $MNP$ will mean $(MN)P$.
Multiple \lambda-abstractions can be also contracted to a single 
multivariate abstraction; thus $\lambda x.\lambda y.M$ can 
become $\lambda x,y.M$.

*** Free and bound variables, substitution
Any ocurrence of a variable $x$ inside the /scope/ of a lambda is said to be
bound; and any not bound variable is said to be free. We can define formally
the set of free variables as follows.

#+attr_latex: :options [Free variables]
#+begin_definition
<<def-freevariables>>
The *set of free variables* of a term $M$ is defined inductively as
\[\begin{aligned}
\freevars(x) &= \{x\}, \\
\freevars(MN) &= \freevars(M) \cup \freevars(N), \\
\freevars(\lambda x.M) &= \freevars(M) \setminus \{x\}.
\end{aligned}\]
#+end_definition

A free ocurrence of a variable can be substituted by a term. This should
be done avoiding the unintended bounding of free variables which happens
when a variable is substituted inside of the scope of a binder with the
same name, as in the following example, where we substitute $y$ by $(\lambda z.xz)$
on $(\lambda x.yx)$ and the second free variable $x$ gets bounded by the first binder
\[
(\lambda x.yx) \overset{y \mapsto (\lambda z.xz)}\longrightarrow (\lambda x.(\lambda z.xz)x).
\]

To avoid this, the $x$ should be renamed before the substitution.

#+attr_latex: :options [Substitution on lambda terms]
#+begin_definition
The *substitution* of a variable $x$ by a term $N$ on $M$ is defined
inductively as
\[\begin{aligned}
x[N/x] &\equiv N,\\
y[N/x] &\equiv y,\\
(MP)[N/x] &\equiv (M[N/x])(P[N/x]),\\
(\lambda x.P)[N/x] &\equiv \lambda x.P,\\
(\lambda y.P)[N/x] &\equiv \lambda y.P[N/x] & \text{ if } y \notin \freevars(N), \\
(\lambda y.P)[N/x] &\equiv \lambda z.P[z/y][N/x] & \text{ if } y \in \freevars(N);
\end{aligned}\]

where, in the last clause, $z$ is a fresh unused variable.
#+end_definition

We could define a criterion for choosing exactly what this new
variable should be, or simply accept that our definition will not be
well-defined, but well-defined up to a change on the name of the
variables. This equivalence relation will be defined formally on the
next section. In practice, it is common to follow the 
/Barendregt's variable convention/ which simply assumes that bound 
variables have been renamed to be distinct.

*** \alpha-equivalence
#+attr_latex: :options [\alpha-equivalence]
#+begin_definition
*\alpha-equivalence* is the smallest relation $=_{\alpha}$ on
\lambda-terms which is an equivalence relation, i.e.,

  * it is /reflexive/, $M =_{\alpha} M$;
  * it is /symmetric/, if $M =_{\alpha} N$, then $N =_{\alpha} M$;
  * and it is /transitive/, if $M=_{\alpha}N$ and $N=_{\alpha}P$, then $M=_{\alpha}P$;

and it is compatible with the structure of lambda terms,

  * if $M =_{\alpha} M'$ and $N =_{\alpha} N'$, then $MN =_{\alpha}M'N'$;
  * if $M=_{\alpha}M'$, then $\lambda x.M =_{\alpha} \lambda x.M'$;
  * if $y$ does not appear on $M$, $\lambda x.M =_{\alpha} \lambda y.M[y/x]$.
#+end_definition

\alpha-equivalence formally captures the fact that the name of a bound
variable can be changed without changing the properties of the term. This
idea appears recurrently on mathematics; e.g., the renaming of the variable of
integration is an example of \alpha-equivalence.
\[
\int_0^1 x^2\ dx = \int_0^1 y^2\ dy
\]

*** \beta-reduction
The core idea of evaluation in \lambda-calculus is captured by the notion
of \beta-reduction.

#+attr_latex: :options [Beta-reduction]
#+begin_definition
<<def-betared>>
The *single-step \beta-reduction* is the smallest relation on \lambda-terms
capturing the notion of evaluation 
\[(\lambda x.M)N \to_{\beta}M[N/x],\]

and some congruence rules that preserve the structure of
\lambda-terms, such as

  * $M \to_{\beta} M'$ implies $MN \to_{\beta} M'N$ and $NM \to_{\beta} NM'$;
  * $M \to_{\beta}M'$ implies $\lambda x.M \to_{\beta} \lambda x.M'$.

The reflexive transitive closure of $\to_{\beta}$ is written as $\twoheadrightarrow_{\beta}$. The symmetric
closure of $\twoheadrightarrow_{\beta}$ is called *\beta-equivalence* and written as $=_{\beta}$ or simply $=$.
#+end_definition

*** \eta-reduction
The idea of function extensionality in \lambda-calculus is captured by the
notion of \eta-reduction. Function extensionality implies the equality of
any two terms that define the same function over any argument.

#+attr_latex: :options [Eta reduction]
#+begin_definition
The \eta-reduction is the smallest relation on \lambda-terms satisfiying the
same congruence rules as \beta-reduction and the following axiom
\[
\lambda x.Mx \to_{\eta} M,\text{ for any } x \notin \mathrm{FV}(M).
\]
#+end_definition

We define single-step \beta\eta-reduction as the union of \beta-reduction
and \eta-reduction. This will be written as $\to_{\beta\eta}$, and its reflexive transitive
closure will be $\tto_{\beta\eta}$.

*** Confluence
#+begin_definition
A relation $\to$ is *confluent* if, given its reflexive transitive closure
$\tto$, $M \tto N$ and $M \tto P$ imply the existence of some $Z$ such that
$N \tto Z$ and $P \tto Z$.
#+end_definition

Given any binary relation $\to$ of which $\tto$ is its reflexive transitive
closure, we can consider three seemingly related properties

  * the *confluence* or Church-Rosser property we have just defined.
  * the *quasidiamond property*, which assumes $M \to N$ and $M \to P$.
  * the *diamond property*, which is defined substituting $\tto$ by $\to$ on
    the definition on confluence.

Diagrammatically, the three properties can be represented as

\[\begin{tikzcd}[column sep=small]
& 
M \drar[two heads]\dlar[two heads] &&& 
M \drar\dlar &&& 
M \drar\dlar &\\
N \drar[dashed,two heads] && 
P \dlar[dashed,two heads] & 
N \drar[dashed,two heads] &&
P \dlar[dashed,two heads] &
N \drar[dashed] && 
P \dlar[dashed] \\& 
Z &&&
Z &&&
Z &\\
\end{tikzcd}\]

and the implication relation between them is that the diamond relation
implies confluence; while the quasidiamond does not. Both claims are
easy to prove, and they show us that, in order to prove confluence for
a given relation, we need to prove the diamond property instead of try
to prove it from the quasidiamond property, as a naive attempt of proof
would try.

The statement of $\tto_{\beta}$ and $\tto_{\beta\eta}$ being confluent is what we are going to
call the Church-Rosser theorem. The definition of a relation satisfying
the diamond property and whose reflexive transitive closure is $\tto_{\beta\eta}$ will
be the core of our proof.

*** The Church-Rosser theorem
The proof presented here is due to Tait and Per Martin-Löf; an earlier
but more convoluted proof was discovered by Alonzo Church and Barkley 
Rosser in 1935. It is based on the idea of parallel one-step reduction.

#+attr_latex: :options [Parallel one-step reduction]
#+begin_definition
We define the *parallel one-step reduction* relation, $\rhd$ as the smallest
relation satisfying that, assuming $P \rhd P'$ and $N \rhd N'$, the following
properties of

  * reflexivity, $x \rhd x$;
  * parallel application, $PN \rhd P'N'$;
  * congruence to \lambda-abstraction, $\lambda x.N \rhd \lambda x.N'$;
  * parallel substitution, $(\lambda x.P)N \rhd P'[N'/x]$;
  * and extensionality, $\lambda x.P x \rhd P'$, if $x \not\in \mathrm{FV}(P)$,

hold.
#+end_definition

Using the first three rules, it is trivial to show that this relation
is in fact reflexive.

#+begin_lemma
<<lemma-transclosureparallel>>
The reflexive transitive closure of $\rhd$ is $\tto_{\beta\eta}$.
In particular, given any $M,M'$,

  1) if $M \to_{\beta\eta} M'$, then $M \rhd M'$.
  2) if $M \rhd M'$, then $M \tto_{\beta\eta} M'$;
#+end_lemma
#+begin_proof
  1) We can prove this by exhaustion and structural induction on
     \lambda-terms, the possible ways in which we arrive at $M \to M'$
     are

     * $(\lambda x.M)N \to M[N/x]$; where we know that, by parallel substitution
       and reflexivity $(\lambda x.M)N \rhd M[N/x]$.

     * $MN \to M'N$ and $NM \to NM'$; where we know that, by
       induction $M \rhd M'$, and by parallel application and reflexivity, $MN \rhd M'N$
       and $NM \rhd NM'$.

     * congruence to \lambda-abstraction, which is a shared property between
       the two relations where we can apply structural induction again.

     * $\lambda x. Px \to P$, where $x \not\in \mathrm{FV}(P)$ and we can apply extensionality for $\rhd$
       and reflexivity.

  2) We can prove this by induction on any derivation of $M \rhd M'$. The
     possible ways in which we arrive at this are
     
     * the trivial one, reflexivity.

     * parallel application $NP \rhd N'P'$, where, by induction, we have $P \tto P'$ 
       and $N \tto N'$. Using two steps, $NP \tto N'P \tto N'P'$ we prove $NP \tto N'P'$.

     * congruence to \lambda-abstraction $\lambda x.N \rhd \lambda x.N'$, where, by induction,
       we know that $N \tto N'$, so $\lambda x.N \tto \lambda x.N'$.

     * parallel substitution, $(\lambda x.P)N \rhd P'[N'/x]$, where, by induction,
       we know that $P \tto P'$ and $N\tto N'$. Using multiple steps,
       $(\lambda x.P)N \tto (\lambda x.P')N \tto (\lambda x.P')N' \to P'[N'/x]$.

     * extensionality, $\lambda x.P x \rhd P'$, where by induction $P \tto P'$, and trivially,
       $\lambda x.Px \tto \lambda x.P'x$.

Because of this, the reflexive transitive closure of $\rhd$ should be a subset and a
superset of $\tto$ at the same time.
#+end_proof

#+attr_latex: :options [Substitution Lemma]
#+begin_lemma
<<lemma-subsl>>
Assuming $M \rhd M'$ and $U \rhd U'$, $M[U/y] \rhd M'[U'/y]$.
#+end_lemma
#+begin_proof
We apply structural induction on derivations of $M \rhd M'$, depending
on what the last rule we used to derive it was.

  * Reflexivity, $M = x$. If $x=y$, we simply use $U \rhd U'$; if $x \neq y$,
    we use reflexivity on $x$ to get $x \rhd x$.

  * Parallel application. By induction hypothesis, $P[U/y] \rhd P'[U'/y]$ and
    $N[U/y]\rhd N'[U'/y]$, hence $(PN)[U/y] \rhd (P'N')[U'/y]$.

  * Congruence. By induction, $N[U/y] \rhd N'[U'/y]$ and $\lambda x.N[U/y] \rhd \lambda x.N'[U'/y]$.

  * Parallel substitution. By induction, $P[U/y] \rhd P'[U'/y]$ and $N[U/y] \rhd N[U'/y]$,
    hence $((\lambda x.P)N)[U/y] \rhd P'[U'/y][N'[U'/y]/x] = P'[N'/x][U'/y]$.

  * Extensionality, given $x \notin \mathrm{FV}(P)$. By induction, $P \rhd P'$, hence
    $\lambda x.P[U/y]x \rhd P'[U'/y]$.

Note that we are implicitely assuming the Barendregt's variable convention; all
variables have been renamed to avoid clashes.
#+end_proof

#+attr_latex: :options [Maximal parallel one-step reduct]
#+begin_definition 
The *maximal parallel one-step reduct* $M^{\ast}$ of a \lambda-term $M$ is defined
inductively as

  * $x^{\ast} = x$;
  * $(PN)^{\ast} = P^{\ast}N^{\ast}$;
  * $((\lambda x.P)N)^{\ast} = P^{\ast}[N^{\ast}/x]$;
  * $(\lambda x.N)^{\ast} = \lambda x.N^{\ast}$;
  * $(\lambda x.Px)^{\ast} = P^{\ast}$, given $x \notin \mathrm{FV}(P)$.
#+end_definition

#+attr_latex: :options [Diamond property of parallel reduction]
#+begin_lemma
<<lemma-paralleldiamond>>
Given any $M'$ such that $M \rhd M'$, $M' \rhd M^{\ast}$. Parallel one-step reduction 
has the diamond property.
#+end_lemma
#+begin_proof
We apply again structural induction on the derivation of $M \rhd M'$.

  * Reflexivity gives us $M' = x = M^{\ast}$.

  * Parallel application. By induction, we have $P \rhd P^\ast$ and $N \rhd N^{\ast}$; depending
    on the form of $P$, we have

    - $P$ is not a \lambda-abstraction and $P'N' \rhd P^{\ast}N^{\ast} = (PN)^{\ast}$.

    - $P = \lambda x.Q$ and $P \rhd P'$ could be derived using congruence to \lambda-abstraction
      or extensionality. On the first case we know by induction hypothesis that $Q'\rhd Q^{\ast}$
      and $(\lambda x.Q')N' \rhd Q^{\ast}[N^{\ast}/x]$. On the second case, we can take $P = \lambda x.Rx$, where,
      $R \rhd R'$. By induction, $(R'x) \rhd (Rx)^{\ast}$ and now we apply the substitution lemma
      to have $R'N' = (R'x)[N'/x] \rhd (Rx)^{\ast}[N^{\ast}/x]$.

  * Congruence. Given $N \rhd N'$; by induction $N' \rhd N^{\ast}$, and depending on the form of
    $N$ we have two cases

    - $N$ is not of the form $Px$ where $x \not\in \mathrm{FV}(P)$; we can apply congruence to 
      \lambda-abstraction.

    - $N = Px$ where $x \notin \mathrm{FV}(P)$; and $N \rhd N'$ could be derived by parallel application
      or parallel substitution. On the first case, given $P \rhd P'$, we know that $P' \rhd P^{\ast}$
      by induction hypothesis and $\lambda x.P'x \rhd P^{\ast}$ by extensionality. On the second case,
      $N = (\lambda y.Q)x$ and $N' = Q'[x/y]$, where $Q \rhd Q'$. Hence $P \rhd \lambda y.Q'$, and by
      induction hypothesis, $\lambda y.Q' \rhd P^{\ast}$.

  * Parallel substitution, with $N \rhd N'$ and $Q \rhd Q'$; we know that $M^{\ast} = Q^{\ast}[N^{\ast}/x]$
    and we can apply the substitution lemma (lemma [[lemma-subsl]]) to get $M' \rhd M^{\ast}$.

  * Extensionality. We know that $P \rhd P'$ and $x \notin \mathrm{FV}(P)$, so by induction hypothesis
    we know that $P' \rhd P^{\ast} = M^{\ast}$.
#+end_proof

#+attr_latex: :options [Church-Rosser Theorem]
#+begin_theorem
<<theorem-churchrosser>>
The relation $\tto_{\beta\eta}$ is confluent.
#+end_theorem
#+begin_proof
Parallel reduction, $\rhd$, satisfies the diamond property (lemma [[lemma-paralleldiamond]]), 
which implies the Church-Rosser property. Its reflexive transitive closure is $\tto_{\beta\eta}$
(lemma [[lemma-transclosureparallel]]),
whose diamond property implies confluence for $\to_{\beta\eta}$.
#+end_proof

*** Normalization
#+attr_latex: :options [Normal forms]
#+begin_definition
A \lambda-term is said to be in *\beta-normal form* if \beta-reduction
cannot be applied to it or any of its subformulas. We define *\eta-reduction*
and *\beta\eta-reduction* analogously.
#+end_definition

Computing with \lambda-terms means to apply reductions to them until
a normal form is reached. We know, by virtue of Theorem [[theorem-churchrosser]], 
that, if a normal form exists, it is unique; but we do not know if a
normal form actually exists. We say that a term *has* a normal form
if it can be reduced to a normal form. 

#+begin_definition
A term is *weakly normalizing* if there exists a sequence of reductions
from it to a normal form. It is *strongly* normalizing if every sequence
of reductions is finite.
#+end_definition

A consequence of Theorem [[theorem-churchrosser]] is that a term is weakly
normalizing if and only if it has a normal form. Strong normalization
implies also weak normalization, but the converse is not true; as an
example, the term
\[
\Omega = (\lambda x.(x x))(\lambda x.(x x))
\]
is neither weakly nor strongly normalizing; and the term
\[
(\lambda x.\lambda y.y)\ \Omega\ (\lambda x.x) \longrightarrow_{\beta} (\lambda x.x)
\]
is weakly normalizing but not strongly normalizing. Its normal form
is
\[
(\lambda x.\lambda y.y)\ \Omega\ (\lambda x.x) \longrightarrow_{\beta} (\lambda x.x).
\]

*** Standarization and evaluation strategies
# Barendregt, 1985, section 13.2
# Leftmost vs Rightmost evaluation
# Leftmost does always normalize if it is possible
# Rightmost only normalizes if it is necessary

# https://cs.stackexchange.com/questions/7702/applicative-order-and-normal-order-in-lambda-calculus
# This case illustrates a more general phenomenon: applicative order
# reduction only ever finds a normal form if the term is strongly
# normalizing, whereas normal order reduction always finds the normal
# form if there is one. This happens because applicative order always
# evaluates fully arguments first, and so misses the opportunity for
# an argument to turn out to be unused; whereas normal order evaluates
# arguments as late as possible, and so always wins if the argument
# turns out to be unused.

# Statement: http://www.nyu.edu/projects/barker/Lambda/barendregt.94.pdf
# Barendregt (1984) Theorem 13.2.2
We would like to find a \beta-reduction strategy such that, if a term
has a normal form, it can be found by following this strategy. Our
basic result will be the *standarization theorem*, which shows that,
if a \beta-reduction to a normal form exists, a sequence of
\beta-reductions from left to right on the \lambda-expression will be
able to find it. From this result, we will be able to prove that the
reduction strategy that always reduces the leftmost \beta-abstraction
will always find a normal form if it exists.

This section follows cite:kashima00, cite:barendsen94 and cite:barendregt84.

#+attr_latex: :options [Leftmost one-step reduction]
#+begin_definition
We define the relation $M \to_{n} N$ when $N$ can be obtained by \beta-reducing
the $n\text{-th}$ leftmost \beta-reducible application of the expression.
We call $\to_{1}$ the *leftmost one-step reduction* and we write it as $\to_{l}$;
$\tto_{l}$ is its reflexive transitive closure.
#+end_definition

#+attr_latex: :options [Standard sequence]
#+begin_definition
A sequence of \beta-reductions $M_0 \to_{n_1} M_1 \to_{n_2} M_2 \to_{n_3} \dots \to_{n_k} M_{k}$ 
is *standard* if $\{n_i\}$ is a non-decreasing sequence.
#+end_definition

We will prove that every term that can be reduced to a normal form can
be reduced to it using a standard sequence, from this theorem, the existence
of an optimal beta reduction strategy, in the sense that it will always find
the normal form if it exists, will follow as a corollary.

#+attr_latex: :options [Standarization theorem]
#+begin_theorem
If $M \tto_{\beta} N$, there exists a standard sequence from $M$ to $N$.
#+end_theorem
#+begin_proof
We start by defining the following two binary relations. The first one
is the relation given by the head reduction of the application and it
is defined by

  * $A \tto_h A$, reflexivity.
  * $(\lambda x.A_0)A_1 \dots \tto_{h} A_0[A_1/x]\dots$, head \beta-reduction.
  * $A \tto_{h} B \tto_{h} C$ implies $A \tto_{h} C$, transitivity.

The second one is the standard reduction and it is defined by

  * $M \tto_h x$ implies $M \tto_s x$, where $x$ is a variable.
  * if $M \tto_h AB$, $A \tto_s C$ and $B \tto_s D$, then $M \tto_s CD$.
  * if $M \tto_h \lambda x.A$ and $A \tto_s B$, then $M \to_s \lambda x.B$.

We can check the following trivial properties by structural induction

  1) $\tto_h$ implies $\tto_{l}$.
  2) $\tto_{s}$ implies the existence of a standard \beta-reduction.
  3) $\tto_{s}$ is reflexive, by induction on the structure of a term.
  4) if $M \tto_{h} N$, then $MP \tto_{h} NP$.
  5) if $M \tto_h N \tto_s P$, then $M \tto_{s} P$.
  6) if $M \tto_h N$, then $M[P/x] \tto_h N[P/x]$.
  7) if $M \tto_s N$ and $P \tto_s Q$, then $M[P/z] \tto_{s} N[Q/z]$.

Now we can prove that $K \tto_{s} (\lambda x.M)N$ implies $K \tto_s M[N/x]$.
From the fact that $K \tto_s (\lambda x.M)$, we know that there must exist $P$ and $Q$ such
that $K \tto_h \lambda PQ$, $P \tto_s \lambda x.M$ and $Q \tto_s N$; and from $P \tto_s \lambda x.M$, we know
that there exists $W$ such that $P \tto_h \lambda x.W$ and $W \tto_s M$. From all this information,
we can conclude that
\[
K \tto_h PQ \tto_{h} (\lambda x.W)Q \tto W[Q/x] \tto_s M[N/x];
\]
which, by (3.), implies $K \tto_s M[N/x]$.

We finally prove that, if $K \tto_s M \to_{\beta} N$, then $K \tto_s N$. This proves the theorem,
as every \beta-reduction $M \tto_s M \tto_\beta N$ implies $M \tto_s N$. We analize the possible
ways in which $M \to_{\beta} N$ can be derived.

  1) If $K \tto_{s} (\lambda x.M)N \to_{\beta} M[N/x]$, it has been
     already showed that $K \tto_s M[N/x]$.
  2) If $K \tto_s MN \to_{\beta} M'N$ with $M \to_{\beta} M'$, we know that there exist $K \tto_h WQ$ 
     such that $W \tto_s M$ and $Q \tto_s N$; by induction $W \tto_s M'$, and then $WQ \tto_s M'N$.
     The case $K \tto_s MN \to_{\beta} MN'$ is entirely analogous.
  3) If $K \tto_s \lambda x.M \to_{\beta} \lambda x.M'$, with $M \to_{\beta} M'$, we know that there exists $W$ such
     that $K \tto_h \lambda x.W$ and $W \tto_s M$. By induction $W \tto_s M'$, and $K \tto_s \lambda x.M'$.
#+end_proof

#+attr_latex: :options [Leftmost reduction theorem]
#+begin_corollary
<<cor-leftmosttheorem>>
We define the *leftmost reduction strategy* as the strategy that
reduces the leftmost \beta-reducible application at each step. If $M$ has a
normal form, the leftmost reduction strategy will lead to it.
#+end_corollary
#+begin_proof
Note that, if $M \to_n N$, where $N$ is in \beta-normal form; $n$ must be exactly
$1$. If $M$ has a normal form and $M \tto_{\beta} N$, there must exist a standard sequence
from $M$ to $N$ whose last step is of the form $\to_{l}$; as the sequence is non-decreasing,
every step has to be of the form $\to_{l}$.
#+end_proof

*** SKI combinators
**** TODO Combinatorial algebras
**** Transformation of SKI combinators :ignore:
#+attr_latex: :options [Lambda transform]
#+begin_definition
The *\Lambda-transform* of a Ski-term is a \lambda-term defined
recursively as

  * $\Lambda(x) = x$, for any variable $x$;
  * $\Lambda(I) = (\lambda x.x)$;
  * $\Lambda(K) = (\lambda x.\lambda y.x)$;
  * $\Lambda(S) = (\lambda x.\lambda y.\lambda z.xz(yz))$;
  * $\Lambda(XY) = \Lambda(X)\Lambda(Y)$.
#+end_definition

#+attr_latex: :options [Bracket abstraction]
#+begin_definition
The *bracket abstraction* of the Ski-term $U$ on the variable $x$ is
written as $[x].U$ and defined recursively as

  * $[x].x = I$;
  * $[x].M = KM$, if $x \notin \freevars(M)$;
  * $[x].Ux = U$, if $x \notin \freevars(U)$;
  * $[x].UV = S([x].U)([x].V)$, otherwise.

where $\freevars$ is the set of free variables; as defined on Definition
[[def-freevariables]].
#+end_definition

#+attr_latex: :options [Ski abstraction]
#+begin_definition
The *SKI abstraction* of a \lambda-term $M$, written as $\skiabs(M)$ is
defined recursively as

  * $\skiabs(x) = x$, for any variable $x$;
  * $\skiabs(MN) = \skiabs(M)\skiabs(N)$;
  * $\skiabs(\lambda x.M) = [x].\skiabs(M)$;

where $[x].U$ is the bracket abstraction of the Ski-term $U$.
#+end_definition

#+attr_latex: :options [Ski combinators and lambda terms]
#+begin_theorem
The Ski-abstraction is a retraction of the \Lambda-transform of the term,
that is, for any Ski-term $U$,

\[
\skiabs(\Lambda(U)) = U.
\]
#+end_theorem
#+begin_proof
By induction on $U$,

  * $\skiabs\Lambda(x) = x$, for any variable $x$;
  * $\skiabs\Lambda(I) = [x].x = I$;
  * $\skiabs\Lambda(K) = [x].[y].x = [x].Kx = K$;
  * $\skiabs\Lambda(S) = [x].[y].[z].xz(yz) = [x].[y].Sxy = S$; and
  * $\skiabs\Lambda(MN) = MN$.
#+end_proof

*** TODO Turing completeness
** Simply typed \lambda-calculus
We will give now a presentation of the *simply-typed \lambda-calculus*
(STLC) based on cite:Hindley08. Our presentation will rely only on the
/arrow type/ $\to$; while other presentations of simply typed
\lambda-calculus extend this definition with type constructors such as
pairs or union types, as it is done in cite:selinger13.

It seems clearer to present a first minimal version of the
\lambda-calculus. Such extensions will be explained later, profiting
from the logical interpretation of propositions as types.

*** Simple types
We start assuming that a set of *basic types* exists. Those basic
types would correspond, in a programming language interpretation, with
things like the type of strings or the type of integers.

#+attr_latex: :options [Simple types]
#+begin_definition
The set of *simple types* is given by the following Backus-Naur form
\[\mathtt{Type} ::= 
\iota \mid 
\mathtt{Type} \to \mathtt{Type} \]
where $\iota$ would be any /basic type/.
#+end_definition

That is to say that, for every two types $A,B$, there exists a *function type*
$A \to B$ between them.

*** Typing rules for the simply typed \lambda-calculus
We will now define the terms of the simply typed \lambda-calculus (STLC) using the
same constructors we used on the untyped version. Those are the *raw typed \lambda-terms*.

#+attr_latex: :options [Raw typed lambda terms]
#+begin_definition
The set of *typed lambda terms* is given by the BNF
\[ \mathtt{Term} ::=
x \mid
\mathtt{Term}\mathtt{Term} \mid
\lambda x^{\mathtt{Type}}. \mathtt{Term} \mid
\]
#+end_definition

The set of raw typed \lambda-terms contains some meaningless terms
under our type interpretation, such as $\pi_1(\lambda x^A.M)$. *Typing rules*
will give them the desired semantics; only a subset of these raw
lambda terms will be typeable.

#+attr_latex: :options [Typing context]
#+begin_definition
A *typing context* is a sequence of typing assumptions
$x_1:A_1,\dots,x_n:A_n$, where no variable appears more than once.
#+end_definition

Every typing rule assumes a typing context, usually denoted by $\Gamma$ 
or by a concatenation of typing contexts written as $\Gamma,\Gamma'$; and 
a consequence from that context, separated by the $\vdash$ symbol.

 1) The $(var)$ rule simply makes explicit the type of a variable from
    the context.

    \begin{prooftree}
    \LeftLabel{($var$)}
    \AXC{}
    \UIC{$\Gamma, x:A \vdash x:A$}
    \end{prooftree}

 2) The $(abs)$ gives the type of a \lambda-abstraction as the type of
    functions from the variable type to the result type. It acts as
    a constructor of function terms.

   \begin{prooftree}
   \LeftLabel{$(abs)$}
   \AXC{$\Gamma, x:A \vdash M : B$}
   \UIC{$\Gamma \vdash \lambda x.M : A \to B$}
   \end{prooftree}

 3) The $(app)$ rule gives the type of a well-typed application of a
    lambda term. A term $f : A \to B$ applied to a term $a : A$ is a term
    of type $B$. It acts as a destructor of function terms.

    \begin{prooftree}
    \LeftLabel{$(app)$}
    \AXC{$\Gamma \vdash f : A \to B$}
    \AXC{$\Gamma \vdash a : A$}
    \BIC{$\Gamma \vdash f a : B$}
    \end{prooftree}

#+begin_definition
A term is *typeable* if we can assign types to all its variables in
such a way that a typing judgment for the type is derivable.
#+end_definition

From now on, we only consider typeable terms on the STLC to be used as
real terms. As a consequence, the set of \lambda-terms of the STLC is
only a subset of the terms of the untyped \lambda-calculus.

# Examples of typeable and non-typeable terms.

# It is easy to check if a term is typeable because there is only
# one way to type it.

# If we want to derive term bottom-up, there is only one possible
# choice at each step. Has this to do with the natural deduction
# properties?

*** Curry-style types
Two different approaches to typing \lambda-terms are commonly used.

 * *Church-style* typing, also known as /explicit typing/ originated from
   the work of Alonzo Church in cite:church40, where he described a STLC
   with two basic types. The term's type is defined as an intrinsic property
   of the term; and the same term has to be interpreted always with the same
   type.

 * *Curry-style* typing, also known as /implicit typing/; which creates a
   formalism where every single term can be given an infinite number of types.
   This technique is called *polymorphism* in general; and here it is only used
   to tell the kinds of combinations that are allowed with any given term.

As an example, we can consider the identity term $I = \lambda x.x$. It would have to be 
defined for each possible type. That is, we should consider a family of different 
identity terms $I_A = \lambda x.x : A \to A$. Curry-style typing allow us to consider 
parametric types with type variables, and to type the identity as 
$I = \lambda x.x : \sigma \to \sigma$ where $\sigma$ is a type variable.

#+attr_latex: :options [Type variables]
#+begin_definition
Given a infinite numerable set of /type variables/, we define *parametric types*
or /type-schemes/ inductively as
\[\mathtt{PType} ::= 
\iota \mid
\mathtt{Tvar} \mid 
\mathtt{PType} \to \mathtt{PType}, \]
that is, all basic types and type variables are atomic parametric types; and we also
consider the arrow type between two parametric types.
#+end_definition

The difference between the two typing styles is then not a mere notational
convention, but a difference on the expressive power that we assign to each
term. The interesting property of type variables is that they can act as
placeholders for other type templates. This is formalized with the notion
of type substitution.

#+attr_latex: :options [Type substitution]
#+begin_definition
A *substitution* $\psi$ is any function from type variables to type templates. It can
be applied to a type template as $\overline{\psi}$ by recursion and knowing that

   * $\overline{\psi} \iota = \iota$,
   * $\overline{\psi} \sigma = \psi \sigma$,
   * $\overline{\psi} (A \to B) = \overline{\psi} A \to \overline{\psi} B$.

That is, the type template $\overline{\psi} A$ is the same as $A$ but with every type variable
replaced according to the substitution $\sigma$.
#+end_definition

We consider a type to be /more general/ than other if the latter can be obtained by
applying a substitution to the former. In this case, the latter is called an /instance/
of the former. For instance, $A \to B$ is more general than its instance
$(C \to D) \to B$, where $A$ is a type variable affected by the substitution. An
interesting property of STLC is that every type has a most general type, called
its /principal type/.

#+attr_latex: :options [Principal type]
#+begin_definition
A closed \lambda-term $M$ has a *principal type* $\pi$ if $M : \pi$ and given any
$M : \tau$, we can obtain $\tau$ as an instance of $\pi$, that is, $\overline{\sigma} \pi = \tau$.
#+end_definition

*** Unification and type inference
**** Unification :ignore:
The unification of two type templates is the construction of two substitutions
making them equal as type templates; i.e., the construction of a type that
is a particular instance of both at the same time. We will not only aim for
an unifier but for the most general one between them.

#+attr_latex: :options [Most general unifier]
#+begin_definition
A substitution $\psi$ is called an *unifier* of two sequences of type templates
$\left\{ A_i \right\}_{i = 1,\dots,n}$ and $\left\{ B_i \right\}_{i=1,\dots,n}$ if $\overline{\psi} A_i = \overline{\psi} B_i$ for any $i$. We say that it
is the *most general unifier* if given any other unifier $\phi$ exists a substitution
$\varphi$ such that $\phi = \overline{\varphi} \circ \psi$.
#+end_definition

#+begin_lemma
<<lemma-unification>>
If an unifier of $\left\{ A_i \right\}_{i = 1,\dots,n}$ and $\left\{ B_i \right\}_{i=1,\dots,n}$ exists, the most general unifier
can be found using the following recursive definition of $\mathtt{unify}(A_1,\dots,A_n;B_1,\dots,B_n)$.

  1) $\mathtt{unify}(x;x) = \id$ and $\mathtt{unify}(\iota,\iota) = \id$;
  2) $\mathtt{unify}(x;B) = (x \mapsto B)$, the substitution that only changes $x$ by $B$;
     if $x$ does not occur in $B$. The algorithm *fails* if $x$ occurs in $B$;
  3) $\mathtt{unify}(A;x)$ is defined symmetrically;
  4) $\mathtt{unify}(A \to A'; B \to B') = \mathtt{unify}(A,A';B,B')$;
  5) $\mathtt{unify}(A,A_1,\dots; B,B_1,\dots) = \overline{\psi} \circ \rho$ where $\rho = \mathtt{unify}(A_1,\dots;B_1,\dots)$ 
     and $\psi = \mathtt{unify}(\overline{\rho}A; \overline{\rho}B)$;
  6) $\mathtt{unify}$ fails in any other case.

Where $x$ is any type variable. The two sequences of types have no unifier if and only
if $\mathtt{unify}(A,B)$ fails.
#+end_lemma
#+begin_proof
It is easy to notice that, by structural induction, if
$\mathtt{unify}(A;B)$ exists, it is in fact an unifier.

If the unifier fails in clause 2, there is obviously no possible unifier: the number
of constructors on the first type template will be always smaller than the second one.
If the unifier fails in clause 6, the type templates are fundamentally different, they
have different head constructors and this is invariant to substitutions. This proves
that the failure of the algorithm implies the non existence of an unifier.

We now prove that, if $A$ and $B$ can be unified, $\mathtt{unify}(A,B)$ is the most general unifier.
For instance, in the clause 2, if we call $\psi = (x \mapsto B)$ and, if $\eta$ were another unifier,
then $\eta x = \overline{\eta}x = \overline{\eta} B = \overline{\eta}(\psi(x))$; hence $\overline{\eta} \circ \psi = \eta$ by definition of $\psi$. A similar argument can 
be applied to clauses 3 and 4. In the clause 5, we suppose the existence of some unifier $\psi'$. 
The recursive call gives us the most general unifier $\rho$ of $A_1,\dots,A_n$ and $B_1,\dots,B_{n}$; and 
since it is more general than $\psi'$, there exists an $\alpha$ such that $\overline{\alpha} \circ \rho = \psi'$. Now,
$\overline{\alpha}(\overline{\rho}A) = \psi'(A) = \psi'(B) = \overline{\alpha}(\overline{\rho} B)$, hence $\alpha$ is a unifier of $\overline{\rho}A$ and $\overline{\rho}B$; we can take the 
most general unifier to be $\psi$, so $\overline{\beta} \circ \psi = \overline{\alpha}$; and finally, $\overline{\beta} \circ (\overline{\psi} \circ \rho) = \overline{\alpha} \circ \rho = \psi'$.

We also need to prove that the unification algorithm terminates. Firstly, we note that
every substitution generated by the algorithm is either the identity or it removes at least
one type variable. We can perform induction on the size of the argument on all clauses except
for clause 5, where a substitution is applied and the number of type variables is reduced.
Therefore, we need to apply induction on the number of type variables and only then apply
induction on the size of the arguments.
#+end_proof

**** Type Inference :ignore:
Using unification, we can define type inference.

#+attr_latex: :options [Type inference]
#+begin_theorem
<<thm-typeinfer>>
The algorithm $\mathtt{typeinfer}(M,B)$, defined as follows, finds the most general substitution $\sigma$
such that $x_1 : \sigma A_1, \dots, x_n : \sigma A_n \vdash M : \overline{\sigma} B$ is a valid typing judgment if it exists;
and fails otherwise.

  1) $\mathtt{typeinfer}(x_i:A_i,\Gamma \vdash x_i : B) = \mathtt{unify}(A_i,B)$;
  2) $\mathtt{typeinfer}(\Gamma \vdash MN : B) = \overline{\varphi} \circ \psi$, where $\psi = \mathtt{typeinfer}(\Gamma \vdash M : x \to B)$ and
     $\varphi = \mathtt{typeinfer}(\overline{\psi}\Gamma \vdash N : \overline{\psi}x)$ for a fresh type variable $x$.
  3) $\mathtt{typeinfer}(\Gamma \vdash \lambda x.M : B) = \overline{\varphi} \circ \psi$ where $\psi = \mathtt{unify}(B; z \to z')$ and
     $\varphi = \mathtt{typeinfer}(\overline{\psi}\Gamma, x:\overline{\psi}z \vdash M : \overline{\psi}z')$ for fresh type variables $z,z'$.

Note that the existence of fresh type variables is always asserted by the set of
type variables being infinite. The output of this algorithm is defined up to
a permutation of type variables.
#+end_theorem
#+begin_proof
The algorithm terminates by induction on the size of $M$. It is easy to check
by structural induction that the inferred type judgments are in fact valid.
If the algorithm fails, by Lemma [[lemma-unification]], it is also clear that the
type inference is not possible.

On the first case, the type is obviously the most general substitution
by virtue of the previous Lemma [[lemma-unification]].  On the second
case, if $\alpha$ were another possible substitution, in particular, it should
be less general than $\psi$, so $\alpha = \beta \circ \psi$. As $\beta$ would be then a possible substitution
making $\overline{\psi}\Gamma \vdash N : \overline{\psi}x$ valid, it should be less general than $\varphi$, so 
$\alpha = \overline{\beta} \circ \psi = \overline{\gamma} \circ \overline{\varphi} \circ \beta$.
On the third case, if $\alpha$ were another possible substitution, it should unify
$B$ to a function type, so $\alpha = \overline{\beta} \circ \psi$. Then $\beta$ should make the type inference
$\overline{\psi}\Gamma, x:\overline{\psi}z \vdash M : \overline{\psi}z'$ possible, so $\beta = \overline{\gamma} \circ \varphi$.
We have proved that the inferred type is in general the most general one.
#+end_proof

#+attr_latex: :options [Principal type property]
#+begin_corollary
Every typeable pure \lambda-term has a principal type.
#+end_corollary
#+begin_proof
Given a typeable term $M$, we can compute $\mathtt{typeinfer}(x_1:A_1,\dots,x_n:A_n \vdash M : B)$,
where $x_1,\dots,x_n$ are the free variables on $M$ and $A_1,\dots,A_n,B$ are fresh type
variables. By virtue of Theorem [[thm-typeinfer]], the result is the most general type of $M$
if we assume the variables to have the given types.
#+end_proof

*** TODO Subject reduction

# 6.1 on Selinger
#+attr_latex: :options [Subject reduction]
#+begin_theorem
The type is preserved on \beta-reductions; that is, if $\Gamma \vdash M : A$ and
and $M \longrightarrow_{\beta} M'$, then $\Gamma \vdash M' : A$.
#+end_theorem
#+begin_proof

#+end_proof

*** TODO Normalization
We have seen previously that the term $\Omega = (\lambda x.xx)(\lambda x.xx)$ is
not weakly normalizing; but it is also non-typeable. In this section
we will prove that, in fact, every typeable term is strongly
normalizing.

# Proof in Girard, Lafont, Taylor; chapters 4 and 6
# http://www.paultaylor.eu/stable/prot.pdf
#+attr_latex: :options [Strong normalization in STLC]
#+begin_theorem
In the STLC, all terms are strongly normalizing.
#+end_theorem

# [[https://math.stackexchange.com/questions/1319149/what-breaks-the-turing-completeness-of-simply-typed-lambda-calculus][What breaks turing completeness of STLC]] (link)

** The Curry-Howard correspondence
# Tutorial on Curry-Howard http://purelytheoretical.com/papers/ATCHC.pdf
# Local soundness and completeness http://www.cs.cmu.edu/~fp/courses/15816-s10/lectures/01-judgments.pdf
# https://www.elsevier.com/books/lectures-on-the-curry-howard-isomorphism/sorensen/978-0-444-52077-7

*** Extending the simply typed \lambda-calculus
We will add now special syntax for some terms and types, such as
pairs, unions and unit types. This syntax will make our \lambda-calculus
more expressive, but the unification and type inference algorithms
will continue to work. The previous proofs and algorithms can be extended to cover
all the new cases.
# And this is done on the mikrokosmos implementation

#+attr_latex: :options [Simple types II]
#+begin_definition
The new set of *simple types* is given by the following BNF
\[\mathtt{Type} ::= \iota \mid 
\mathtt{Type} \to \mathtt{Type} \mid
\mathtt{Type} \times \mathtt{Type} \mid
\mathtt{Type} + \mathtt{Type} \mid
1 \mid
0,\]
where $\iota$ would be any /basic type/.
#+end_definition

That is to say that, for any given types $A,B$, there exists a product
type $A \times B$, consisting of the pairs of elements where the first
one is of type $A$ and the second one of type $B$; there exists the
union type $A + B$, consisting of a disjoint union of tagged terms
from $A$ or $B$; an unit type $1$ with only an element, and an empty
or void type $0$ without inhabitants. The raw typed \lambda-terms are
extended to use these new types.

#+attr_latex: :options [Raw typed lambda terms II]
#+begin_definition
The new set of raw *typed lambda terms* is given by the BNF
\[\begin{aligned} 
\mathtt{Term} ::=\ &
x \mid
\mathtt{Term}\mathtt{Term} \mid
\lambda x. \mathtt{Term} \mid \\&
\left\langle \mathtt{Term},\mathtt{Term} \right\rangle \mid
\pi_1 \mathtt{Term} \mid
\pi_2 \mathtt{Term} \mid \\&
\textrm{inl}\ \mathtt{Term} \mid
\textrm{inr}\ \mathtt{Term} \mid
\textrm{case}\ \mathtt{Term}\ \textrm{of}\ \mathtt{Term}; \mathtt{Term} \mid \\&
\textrm{abort}\ \mathtt{Term} \mid \ast
\end{aligned}\]
#+end_definition

The use of these new terms is formalized by the following extended set
of typing rules.

 1) The $(var)$ rule simply makes explicit the type of a variable from
    the context.

    \begin{prooftree}
    \LeftLabel{($var$)}
    \AXC{}
    \UIC{$\Gamma, x:A \vdash x:A$}
    \end{prooftree}

 2) The $(abs)$ gives the type of a \lambda-abstraction as the type of
    functions from the variable type to the result type. It acts as
    a constructor of function terms.

   \begin{prooftree}
   \LeftLabel{$(abs)$}
   \AXC{$\Gamma, x:A \vdash M : B$}
   \UIC{$\Gamma \vdash \lambda x.M : A \to B$}
   \end{prooftree}

 3) The $(app)$ rule gives the type of a well-typed application of a
    lambda term. A term $f : A \to B$ applied to a term $a : A$ is a term
    of type $B$. It acts as a destructor of function terms.

    \begin{prooftree}
    \LeftLabel{$(app)$}
    \AXC{$\Gamma \vdash f : A \to B$}
    \AXC{$\Gamma \vdash a : A$}
    \BIC{$\Gamma \vdash f a : B$}
    \end{prooftree}

 4) The $(pair)$ rule gives the type of a pair of elements. It acts as
    a constructor of pair terms.

    \begin{prooftree}
    \LeftLabel{$(pair)$}
    \AXC{$\Gamma \vdash a : A$}
    \AXC{$\Gamma \vdash b :  B$}
    \BIC{$\Gamma \vdash \pair{a,b} : A \times B$}
    \end{prooftree}

 5) The $(\pi_1)$ rule extracts the first element from a pair. It acts as
    a destructor of pair terms.

    \begin{prooftree}
    \LeftLabel{$(\pi_1)$}
    \AXC{$\Gamma \vdash m : A \times B$}
    \UIC{$\Gamma \vdash \pi_1\ m : A$}
    \end{prooftree}

 6) The $(\pi_1)$ rule extracts the second element from a pair. It acts as
    a destructor of pair terms.

    \begin{prooftree}
    \LeftLabel{$(\pi_2)$}
    \AXC{$\Gamma \vdash m : A \times B$}
    \UIC{$\Gamma \vdash \pi_2\ m : B$}
    \end{prooftree}

 7) The $(inl)$ rule creates a union type from the left side type of
    the sum. It acts as a constructor of union terms.

    \begin{prooftree}
    \LeftLabel{$(inl)$}
    \AXC{$\Gamma \vdash a : A$}
    \UIC{$\Gamma \vdash \mathrm{inl}\ a : A + B$}
    \end{prooftree}

 8) The $(inr)$ rule creates a union type from the right side type of
    the sum. It acts as a constructor of union terms.

    \begin{prooftree}
    \LeftLabel{$(inr)$}
    \AXC{$\Gamma \vdash b : B$}
    \UIC{$\Gamma \vdash \mathrm{inr}\ b : A + B$}
    \end{prooftree}

 9) The $(case)$ rule extracts a term from an union and uses on any of the
    two cases

    \begin{prooftree}
    \LeftLabel{$(case)$}
    \AXC{$\Gamma \vdash m : A + B$}
    \AXC{$\Gamma, a:A \vdash n : C$}
    \AXC{$\Gamma, b:B \vdash p : C$}
    \TIC{$\Gamma \vdash (\mathrm{case}\ m\ \mathrm{of}\ [a].N;\ [b].P) : C$}
    \end{prooftree}

 10) The $(\ast)$ rule simply creates the only element of $1$. It is a constructor
     of the unit type.

     \begin{prooftree}
     \LeftLabel{$(\ast)$}
     \AXC{$$}
     \UIC{$\Gamma \vdash \ast : 1$}
     \end{prooftree}


 11) The $(abort)$ rule extracts a term of any type from the void type.

     \begin{prooftree}
     \LeftLabel{$(abort)$}
     \AXC{$\Gamma \vdash M : 0$}
     \UIC{$\Gamma \vdash \mathrm{abort}_A\ M : A$}
     \end{prooftree} 

     The abort function must be understood as the unique function going
     from the empty set to any given set.

The \beta-reduction of terms is defined the same way as for the untyped
\lambda-calculus; except for the inclusion of \beta-rules governing the
new terms, one for every destruction rule.

  1) Function application, $(\lambda x.M)N \to_{\beta} M[N/x]$.
  2) First projection, $\pi_1 \left\langle M,N \right\rangle \to_{\beta} M$.
  3) Second projection, $\pi_2 \left\langle M,N \right\rangle \to_{\beta} N$.
  4) Case rule, $(\mathrm{case}\ m\ \mathrm{of}\ [a].N;\ [b].P) \to_{\beta} N a$ if $m$ is of the form $m = \mathrm{inl}\ a$; and
     $(\mathrm{case}\ m\ \mathrm{of}\ [a].N;\ [b].P) \to_{\beta} P b$ if $m$ is of the form $m = \mathrm{inr}\ b$.

On the other side, \eta-rules are defined one for every construction rule.

  1) Function extensionality, $\lambda x.M x \to_{\eta} M$.
  2) Definition of product, $\langle \pi_1 M, \pi_{2} M \rangle \to_{\eta} M$.
  3) Uniqueness of unit, $M \to_{\eta} \ast$.
  4) Case rule, $(\mathrm{case}\ m\ \mathrm{of}\ [a].P[ \mathrm{inl}\ a/c ];\ [b].P[ \mathrm{inr}\ b/c ]) \to_{\eta} P[m/c]$.

*** Natural deduction
The natural deduction is a logical system due to Gentzen. We introduce
it here following cite:selinger13 and cite:wadler15. It relationship
with the STLC will be made explicit on the [[*Propositions as types][next section]].

We will use the logical binary connectives $\to,\land,\lor$, and two
given propositions, $\top,\bot$ representing truth and falsity. The
rules defining natural deduction come in pairs; there are introductors
and eliminators for every connective. Every introductor uses a set of
assumptions to generate a formula and every eliminator gives a way to
extract precisely that set of assumptions.

 1) Every axiom on the context can be used.

    \begin{prooftree}
    \RightLabel{(Ax)}
    \AXC{}
    \UIC{$\Gamma,A \vdash A$}
    \end{prooftree}

 2) Introduction and elimination of the $\to$ connective. Note that the
    elimination rule corresponds to /modus ponens/.

    \begin{prooftree}
    \RightLabel{($I_{\to}$)}
    \AXC{$\Gamma, A \vdash B$}
    \UIC{$\Gamma \vdash A \to B$}
    \RightLabel{($E_{\to}$)}
    \AXC{$\Gamma \vdash A \to B$}
    \AXC{$\Gamma \vdash A$}
    \BIC{$\Gamma \vdash B$}
    \noLine
    \BIC{}
    \end{prooftree}

 3) Introduction and elimination of the $\land$ connective. Note that the
    introduction in this case takes two assumptions, and there are
    two different elimination rules.

    \begin{prooftree}
    \RightLabel{($I_{\land}$)}
    \AXC{$\Gamma \vdash A$}
    \AXC{$\Gamma \vdash B$}
    \BIC{$\Gamma \vdash A \land B$}
    \RightLabel{($E_{\land}^1$)}
    \AXC{$\Gamma \vdash A \land B$}
    \UIC{$\Gamma \vdash A$}
    \RightLabel{($E_{\land}^2$)}
    \AXC{$\Gamma \vdash A \land B$}
    \UIC{$\Gamma \vdash B$}
    \noLine
    \TIC{}
    \end{prooftree}

 4) Introduction and elimination of the $\lor$ connective. Here, we need
    two introduction rules to match the two assumptions we use on the
    eliminator.

    \begin{prooftree}
    \RightLabel{($I_{\lor}^1$)}
    \AXC{$\Gamma \vdash A$}
    \UIC{$\Gamma \vdash A \lor B$}
    \RightLabel{($I_{\lor}^2$)}
    \AXC{$\Gamma \vdash B$}
    \UIC{$\Gamma \vdash A \lor B$}
    \RightLabel{($E_{\lor}$)}
    \AXC{$\Gamma \vdash A \lor B$}
    \AXC{$\Gamma,A \vdash C$}
    \AXC{$\Gamma,B \vdash C$}
    \TIC{$\Gamma \vdash C$}
    \noLine
    \TIC{}
    \end{prooftree}

 5) Introduction for $\top$. It needs no assumptions and, consequently,
    there is no elimination rule for it.

    \begin{prooftree}
    \RightLabel{($I_{\top}$)}
    \AXC{}
    \UIC{$\Gamma \vdash \top$}
    \end{prooftree}

 6) Elimination for $\bot$. It can be eliminated in all generality, and,
    consequently, there are no introduction rules for it. This elimination
    rule represents the /"ex falsum quodlibet"/ principle that says that
    falsity implies anything.

    \begin{prooftree}
    \RightLabel{($E_{\bot}$)}
    \AXC{$\Gamma \vdash \bot$}
    \UIC{$\Gamma \vdash C$}
    \end{prooftree}

Proofs on natural deduction are written as deduction trees, and they
can be simplified according to some simplification rules, which can
be applied anywhere on the deduction tree. On these rules, a chain
of dots represents any given part of the deduction tree.

  1) An implication and its antecedent can be simplified using the
     antecedent directly on the implication.

    \begin{prooftree}
    \AXC{$[A]$}\noLine
    \UIC{$\vdots^{1}$}\noLine
    \UIC{$B$}
    \UIC{$A \to B$}
    \AXC{$\vdots^2$}\noLine
    \UIC{$A$}
    \BIC{$B$}

    \UIC{$\vdots$}\noLine
    \AXC{$\Longrightarrow$}
    \UIC{}\noLine\UIC{}\noLine\UIC{}\noLine

    \AXC{$\vdots^{2}$}\noLine
    \UIC{$A$}\noLine
    \UIC{$\vdots^{1}$}\noLine
    \UIC{$B$}
    \UIC{$\vdots$}\noLine
    \noLine
    \TIC{}
    \end{prooftree}

  2) The introduction of an unused conjunction can be simplified
     as

    \begin{prooftree}
    \AXC{$\vdots^{1}$}\noLine
    \UIC{$A$}
    \AXC{$\vdots^{2}$}\noLine
    \UIC{$B$}
    \BIC{$A \land B$}
    \UIC{$A$}
    \UIC{$\vdots$}\noLine
    \AXC{$\Longrightarrow$}
    \UIC{}\noLine\UIC{}\noLine\UIC{}\noLine
    \AXC{$\vdots^{1}$}\noLine
    \UIC{$A$}
    \UIC{$\vdots$}\noLine
    \noLine
    \TIC{}
    \end{prooftree}

    and, similarly, on the other side as

    \begin{prooftree}
    \AXC{$\vdots^{1}$}\noLine
    \UIC{$A$}
    \AXC{$\vdots^{2}$}\noLine
    \UIC{$B$}
    \BIC{$A \land B$}
    \UIC{$B$}
    \UIC{$\vdots$}\noLine
    \AXC{$\Longrightarrow$}
    \UIC{}\noLine\UIC{}\noLine\UIC{}\noLine
    \AXC{$\vdots^{2}$}\noLine
    \UIC{$B$}
    \UIC{$\vdots$}\noLine
    \noLine
    \TIC{}
    \end{prooftree}

  3) The introduction of a disjunction followed by its elimination can
     be also simplified

    \begin{prooftree}
    \AXC{$\vdots^{1}$}\noLine
    \UIC{$A$}
    \UIC{$A+B$}
    \AXC{$[A]$}\noLine
    \UIC{$\vdots^2$}\noLine
    \UIC{$C$}
    \AXC{$[B]$}\noLine
    \UIC{$\vdots^3$}\noLine
    \UIC{$C$}
    \TIC{$C$}

    \UIC{$\vdots$}\noLine
    \AXC{$\Longrightarrow$}
    \UIC{}\noLine\UIC{}\noLine\UIC{}\noLine

    \AXC{$\vdots^{1}$}\noLine
    \UIC{$A$}\noLine
    \UIC{$\vdots^{2}$}\noLine
    \UIC{$C$}
    \UIC{$\vdots$}\noLine
    \noLine
    \TIC{}
    \end{prooftree}

    and a similar pattern is used on the other side of the disjunction

    \begin{prooftree}
    \AXC{$\vdots^{1}$}\noLine
    \UIC{$B$}
    \UIC{$A+B$}
    \AXC{$[A]$}\noLine
    \UIC{$\vdots^2$}\noLine
    \UIC{$C$}
    \AXC{$[B]$}\noLine
    \UIC{$\vdots^3$}\noLine
    \UIC{$C$}
    \TIC{$C$}

    \UIC{$\vdots$}\noLine
    \AXC{$\Longrightarrow$}
    \UIC{}\noLine\UIC{}\noLine\UIC{}\noLine

    \AXC{$\vdots^{1}$}\noLine
    \UIC{$B$}\noLine
    \UIC{$\vdots^{3}$}\noLine
    \UIC{$C$}
    \UIC{$\vdots$}\noLine
    \noLine
    \TIC{}
    \end{prooftree}

*** Propositions as types
In 1934, Curry observed in cite:curry34 that the type of a function
$(A \to B)$ could be read as an implication and that the existence of a
function of that type was equivalent to the provability of the proposition.
Previously, the *Brouwer-Heyting-Kolmogorov interpretation* of intuitionistic
logic had given a definition of what it meant to be a proof of an intuinistic
formula, where a proof of the implication $(A \to B)$ was a function converting
a proof of $A$ into a proof of $B$. It was not until 1969 that Howard pointed
a deep correspondence between the simply-typed \lambda-calculus and the
natural deduction at three levels

  1) propositions are types.
  2) proofs are programs.
  3) simplification of proofs is the evaluation of programs.

In the case of STLC and natural deduction, the correspondence starts when
we describe the following isomorphism between types and propositions.

\begin{center}\begin{tabular}{c|c}
Types & Propositions \\
\hline
Unit type ($1$) & Truth ($\top$) \\
Product type ($\times$) & Conjunction ($\land$) \\
Union type ($+$) & Disjunction ($\lor$) \\
Function type ($\to$) & Implication ($\to$) \\
Empty type ($0$) & False ($\bot$)
\end{tabular}\end{center}

Now it is easy to notice that every [[*Natural deduction][deduction rule]] for a proposition has a
correspondence with a [[*Extending the simply typed \lambda-calculus][typing rule]]. The only distinction between them is the
appearance of \lambda-terms on the first set of rules. As every typing rule
results on the construction of a particular kind of \lambda-term, they can
be interpreted as encodings of proof in the form of derivation trees. That is,
terms are proofs of the propositions represented by their types.

# TODO: Example of the correspondence. Terms-Proofs

Under this interpretation, simplification rules are precisely the
\beta-reduction rules. This makes execution of \lambda-calculus
programs correspond to proof simplification on natural deduction.
The Curry-Howard correspondence is then not only a simple bijection
between types and propositions, but a deeper isomorphism regarding the
way they are constructed, used in derivations, and simplified.

#+attr_latex: :options [Curry-Howard example]
#+begin_exampleth
As an example, we will write a proof/term of the proposition/type =A → B + A=
and we are going to simplify/compute it using proof simplification rules/\beta-rules.

We start with the following derivation tree

\begin{prooftree}\EnableBpAbbreviations
\AXC{$[A+B]$}
\AXC{$[A]$}
\RightLabel{$(inr)$}
\UIC{$B+A$}
\AXC{$[B]$}
\RightLabel{$(inl)$}
\UIC{$B+A$}
\RightLabel{$(case)$}
\TIC{$B+A$}
\RightLabel{$(abs)$}
\UIC{$A+B \to B +A$}

\AXC{$[A]$}
\RightLabel{$(inl)$}
\UIC{$A+B$}
\RightLabel{$(app)$}
\BIC{$B+A$}
\RightLabel{$(abs)$}
\UIC{$A \to B + A$}
\end{prooftree}

which is encoded by the term =λa.(λc.case c of (λx.inr) (λy.inl y)) (inl a)=.
We apply the simplification rule/\beta-rule of the implication/function application
to get

\begin{prooftree}\EnableBpAbbreviations
\AXC{$[A]$}
\RightLabel{$(inl)$}
\UIC{$A+B$}
\AXC{$[A]$}
\RightLabel{$(inr)$}
\UIC{$B+A$}
\AXC{$[B]$}
\RightLabel{$(inl)$}
\UIC{$B+A$}
\RightLabel{$(case)$}
\TIC{$B+A$}
\RightLabel{$(abs)$}
\UIC{$A \to B +A$}
\end{prooftree}

which is encoded by the term =λa.case (inl a) of (λx.inr) (λy.inl y)=. We finally
apply the =case= simplification/reduction rule to get

\begin{prooftree}\EnableBpAbbreviations
\AXC{$[A]$}
\RightLabel{$(inr)$}
\UIC{$B+A$}
\RightLabel{$(abs)$}
\UIC{$A \to B +A$}
\end{prooftree}

which is encoded by =λa.(inr a)=.

On the chapter on [[*Mikrokosmos][Mikrokosmos]], we develop a \lambda-calculus interpreter
which is able to check and simplify proofs on intuitionistic logic.
This example could be checked and simplified by this interpreter as

#+BEGIN_SRC haskell
\a.((\c.caseof c (\x.inr x) (\y.inl y))(inl a))
---> output: λa.(INR a) ⇒ inr :: A → B + A
#+END_SRC
#+end_exampleth

# Extending the Curry-Howard correspondence
** Other type systems
*** TODO Hindley-Milner
*** TODO System F                                                :noexport:
**** TODO System F is strongly normalizing
*** \lambda-cube
The *\lambda-cube* is a taxonomy for Church-style type systems given
by Barendregt in cite:barendregt92. It describes eight type systems
based on the \lambda-calculus along three axes, representing three
properties of the systems. These properties are

  1) *parametric polymorphism*, terms that depend on types. This is
     achieved via universal quantification over types. It allows type
     variables and binders for them as in the following parametric
     identity function
     \[
     \mathrm{id} \equiv \Lambda \tau . \lambda x . x : \forall \tau . \tau \to \tau, 
     \]
     that can be applied to any particular type $\sigma$ to obtain the 
     specific identity function for that type as
     \[
     \mathrm{id}_{\sigma} \equiv \lambda x.x : \sigma \to \sigma.
     \]

     *System F* is the simplest type system on the cube implementing
     polymorphism.

  2) *type operators*, types that depend on types.

  3) *dependent types*, types that depend on terms.

# Pierce
# Lectures on the Curry-Howard isomorphism
# Introduction to generalized type systems - Barendregt

# https://en.wikipedia.org/wiki/System_F#System_F.CF.89

\[\begin{tikzcd}[column sep=small]
&&& |[label={above:\lcubett{System F$\omega$}}]| \systemfo \ar{rr}
&&  |[label=above:\lcubett{CoC},label=above:\phantom{System Fo}]| \systemcoc 
& \\
\phantom{.}  
&&  |[label={left:\lcubett{System F}}]| \systemf \ar{ur}\ar{rr} 
&&  \systemfp \ar{ur}
&& \\ 
&&& \systemo \ar{rr}\ar{uu} 
&&  |[label=right:\lcubett{wCoC}]| \systemlpo \ar{uu}
&&  \phantom{\lambda}\phantom{PQW}  \\
\ar[\lcred]{uu}[\lcred]{\text{\parbox{2cm}{\centering terms depend on types}}}
&&  |[label=below:\lcubett{STLC}]| \stlc \ar{uu}\ar{rr}\ar{ur} 
&&  |[label=below:\lcubett{DTLC}]| \systemlp \ar{uu}\ar{ur} 
&& \phantom{.} \\ 
&&  \ar[\lcred]{rr}[swap,\lcred]{\text{\parbox{2cm}{\centering types depend on terms}}} 
&& \phantom{.} 
& \ar[\lcred]{ur}[swap,\lcred]{\text{\parbox{2cm}{\centering types depend on types}}} 
&
\end{tikzcd}\]

The following type systems

 * *Simply typed \lambda-calculus* ($\stlc$);
 * *System F* ($\systemf$);
 * typed \lambda-calculus with *dependent types* ($\systemlp$);
 * typed \lambda-calculus with *type operators* ($\systemo$);
 * *System F-omega* ($\systemfo$);
   
The \lambda-cube is generalized by the theory of pure type systems.

All systems on the \lambda-cube are strongly normalizing.

# https://cstheory.stackexchange.com/questions/7561/whats-the-relation-and-difference-between-calculus-of-inductive-constructions-a
A different approach to higher-order type systems will be presented on the
chapter on Type Theory.

*** TODO Pure type systems
In particular *System F* is equivalent to the single-sorted pure system $\lambda 2$.
# https://www.ps.uni-saarland.de/extras/fscd17/

*** TODO Subtyping (?)

*** TODO Inductive and coinductive definitions
* Mikrokosmos (abstract)                                             :ignore:
#+LATEX: \ctparttext{\color{black}\begin{center}
We have developed *Mikrokosmos*, an untyped and simply typed \lambda-calculus interpreter
written in the purely functional programming language Haskell cite:hudak07_haskell.
It aims to provide students with a tool to learn and understand \lambda-calculus
and the relation between logic and types.
#+LATEX: \end{center}}

* Mikrokosmos
** Programming environment
*** The Haskell programming language
**** Haskell as a programming choice                              :ignore:
*Haskell* is the purely functional programming language of our choice
to implement Mikrokosmos. Its design is heavily influenced by the
\lambda-calculus and is a general-purpose language with a rich
ecosystem and plenty of consolidated libraries [fn:hackagelibs] in
areas such as parsing, testing or system interaction, satisfying the
requisites of our project. In the following sections, we describe this
ecosystem in more detail.

[fn:hackagelibs]: In the central package archive of the Haskell community,
Hackage, a categorized list of libraries can be found: https://hackage.haskell.org/packages/

**** History of Haskell                                           :ignore:
In the 1980s, many lazy programming languages were independently being
written by researchers such as /Miranda/, /Lazy ML/, /Orwell/, /Clean/
or /Daisy/. All of them were similar in expressive power, but their
differences were holding back the efforts to communicate ideas on
functional programming.  A comitee was created in 1987 with the
mission of designing a common lazy functional language. Several
versions of the language were developed, and the first standarized
reference of the language was published in the *Haskell 98 Report*,
whose revised version can be read on cite:haskell98. Its more popular
implementation is the *Glasgow Haskell Compiler (GHC)*; an open source
compiler written in Haskell and C.

The complete history of Haskell and its design decisions is detailed
on cite:hudak07_haskell.

**** Haskell's properties                                         :ignore:
Haskell is

  * *strongly and statically typed*, meaning that it only compiles
    well-typed programs and it does not allow implicit type
    casting. The compiler will generate an error if a term is
    non-typeable.

  * *lazy*, with /non-strict semantics/, meaning that 
    
    This /call-by-need/ approach is usually less efficient than the
    /call-by-value/ one.
    
    In [ Hughes Why functional programming matters ], Hughes argued
    for the benefits of a lazy functional language.

  * *purely functional*. As the evaluation order is demand-driven and
    not explicitly known, it is not possible in practice to perform
    ordered input/output actions or any other side-effects by relying
    on the evaluation order. This helps modularity of the code,
    testing and verfication.

  * *referentially transparent*. As a consequence of its purity, every
    term on the code could be replaced by its definition without
    changing the global meaning of the program. This allows equational
    reasoning with rules that derive directly from \lambda-calculus.

  * based on *System F\omega* with some restrictions. Crucially, it
    implements *System F* adding quantification over type operators
    even if it does not allow abstraction on type operators. The GHC
    Haskell compiler, however, allows the user the ability to activate
    extensions that implement dependent types.
    # https://stackoverflow.com/a/21220357/2552681

**** Haskell's syntax                                             :ignore:
Where most imperative languages use semicolons to separate sequential
commands, Haskell has no notion of sequencing, and programs are
written in a purely declarative way. A Haskell program essentially
consist on a series of definitions (of both types and terms) and type
declarations. The following example shows the definition of a binary
tree and its preorder as

#+BEGIN_SRC haskell
-- A tree is either empty or a node with two subtrees.
data Tree a = Empty | Node a (Tree a) (Tree a)

-- The preorder function takes a tree and returns a list
preorder :: Tree a -> [a]
preorder Empty            = []
preorder (Node x lft rgt) = preorder lft ++ [x] ++ preorder rgt
#+END_SRC

We can see on the previous example that function definitions allow
/pattern matching/, that is, data constructors can be used in
definitions to decompose values of the type. This increases readability
when working with algebraic data types.

While infix operators are allowed, function application is
left-associative in general. Definitions using partial application are
allowed, meaning that functions on multiple arguments can use currying
and can be passed only one of its arguments to define a new
function. For example, a function that squares every number on a list
could be written in two ways as

#+BEGIN_SRC haskell
squareList :: [Int] -> [Int]
squareList list = map square list

squareList' :: [Int] -> [Int]
squareList' = map square
#+END_SRC

where the second one, because of its simplicity, is usually
preferred. 

**** Type classes, monads                                         :ignore:
A characteristic piece of Haskell are *type classes*, which allow to
define common interfaces for different types. In the following
example, we define a =Monad= as a type with a suitably typed =return=
and =bind= operators.

#+BEGIN_SRC haskell
class Monad m where
  return :: a -> m a
  (>>=)  :: m a -> (a -> m b) -> m b
#+END_SRC

And lists, for example, are monads in this sense.

#+BEGIN_SRC haskell
instance Monad [] where
  return x = [x]
  xs >>= f = concat (map f xs)
#+END_SRC

Haskell uses monads in varied forms. They are used in I/O, error
propagation and stateful computations. Another characteristical syntax
bit of Haskell is the =do= notation, which provides a nicer, cleaner
way to work with types that happen to be monads. The following example
uses the list monad to compute the list of Pythagorean triples.

#+BEGIN_SRC haskell
pythagorean = do
  a <- [1..]
  b <- [1..a]
  c <- [1..b]
  guard (a^2 == b^2 + c^2)
  return (a,b,c)
#+END_SRC

Note that this list is infinite. As the language is lazy, this does not
represent a problem: the list will be evaluated only on demand.

For a more detailed treatment of monads, and their relation to
categorical monads, see the chapter on Type Theory, where we will
program with monads in Agda.

# [[http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.22.2636][CiteSeerX — Faking It: Simulating Dependent Types in Haskell]]

*** TODO Cabal, Stack and Haddock
*** Testing
*Tasty* is the Haskell testing framework of our choice for this
project. It allows the user to create a comprehensive test suite
combining multiple types of tests. The Mikrokosmos code is testing
using the following techniques

  * *unit tests*, in which individual core functions are tested
    independently of the rest of the application;

  * *property-based testing*, in which multiple test cases are
    created automatically in order to verfiy that a specified
    property always holds.

  * *golden tests*, a special case of unit tests in which the expected
    results of an IO action, as described on a file, are checked to
    match the actual ones.
    
We are using the *HUnit* library for unit tests. It tests particular
cases of type inference, unification and parsing. The following is an
example of unit test, as found in =tests.hs=. It checks that the type
inference of the identity term is correct.

#+BEGIN_SRC haskell
testCase "Identity type inference" $
typeinference (Lambda (Var 1))
@?=
Just (Arrow (Tvar 0) (Tvar 0))
#+END_SRC

We are using the *QuickCheck* library for property-based tests. It
tests transformation properties of lambda expressions. In the following
example, it tests that any deBruijn expression keeps is meaning when
is translated into a \lambda-term.

#+BEGIN_SRC haskell
QC.testProperty "Expression -> named -> expression" $
  \expr -> toBruijn emptyContext (nameExp expr) == expr
#+END_SRC

# We are using the *tasty-golden* package for golden tests.
# TODO

*** Version control and continuous integration
Mikrokosmos uses *git* as its version control system and the code,
which is licensed under GPLv3, can be publicly accessed on the
following GitHub repository:

#+begin_center
https://github.com/M42/mikrokosmos
#+end_center

Development takes place on the =development= git branch and permanent
changes are released into the =master= branch.

The code uses the *Travis CI* continuous integration system to run
tests and check that the software builds correctly after each change
and in a reproducible way on a fresh Linux installation provided by
the service.

** Implementation of \lambda-expressions
*** De Bruijn indexes
Nicolaas Govert *De Bruijn* proposed in cite:debruijn81 a way of defining \lambda-terms modulo
\alpha-conversion based on indices.  The main idea of De Bruijn
indices is to remove all variables from binders and replace every
variable on the body of an expression with a number, called /index/,
representing the number of \lambda-abstractions in scope between the
ocurrence and its binder.

Consider the following example, the \lambda-term
\[ \lambda x.(\lambda y.\ y (\lambda z.\ y z)) (\lambda t.\lambda z.\ t x)
\]
can be written with de Bruijn indices as
\[
\lambda\ (\lambda(1 \lambda(2 1))\ \lambda\lambda(2 3)\ ).
\]

De Bruijn also proposed a notation for the \lambda-calculus
changing the order of binders and \lambda-applications.  A review on
the syntax of this notation, its advantages and De Bruijn indexes, can be found in
cite:kamareddine01. In this section, we are going to describe De Bruijn
indexes but preserve the usual notation of \lambda-terms; that is, the /De Bruijn/
/indexes/ and the /De Bruijn notation/ are different concepts and we are going to
use only the former.

#+attr_latex: :options [De Bruijn indexed terms]
#+begin_definition
We define recursively the set of \lambda-terms using de Bruijn notation
following this BNF
\[ \mathtt{Exp} ::= \mathbb{N}
 \mid (\lambda\ \mathtt{Exp})
 \mid (\mathtt{Exp}\ \mathtt{Exp})
\]
#+end_definition

Our internal definition closely matches the formal one. The names of
the constructors here are =Var=, =Lambda= and =App=:

#+BEGIN_SRC haskell
-- | A lambda expression using DeBruijn indexes.
data Exp = Var Integer -- ^ integer indexing the variable.
         | Lambda Exp  -- ^ lambda abstraction
         | App Exp Exp -- ^ function application
         deriving (Eq, Ord)
#+END_SRC

This notation avoids the need for the Barendregt's variable convention and
the \alpha-reductions. It will be useful to implement \lambda-calculus without
having to worry about the specific names of variables.

*** Substitution
We define the [[*Free and bound variables, substitution][substitution]] operation needed for the [[*\beta-reduction][\beta-reduction]] on
de Bruijn indices. In order to define the substitution of the n-th
variable by a \lambda-term $P$ on a given term, we must

 * find all the ocurrences of the variable. At each level of scope
   we are looking for the successor of the number we were looking
   for before.

 * decrease the higher variables to reflect the disappearance of
   a lambda.

 * replace the ocurrences of the variables by the new term, taking
   into account that free variables must be increased to avoid them
   getting captured by the outermost lambda terms. 

In our code, we apply =subs= to any expression. When it is applied to
a \lambda-abstraction, the index and the free variables of the
replaced term are increased with =incrementFreeVars=; whenever it is
applied to a variable, the previous cases are taken into consideration.

#+BEGIN_SRC haskell
-- | Substitutes an index for a lambda expression
subs :: Integer -> Exp -> Exp -> Exp
subs n p (Lambda e) = Lambda (subs (n+1) (incrementFreeVars 0 p) e)
subs n p (App f g)  = App (subs n p f) (subs n p g)
subs n p (Var m)
  | n == m    = p         -- The lambda is replaced directly  
  | n <  m    = Var (m-1) -- A more exterior lambda decreases a number
  | otherwise = Var m     -- An unrelated variable remains untouched
#+END_SRC

Then \beta-reduction can be then defined using this =subs= function.

#+BEGIN_SRC haskell
betared :: Exp -> Exp
betared (App (Lambda e) x) = substitute 1 x e
betared e = e
#+END_SRC

*** De Bruijn-terms and \lambda-terms
The internal language of the interpreter uses de Bruijn expressions,
while the user interacts with it using lambda expressions with alphanumeric
variables. Our definition of a \lambda-expression with variables will be
used in parsing and output formatting.

#+BEGIN_SRC haskell
data NamedLambda = LambdaVariable String                    
                 | LambdaAbstraction String NamedLambda     
                 | LambdaApplication NamedLambda NamedLambda
#+END_SRC

**** Lambda to deBruijn                                           :ignore:
The translation from a natural \lambda-expression to de Bruijn notation
is done using a dictionary which keeps track of the bounded variables

#+BEGIN_SRC haskell
tobruijn :: Map.Map String Integer -- ^ names of the variables used
         -> Context                -- ^ names already binded on the scope
         -> NamedLambda            -- ^ initial expression
         -> Exp
-- Every lambda abstraction is inserted in the variable dictionary,
-- and every number in the dictionary increases to reflect we are entering
-- into a deeper context.
tobruijn d context (LambdaAbstraction c e) = 
     Lambda $ tobruijn newdict context e
        where newdict = Map.insert c 1 (Map.map succ d)

-- Translation of applications is trivial.
tobruijn d context (LambdaApplication f g) = 
     App (tobruijn d context f) (tobruijn d context g)

-- We look for every variable on the local dictionary and the current scope.
tobruijn d context (LambdaVariable c) =
  case Map.lookup c d of
    Just n  -> Var n
    Nothing -> fromMaybe (Var 0) (MultiBimap.lookupR c context)
#+END_SRC

**** deBruijn to Lambda                                           :ignore:
while the translation from a de Bruijn expression to a natural one is done
considering an infinite list of possible variable names and keeping a list
of currently-on-scope variables to name the indices.

#+BEGIN_SRC haskell
-- | An infinite list of all possible variable names 
-- in lexicographical order.
variableNames :: [String]
variableNames = concatMap (`replicateM` ['a'..'z']) [1..]

-- | A function translating a deBruijn expression into a 
-- natural lambda expression.
nameIndexes :: [String] -> [String] -> Exp -> NamedLambda
nameIndexes _    _   (Var 0) = LambdaVariable "undefined"
nameIndexes used _   (Var n) = LambdaVariable (used !! pred (fromInteger n))
nameIndexes used new (Lambda e) = 
  LambdaAbstraction (head new) (nameIndexes (head new:used) (tail new) e)
nameIndexes used new (App f g) = 
  LambdaApplication (nameIndexes used new f) (nameIndexes used new g)
#+END_SRC

*** Evaluation
As we proved on Corollary [[cor-leftmosttheorem]], the leftmost reduction
strategy will find the leftmost reduction strategy if it
exists. Consequently, we will implement it using a function that
simply applies the leftmost possible reductions at each step. This
will allow us to show how the interpreter performs step-by-step
evaluations to the final user, as discussed in the [[*Verbose mode][verbose mode]] section.

#+BEGIN_SRC haskell
-- | Simplifies the expression recursively.
-- Applies only one parallel beta reduction at each step.
simplify :: Exp -> Exp
simplify (Lambda e)           = Lambda (simplify e)
simplify (App (Lambda f) x)   = betared (App (Lambda f) x)
simplify (App (Var e) x)      = App (Var e) (simplify x)
simplify (App a b)            = App (simplify a) (simplify b)
simplify (Var e)              = Var e

-- | Applies repeated simplification to the expression until it stabilizes and
-- returns all the intermediate results.
simplifySteps :: Exp -> [Exp]
simplifySteps e
  | e == s    = [e]
  | otherwise = e : simplifySteps s
  where s = simplify e
#+END_SRC

From the code we can see that the evaluation finishes whenever the
expression stabilizes. This can happen in two different cases

  * there are no more possible \beta-reductions, and the algorithm
    stops.
  * \beta-reductions do not change the expression. The computation
    would lead to an infinite loop, so it is immediately stopped.
    An common example of this is the \lambda-term $(\lambda x.x x)(\lambda x.x x)$.

*** Principal type inference
The interpreter implements the [[*Unification and type inference][unification and type inference]] algorithms
described in Lemma [[lemma-unification]] and Theorem [[thm-typeinfer]]. Their
recursive nature makes them very easy to implement directly on Haskell.

**** Type templates and substitutions                             :ignore:
We implement a simply-typed lambda calculus with [[*Curry-style types][Curry-style typing]]
and type templates. Our type system has

  * an unit type;
  * a bottom type;
  * product types;
  * union types;
  * and function types.

#+BEGIN_SRC haskell
-- | A type template is a free type variable or an arrow between two
-- types; that is, the function type.
data Type = Tvar Variable
          | Arrow Type Type
          | Times Type Type
          | Union Type Type
          | Unitty
          | Bottom
          deriving (Eq)
#+END_SRC

We will work with substitutions on type templates. They can be directly
defined as functions from types to types. A basic substitution that
inserts a given type on the place of a variable will be our building
block for more complex ones.

#+BEGIN_SRC haskell
type Substitution = Type -> Type

-- | A basic substution. It changes a variable for a type
subs :: Variable -> Type -> Substitution
subs x typ (Tvar y)
  | x == y    = typ
  | otherwise = Tvar y
subs x typ (Arrow a b) = Arrow (subs x typ a) (subs x typ b)
subs x typ (Times a b) = Times (subs x typ a) (subs x typ b)
subs x typ (Union a b) = Union (subs x typ a) (subs x typ b)
subs _ _ Unitty = Unitty
subs _ _ Bottom = Bottom
#+END_SRC

**** Unification                                                  :ignore:
Unification will be implemented making extensive use of the =Maybe=
monad. If the unification fails, it will return an error value, and
the error will be propagated to all the computation. The algorithm
is exactly the same that was defined in Lemma [[lemma-unification]].

#+BEGIN_SRC haskell
-- | Unifies two types with their most general unifier. Returns the substitution
-- that transforms any of the types into the unifier.
unify :: Type -> Type -> Maybe Substitution
unify (Tvar x) (Tvar y)
  | x == y    = Just id
  | otherwise = Just (subs x (Tvar y))
unify (Tvar x) b
  | occurs x b = Nothing
  | otherwise  = Just (subs x b)
unify a (Tvar y)
  | occurs y a = Nothing
  | otherwise  = Just (subs y a)
unify (Arrow a b) (Arrow c d) = unifypair (a,b) (c,d)
unify (Times a b) (Times c d) = unifypair (a,b) (c,d)
unify (Union a b) (Union c d) = unifypair (a,b) (c,d)
unify Unitty Unitty = Just id
unify Bottom Bottom = Just id
unify _ _ = Nothing

-- | Unifies a pair of types
unifypair :: (Type,Type) -> (Type,Type) -> Maybe Substitution
unifypair (a,b) (c,d) = do
  p <- unify b d
  q <- unify (p a) (p c)
  return (q . p)
#+END_SRC

**** Type inference                                               :ignore:
The type inference algorithm is more involved. It takes a list
of fresh variables, a type context, a lambda expression and a
constraint on the type, expressed as a type template. It outputs
a substitution. As an example, the following code shows the type
inference algorithm for function types.

#+BEGIN_SRC haskell
-- | Type inference algorithm. Infers a type from a given context and expression
-- with a set of constraints represented by a unifier type. The result type must
-- be unifiable with this given type.
typeinfer :: [Variable] -- ^ List of fresh variables
          -> Context    -- ^ Type context
          -> Exp        -- ^ Lambda expression whose type has to be inferred
          -> Type       -- ^ Constraint
          -> Maybe Substitution

typeinfer (x:vars) ctx (App p q) b = do -- Writing inside the Maybe monad.
  sigma <- typeinfer (evens vars) ctx                  p (Arrow (Tvar x) b)
  tau   <- typeinfer (odds  vars) (applyctx sigma ctx) q (sigma (Tvar x))
  return (tau . sigma)
  where
    -- The list of fresh variables has to be split into two
    odds [] = []
    odds [_] = []
    odds (_:e:xs) = e : odds xs
    evens [] = []
    evens [e] = [e]
    evens (e:_:xs) = e : evens xs
#+END_SRC

The final form of the type inference algorithm will use a
normalization algorithm shortening the type names and will apply the
type inference to the empty type context.

#+BEGIN_SRC haskell
-- | Type inference of a lambda expression.
typeinference :: Exp -> Maybe Type
typeinference e = normalize <$> 
  (typeinfer variables emptyctx e (Tvar 0) <*> pure (Tvar 0))
#+END_SRC

The complete code can be found on the [[*Mikrokosmos complete code][Appendix]].

** User interaction
*** Monadic parser combinators
A common approach to building parsers in functional programming is to
model parsers as functions. Higher-order functions on parsers act as
/combinators/, which are used to implement complex parsers in a
modular way from a set of primitive ones. In this setting, parsers
exhibit a monad algebraic structure, which can be used to simplify
the combination of parsers. A technical report on *monadic parser combinators*
can be found on cite:hutton96.

The use of monads for parsing is discussed firstly in cite:Wadler85,
and later in cite:Wadler90 and cite:hutton98. The parser type is
defined as a function taking a =String= and returning a list of pairs,
representing a successful parse each. The first component of the pair
is the parsed value and the second component is the remaining
input. The Haskell code for this definition is

#+BEGIN_SRC haskell
newtype Parser a = Parser (String -> [(a,String)])

parse :: Parser a -> String -> [(a,String)]
parse (Parser p) = p

instance Monad Parser where
  return x = Parser (\s -> [(x,s)])
  p >>= q  = Parser (\s -> 
               concat [parse (q x) s' | (x,s') <- parse p s ])
#+END_SRC

where the monadic structure is defined by =bind= and =return=. Given a
value, the =return= function creates a parser that consumes no input
and simply returns the given value. The =>>== function acts as a sequencing
operator for parsers. It takes two parsers and applies the second one
over the remaining inputs of the first one, using the parsed values on
the first parsing as arguments.

An example of primitive *parser* is the =item= parser, which consumes a
character from a non-empty string. It is written in Haskell code as

#+BEGIN_SRC haskell
item :: Parser Char
item = Parser (\s -> case s of 
                       "" -> []
                       (c:s') -> [(c,s')])
#+END_SRC

and an example of *parser combinator* is the =many= function, which
creates a parser that allows one or more applications of the given
parser

#+BEGIN_SRC haskell
many :: Paser a -> Parser [a]
many p = do
  a  <- p
  as <- many p
  return (a:as)
#+END_SRC

in this example =many item= would be a parser consuming all characters
from the input string.

*** Parsec
*Parsec* is a monadic parser combinator Haskell library described in
cite:leijen2001. We have chosen to use it due to its simplicity and
extensive documentation. As we expect to use it to parse user live
input, which will tend to be short, performance is not a critical
concern. A high-performace library supporting incremental parsing,
such as *Attoparsec* cite:attoparsec, would be suitable otherwise.
*** Verbose mode
As we explained previously on the Evaluation section, the simplification
can be analyzed step-by-step. The interpreter allows us to see the
complete evaluation when the =verbose= mode is activated. To activate
it, we can execute =:verbose on= in the interpreter.

The difference can be seen on the following example.

#+BEGIN_EXAMPLE
mikro> plus 1 2
λa.λb.(a (a (a b))) ⇒ 3

mikro> :verbose on
verbose: on
mikro> plus 1 2
((plus 1) 2)
((λλλλ((4 2) ((3 2) 1)) λλ(2 1)) λλ(2 (2 1)))
(λλλ((λλ(2 1) 2) ((3 2) 1)) λλ(2 (2 1)))
λλ((λλ(2 1) 2) ((λλ(2 (2 1)) 2) 1))
λλ(λ(3 1) (λ(3 (3 1)) 1))
λλ(2 (λ(3 (3 1)) 1))
λλ(2 (2 (2 1)))

λa.λb.(a (a (a b))) ⇒ 3
#+END_EXAMPLE

The interpreter output can be colored to show specifically where it
is performing reductions. It is activated by default, but can be deactivated
by executing =:color off=. The following code implements /verbose mode/ in both
cases.

#+BEGIN_SRC haskell
-- | Shows an expression, coloring the next reduction if necessary
showReduction :: Exp -> String
showReduction (Lambda e)         = "λ" ++ showReduction e
showReduction (App (Lambda f) x) = betaColor (App (Lambda f) x)
showReduction (Var e)            = show e
showReduction (App rs x)         = 
  "(" ++ showReduction rs ++ " " ++ showReduction x ++ ")"
showReduction e                  = show e
#+END_SRC

*** SKI mode
Every \lambda-term can be written in terms of SKI combinators.
SKI combinator expressions can be defined as a binary tree having
S, K, and I as possible leafs.

#+BEGIN_SRC haskell
data Ski = S | K | I | Comb Ski Ski
#+END_SRC

The SKI-abstraction and bracket abstraction algorithms are implemented
on Mikrokosmos, and they can be used by activating the /ski mode/ with
=:ski on=. When this mode is activated, every result is written in terms
of SKI combinators.

#+begin_example
mikro> 2
λa.λb.(a (a b)) ⇒ S(S(KS)K)I ⇒ 2

mikro> and
λa.λb.((a b) a) ⇒ SSK ⇒ and
#+end_example

The code implementing these algorithms follows directly from the
theoretical version.

#+BEGIN_SRC haskell
-- | Bracket abstraction of a SKI term, as defined in Hindley-Seldin
-- (2.18).
bracketabs :: String -> Ski -> Ski
bracketabs x (Cte y) = if x == y then I else Comb K (Cte y)
bracketabs x (Comb u (Cte y))
  | freein x u && x == y = u
  | freein x u           = Comb K (Comb u (Cte y))
  | otherwise            = Comb (Comb S (bracketabs x u)) (bracketabs x (Cte y))
bracketabs x (Comb u v)
  | freein x (Comb u v)  = Comb K (Comb u v)
  | otherwise            = Comb (Comb S (bracketabs x u)) (bracketabs x v)
bracketabs _ a           = Comb K a


-- | SKI abstraction of a named lambda term. From a lambda expression
-- creates a SKI equivalent expression. The following algorithm is a
-- version of the algorithm (9.10) on the Hindley-Seldin book.
skiabs :: NamedLambda -> Ski
skiabs (LambdaVariable x)      = Cte x
skiabs (LambdaApplication m n) = Comb (skiabs m) (skiabs n)
skiabs (LambdaAbstraction x m) = bracketabs x (skiabs m)
#+END_SRC
 
** Usage
*** Installation
The complete Mikrokosmos suite is divided in multiple parts:

 1) the *Mikrokosmos interpreter*, written in Haskell;
 2) the *Jupyter kernel*, written in Python;
 3) the *CodeMirror Lexer*, written in Javascript; and
 4) the *Mikrokosmos libraries*, written in the Mikrokosmos language.

These parts will be detailed on the following sections. A system that
already satisfies all dependencies (Stack, Pip and Jupyter), can install
Mikrokosmos using the following script, which is detailed on this section

#+BEGIN_SRC sh
# Mikrokosmos interpreter
stack install mikrokosmos
# Jupyter kernel for Mikrokosmos
sudo pip install imikrokosmos
# Libraries
git clone https://github.com/M42/mikrokosmos-lib.git ~/.mikrokosmos
#+END_SRC

**** Mikrokosmos interpreter :ignore:
The *Mikrokosmos interpreter* is listed in the central Haskell
package archive, /Hackage/ [fn:hackage]. The packaging of Mikrokosmos
has been done using the *cabal* tool; and the configuration of the
package can be read on the file =mikrokosmos.cabal= on the Mikrokosmos
code. As a result, Mikrokosmos can be installed using the *cabal* and
*stack* Haskell package managers. That is,

#+BEGIN_SRC sh
# With cabal
cabal install mikrokosmos
# With stack
stack install mikrokosmos
#+END_SRC

**** Mikrokosmos Jupyter kernel :ignore:
The *Mikrokosmos Jupyter kernel* is listed in the central Python
package archive. Jupyter is a dependency of this kernel, which only
can be used in conjunction with it. It can be installed with the
=pip= package manager as

#+BEGIN_SRC sh
sudo pip install imikrokosmos
#+END_SRC

and the installation can be checked by listing the available Jupyter
kernels with

#+BEGIN_SRC sh
jupyter kernelspec list
#+END_SRC

**** Mikrokosmos libraries :ignore:
The *Mikrokosmos libraries* can be downloaded directly from its GitHub
repository. [fn:mikrokosmoslibgit] They have to be placed under
=~/.mikrokosmos= if we want them to be locally available or under
=/usr/lib/mikrokosmos= if we want them to be globally available.

#+BEGIN_SRC sh
git clone https://github.com/M42/mikrokosmos-lib.git ~/.mikrokosmos
#+END_SRC

**** Complete script :ignore:
The following script installs the complete Mikrokosmos suite on a
fresh system. It has been tested under =Ubuntu 16.04.3 LTS (Xenial
Xerus)=.

#+BEGIN_SRC sh
# 1. Installs Stack, the Haskell package manager
wget -qO- https://get.haskellstack.org | sh
STACK=$(which stack)

# 2. Installs the ncurses library, used by the console interface
sudo apt install libncurses5-dev libncursesw5-dev

# 3. Installs the Mikrokosmos interpreter using Stack
$STACK setup
$STACK install mikrokosmos

# 4. Installs the Mikrokosmos standard libraries
sudo apt install git
git clone https://github.com/M42/mikrokosmos-lib.git ~/.mikrokosmos

# 5. Installs the IMikrokosmos kernel for Jupyter
sudo apt install python3-pip
sudo -H pip install --upgrade pip
sudo -H pip install jupyter
sudo -H pip install imikrokosmos
#+END_SRC

[fn:hackage]: Hackage can be accesed in: http://hackage.haskell.org/
[fn:mikrokosmoslibgit]: The repository can be accessed in: https://github.com/M42/mikrokosmos-lib.git

*** Mikrokosmos interpreter
Once installed, the Mikrokosmos \lambda interpreter can be opened from
the terminal with the =mikrokosmos= command. It will enter a /read-eval-print loop/
where \lambda-expressions and interpreter commands can be evaluated.

#+BEGIN_EXAMPLE
$> mikrokosmos
Welcome to the Mikrokosmos Lambda Interpreter!
Version 0.5.0. GNU General Public License Version 3.
mikro> _
#+END_EXAMPLE

The interpreter evaluates every line as a lambda expression. Examples
on the use of the interpreter can be read on the following
sections. Apart from the evaluation of expressions, the interpreter
accepts the following commands

  * =:quit= and =:restart=, stop the interpreter;
  * =:verbose= activates /verbose mode/;
  * =:ski= activates /SKI mode/;
  * =:types= changes between untyped and simply typed \lambda-calculus;
  * =:color= deactivates colored output;
  * =:load= loads a library.

The Figure [[mikrosession]] is an example session on the mikrokosmos interpreter.

#+caption: Mikrokosmos interpreter session.
#+name: mikrosession
[[./images/mikrosession.png]]

*** Jupyter kernel
The *Jupyter Project* cite:jupyter is an open source project providing
support for interactive scientific computing. Specifically, the
Jupyter Notebook provides a web application for creating interactive
documents with live code and visualizations. 

We have developed a Mikrokosmos kernel for the Jupyter Notebook,
allowing the user to write and execute arbitrary Mikrokosmos code
on this web application. An example session can be seen on Figure
[[jupytersession]].

#+caption: Jupyter notebook Mikrokosmos session.
#+name: jupytersession
[[./images/jupytersession.png]]

The implementation is based on the =pexpect= library for Python.  It
allows direct interaction with any REPL and collects its results.
Specifically, the following Python lines represent the central idea of
this implementation

#+BEGIN_SRC python
# Initialization
mikro = pexpect.spawn('mikrokosmos')
mikro.expect('mikro>')

# Interpreter interaction
# Multiple-line support
output = ""
for line in code.split('\n'):
    # Send code to mikrokosmos
    self.mikro.sendline(line)
    self.mikro.expect('mikro> ')

    # Receive and filter output from mikrokosmos
    partialoutput = self.mikro.before
    partialoutput = partialoutput.decode('utf8')
    output = output + partialoutput
#+END_SRC

A =pip= installable package has been created following the
Python Packaging Authority guidelines. [fn:pypaguide] This allows
the kernel to be installed directly using the =pip= python package manager.

#+BEGIN_SRC bash
sudo -H pip install imikrokosmos
#+END_SRC

[fn:pypaguide]: The PyPA packaging user guide can be found in its official
page: https://packaging.python.org/

*** CodeMirror lexer
*CodeMirror* [fn:codemirror] is a text editor for the browser
implemented in Javascript. It is used internally by the Jupyter
Notebook.

A CodeMirror lexer for Mikrokosmos has been written. It uses
Javascript regular expressions and signals the ocurrence of any kind
of operator to CodeMirror. It enables syntax highlighting for Mikrokosmos
code on Jupyter Notebooks. It comes bundled with the kernel specification
and no additional installation is required.

#+BEGIN_SRC javascript
	CodeMirror.defineSimpleMode("mikrokosmos", {
	    start: [
		// Comments
		{regex: /\#.*/, token: "comment"},
		// Interpreter
		{regex: /\:load|\:verbose|\:ski|\:restart|\:types|\:color/, 
           token: "atom"},
		// Binding
		{regex: /(.*?)(\s*)(=)(\s*)(.*?)$/,
		 token: ["def",null,"operator",null,"variable"]},
		// Operators
		{regex: /[=!]+/, token: "operator"},
	    ],
	    meta: {
		dontIndentStates: ["comment"],
		lineComment: "#"
	    }
	}
#+END_SRC

[fn:codemirror]: Documentation for CodeMirror can be found in its
official page: https://codemirror.net/

*** TODO JupyterHub
# DigitalOcean
# NameCheap
# SSL
# JupyterHub setup
# OAuth
** Programming in the untyped \lambda-calculus
This section explains how to use the untyped \lambda-calculus to
encode data structures and useful data, such as booleans, linked lists,
natural numbers or binary trees. All this is done on pure \lambda-calculus
avoiding the addition of any new syntax or axioms.

This presentation follows the Mikrokosmos tutorial on \lambda-calculus, which
aims to teach how it is possible to program using untyped \lambda-calculus
without discussing technical topics such as those we have discussed on
the chapter on [[*Untyped \lambda-calculus][untyped \lambda-calculus]]. It also follows
the exposition on cite:selinger13 of the usual Church encodings.

All the code on this section is valid Mikrokosmos code.

*** Basic syntax
In the interpreter, \lambda-abstractions are written with the symbol =\=,
representing a \lambda. This is a convention used on some functional languages
such as Haskell or Agda. Any alphanumeric string can be a variable and
can be defined to represent a particular \lambda-term using the === operator.

As a first example, we define the identity function (=id=), function 
composition (=compose=) and a constant function on two arguments which
always returns the first one untouched (=const=).

#+BEGIN_SRC haskell
id = \x.x
compose = \f.\g.\x.f (g x)
const = \x.\y.x
#+END_SRC

Evaluation of terms will be denoted with the ==>= symbol, as in

#+BEGIN_SRC haskell
compose id id
---> λa.a ⇒ id
#+END_SRC

It is important to notice that multiple argument functions are defined as
higher one-argument functions which return another functions as arguments.
These intermediate functions are also valid \lambda-terms. For example

#+BEGIN_SRC haskell
alwaysid = const id
#+END_SRC

is a function that discards one argument and returns the identity =id=.
This way of defining multiple argument functions is called the *currying*
of a function in honor to the american logician Haskell Curry in cite:haskell58.
It is a particular instance of a deeper fact, the *hom-tensor adjunction*
\[
\hom(A \times B, C) \cong \hom(A, \hom(B,C))
\]
or the definition of exponentials.

*** A technique on inductive data encoding
Over this presentation, we will implicitly use a technique on the
majority of our data encodings which allows us to write an encoding
for any algebraically inductive generated data. This technique is
used without comment on cite:selinger13 and is the basic of what is
called the *Church encoding*.

We start considering the usual inductive representation of
the data type with data constructors, as we do when we represent a
syntax with a BNF, for example,
\[ \mathtt{Nat} ::= \mathtt{Zero} \mid \mathtt{Succ}\ \mathtt{Nat}. \]

Or, in general
\[
\mathtt{T} ::= C_1 \mid C_2 \mid C_3 \mid \dots
\]

We do not have any possibility of encoding constructors on
\lambda-calculus. Even if we had, they would have, in theory, no
computational content; the application of constructors would not
be reduced under any \lambda-term, and we would need at least the 
ability to pattern match on the constructors to define functions
on them. The \lambda-calculus would need to be extended with
additional syntax for every new type.

This technique, instead, defines a data term as a function on
multiple variables representing the constructors. In our example, the number $2$, which
would be written as $\mathtt{Succ}(\mathtt{Succ}(\mathtt{Zero}))$, would be encoded as
\[
\lambda s.\ \lambda z.\ s (s (z)).
\]

In general, any instance of the type $\mathtt{T}$ would be encoded as a
\lambda-expression depending on all its constuctors
\[
\lambda c_{1}.\ \lambda c_{2}.\ \lambda c_{3}.\ \dots\ \lambda c_{n}. (\textit{term}).
\]

This acts as the definition of an initial algebra over the
constructors and lets us compute by instantiating this algebra on
particular cases. Particular examples are described on the following
sections.
# Link to categories

*** Booleans
Booleans can be defined as the data generated by a pair of constuctors
\[\mathtt{Bool} ::= \mathtt{True} \mid \mathtt{False}.
\]

Consequently, the Church encoding of booleans takes these constructors as
arguments and defines

#+BEGIN_SRC haskell
true  = \t.\f.t
false = \t.\f.f
#+END_SRC

**** If-else interpretation                                       :ignore:
Note that =true= and =const= are exactly the same term up to
\alpha-conversion. The same thing happens with =false= and =alwaysid=.
The absence of types prevents us to make any effort to discriminate
between these two uses of the same \lambda-term. Another side-effect
of this definition is that our =true= and =false= terms can be interpreted
as binary functions choosing between two arguments, i.e.,

  * $\mathtt{true}(a,b) = a$
  * $\mathtt{false}(a,b) = b$

We can test this interpretation on the interpreter to get

#+BEGIN_SRC haskell
true id const
--- => id

false id const
--- => const
#+END_SRC

This inspires the definition of an =ifelse= combinator as the identity

#+BEGIN_SRC haskell
ifelse = \b.b
(ifelse true) id const
--- => id
(ifelse false) id const
--- => false
#+END_SRC

**** Logic gates                                                  :ignore:
The usual logic gates can be defined profiting from this interpretation
of booleans

#+BEGIN_SRC haskell
and = \p.\q.p q p
or = \p.\q.p p q
not = \b.b false true
xor = \a.\b.a (not b) b
implies = \p.\q.or (not p) q

xor true true
--- => false
#+END_SRC

*** Natural numbers
**** Peano natural numbers :ignore:
Our definition of natural numbers is inspired by the Peano natural numbers.
We use two constructors

 * zero is a natural number, written as Z;
 * the successor of a natural number is a natural number, written as S;

and the BNF we defined when discussing how to [[*A technique on inductive data encoding][encode inductive data]].

#+BEGIN_SRC haskell
0    = \s.\z.z
succ = \n.\s.\z.s (n s z)
#+END_SRC

This definition of =0= is trivial: given a successor function and a
zero, return zero. The successor function seems more complex, but
it uses the same underlying idea: given a number, a successor and a
zero, apply the successor to the interpretation of that number using
the same successor and zero.

We can then name some natural numbers as

#+BEGIN_SRC haskell
1 = succ 0
2 = succ 1
3 = succ 2
4 = succ 3
5 = succ 4
6 = succ 5
...
#+END_SRC

even if we can not define an infinite number of terms as we might wish.

**** Interpretation as higher-order functions :ignore:
The interpretation the natural number $n$ as a higher order function
is a function taking an argument =f= and applying them $n$ times over
the second argument.

#+BEGIN_SRC haskell
5 not true
--- => false
4 not true
--- => true

double = \n.\s.\z.n (compose s s) z
double 3
--- => 6
#+END_SRC

**** Addition and multiplication :ignore:
Addition $n+m$ applies the successor $m$ times to $n$; and multiplication
$nm$ applies the $n\text{-fold}$ application of the successor $m$ times to $0$.

#+BEGIN_SRC haskell
plus = \m.\n.\s.\z.m s (n s z)
mult = \m.\n.\s.\z.m (n s) z

plus 2 1
--- => 3
mult 2 4
--- => 8
#+END_SRC

*** The predecessor function and predicates on numbers
**** TODO Predecessor                                             :ignore:
**** Predicates                                                   :ignore:
From the definition of =pred=, some predicates on numbers can be
defined. The first predicate will be a function distinguishing a
successor from a zero. It will be user later to build more complex
ones. It is built by appliying a =const false= function =n= times to a
true constant. Only if it is applied =0= times, it will return a true
value.

#+BEGIN_SRC haskell
iszero = \n.(n (const false) true)
iszero 0
--- => true
iszero 2
--- => false
#+END_SRC

From this predicate, we can derive predicates on equality and ordering.

#+BEGIN_SRC haskell
leq = \m.\n.(iszero (minus m n))
eq  = \m.\n.(and (leq m n) (leq n m))
#+END_SRC

*** Lists
We would need two constructors to represent a list: a =nil= signaling
the end of the list and a =cons=, joining an element to the head of
the list. An example of list would be
\[\mathtt{cons}\ 1\ (\mathtt{cons}\ 2\ (\mathtt{cons}\ 3\ \mathtt{nil})).\]

Our definition takes those two constructors into account
#+BEGIN_SRC haskell
nil  = \c.\n.n
cons = \h.\t.\c.\n.(c h (t c n))
#+END_SRC
and the interpretation of a list as a higher-order function is its
=fold= function, a function taking a binary operation and an initial
element and appliying the operation repeteadly to every element on
the list.

\[\mathtt{cons}\ 1\ (\mathtt{cons}\ 2\ (\mathtt{cons}\ 3\ \mathtt{nil}))
\overset{fold\ plus\ 0}\longrightarrow 
\mathtt{plus}\ 1\ (\mathtt{plus}\ 2\ (\mathtt{plus}\ 3\ 0)) = 6\]

The =fold= operation and some operations on lists can be defined
explicitly as

#+BEGIN_SRC haskell
fold = \c.\n.\l.(l c n)
sum  = fold plus 0
prod = fold mult 1
all  = fold and true
any  = fold or false
length = foldr (\h.\t.succ t) 0

sum (cons 1 (cons 2 (cons 3 nil)))
--- => 6
all (cons true (cons true (cons true nil)))
--- => true
#+END_SRC

**** Map and filter                                               :ignore:
The two most commonly used particular cases of fold and frequent examples
of the functional programming paradigm are =map= and =filter=.

  - The *map* function applies a function =f= to every element on a
    list.
  - The *filter* function removes the elements of the list that do not
    satisfy a given predicate. It /filters/ the list, leaving only
    elements that satisfy the predicate.

They can be defined as follows.

#+BEGIN_SRC haskell
map    = \f.(fold (\h.\t.cons (f h) t) nil)
filter = \p.(foldr (\h.\t.((p h) (cons h t) t)) nil)
#+END_SRC

On =map=, given a =cons h t=, we return a =cons (f h) t=; and given a
=nil=, we return a =nil=. On =filter=, we use a boolean to decide at
each step whether to return a list with a head or return the tail
ignoring the head.

#+BEGIN_SRC haskell
mylist = cons 1 (cons 2 (cons 3 nil))
sum (map succ mylist)
--- => 9
length (filter (leq 2) mylist)
--- => 2
#+END_SRC

**** TODO The universal properties of fold, map and filter
*** TODO Binary trees
*** TODO Fixed points                                            :noexport:
** Programming in the simply typed \lambda-calculus
This section explains how to use the simply typed \lambda-calculus to
encode compound data structures and proofs on intuitionistic logic.

This presentation of simply typed structures follows the Mikrokosmos tutorial
and the previous sections on [[*Simply typed \lambda-calculus][simply typed \lambda-calculus]].

All the code on this section is valid Mikrokosmos code.

*** Function types and typeable terms
Types can be activated with the commmand =:types on=. If types are activated,
the interpreter will [[*Principal type inference][infer]] the principal type every term before its evaluation.
The type will then be displayed after the result of the computation.

#+attr_latex: :options [Typed terms on Mikrokosmos]
#+begin_exampleth
The following are examples of already defined terms on lambda calculus and
their corresponding types. It is important to notice how our previously
defined booleans have two different types; while our natural numbers will
have all the same type except from zero, whose type is a generalization on
the type of the natural numbers.

#+begin_src haskell
id
---> λa.a ⇒ id, I, ifelse :: A → A

true
false
---> λa.λb.a ⇒ K, true :: A → B → A
---> λa.λb.b ⇒ nil, 0, false :: A → B → B

0
1
2
---> λa.λb.b ⇒ nil, 0, false :: A → B → B
---> λa.λb.(a b) ⇒ 1 :: (A → B) → A → B
---> λa.λb.(a (a b)) ⇒ 2 :: (A → A) → A → A

S
K
---> λa.λb.λc.((a c) (b c)) ⇒ S :: (A → B → C) → (A → B) → A → C
---> λa.λb.a ⇒ K, true :: A → B → A
#+end_src
#+end_exampleth

If a term is found to be non-typeable, Mikrokosmos will output an error
message signaling the fact. In this way, the evaluation of \lambda-terms
which could potentially not terminate is prevented. Only typed \lambda-terms
will be evaluated while the option =:types= is on; this ensures the termination
of every computation on typed terms.

#+attr_latex: :options [Non-typeable terms on Mikrokosmos]
#+begin_exampleth
Fixed point operators are a common example of non typeable terms. Its evaluation
on untyped \lambda-calculus would not terminate; and the type inference algorithm
fails on them.

#+BEGIN_SRC haskell
fix
---> Error: non typeable expression
fix (\f.\n.iszero n 1 (plus (f (pred n)) (f (pred (pred n))))) 3
---> Error: non typeable expression
#+END_SRC

Note that the evaluation of compound \lambda-expressions where the fixpoint
operators appear applied to other terms can terminate, but the terms are
still non typeable.
#+end_exampleth

*** Product, union, unit and void types
Until this point, we have only used the function type. We are working on the
implicational fragment of the STLC we described on the first [[*Typing rules for the simply typed \lambda-calculus][typing rules]].
We are now going to extend the type system in the same sense we [[*Extending the simply typed \lambda-calculus][extended]] the
STLC. The following types are added to the type system

| Type | Name          | Description                       |
|------+---------------+-----------------------------------|
| =→=  | Function type | Functions from a type to another. |
| =×=  | Product type  | Cartesian product of types.       |
| =+=  | Union type    | Disjoint union of types.          |
| =⊤=  | Unit type     | A type with exactly one element.  |
| =⊥=  | Void type     | A type with no elements.          |

And the following typed constructors are added to the language,

| Constructor | Type                              | Description               |
|-------------+-----------------------------------+---------------------------|
| =(-,-)=     | =A → B → A × B=                   | Pair of elements          |
| =fst=       | =(A × B) → A=                     | First projection          |
| =snd=       | =(A × B) → B=                     | Second projection         |
| =inl=       | =A → A + B=                       | First inclusion           |
| =inr=       | =B → A + B=                       | Second inclusion          |
| =caseof=    | =(A + B) → (A → C) → (B → C) → C= | Case analysis of an union |
| =unit=      | =⊤=                               | Unital element            |
| =abort=     | =⊥ → A=                           | Empty function            |
| =absurd=    | =⊥ → ⊥=                           | Particular empty function |

which correspond to the constructors we described on previous
sections. The only new addition is the =absurd= function, which is
only a particular case of =abort= useful when we want to make explicit
that we are deriving an instance of the empty type. This addition will
only make the logical interpretation on the following sections
clearer.

#+attr_latex: :options [Extended STLC on Mikrokosmos]
#+begin_exampleth
The following are examples of typed terms and functions on Mikrokosmos
using the extended typed constructors. The following terms are presented

  * a function swapping pairs, as an example of pair types.
  * two-case analysis of a number, deciding whether to multiply it by two
    or to compute its predecessor.
  * difference between =abort= and =absurd=.
  * example term containing the unit type.

#+BEGIN_SRC haskell
:load types

swap = \m.(snd m,fst m)
swap
---> λa.((SND a),(FST a)) ⇒ swap :: (A × B) → B × A

caseof (inl 1) pred (mult 2)
caseof (inr 1) pred (mult 2)
---> λa.λb.b ⇒ nil, 0, false :: A → B → B
---> λa.λb.(a (a b)) ⇒ 2 :: (A → A) → A → A

\x.((abort x),(absurd x))
---> λa.((ABORT a),(ABSURD a)) :: ⊥ → A × ⊥
#+END_SRC

Now it is possible to define a new encoding of the booleans with an
uniform type. The type =⊤ + ⊤= has two inhabitants, =inl ⊤= and =inr
⊤=; and they can be used by case analysis.

#+BEGIN_SRC haskell
btrue = inl unit
bfalse = inr unit
bnot = \a.caseof a (\a.bfalse) (\a.btrue)
bnot btrue
---> (INR UNIT) ⇒ bfalse :: A + ⊤
bnot bfalse
---> (INL UNIT) ⇒ btrue :: ⊤ + A
#+END_SRC
#+end_exampleth

With these extended types, Mikrokosmos can be used as a proof checker on
first-order intuitionistic logic by virtue of the Curry-Howard
correspondence.

*** A proof on intuitionistic logic
Under the logical interpretation of Mikrokosmos, we can transcribe proofs in
intuitionistic logic to \lambda-terms and check them on the interpreter.

#+begin_theorem
In intuitionistic logic, the double negation of the LEM holds for every
proposition, that is,
\[
\forall A\colon \neg \neg (A \lor \neg A)
\]
#+end_theorem
#+begin_proof
Suppose $\neg (A \lor \neg A)$. We are going to prove first that, under this
specific assumption, $\neg A$ holds. If $A$ were true, $A \lor \neg A$ would be true and we
would arrive to a contradition, so $\neg A$. But then, if we have $\neg A$ we also have
$A \lor \neg A$ and we arrive to a contradiction with the assumption. We should conclude
that $\neg \neg (A \lor \neg A)$.
#+end_proof

Note that this is, in fact, an intuitionistic proof. Although it seems
to use the intuitionistically forbidden technique of proving by
contradiction, it is actually only proving a negation.  There is a
difference between assuming $A$ to prove $\neg A$ and assuming $\neg
A$ to prove $A$: the first one is simply a proof of a negation, the
second one uses implicitly the law of excluded middle.

This can be translated to the Mikrokosmos implementation of simply typed \lambda-calculus
as the term

#+BEGIN_SRC haskell
notnotlem = \f.absurd (f (inr (\a.f (inl a))))
notnotlem
---> λa.(ABSURD (a (INR λb.(a (INL b))))) :: ((A + (A → ⊥)) → ⊥) → ⊥
#+END_SRC

whose type is precisely $\mathtt{((A + (A \to \bot)) \to \bot) \to \bot}$.

*** TODO Type algebra
**** Lists, trees and generating functions
**** Derivatives and one-hole contexts
**** Seven trees in one
* Categorical logic (abstract)                                       :ignore:
#+LATEX: \ctparttext{\color{black}\begin{center}
This section is based on cite:maclane94.

Topos theory arises independently with Grothendieck and sheaf theory,
Lawvere and the axiomatization of set theory and Paul Cohen with the
forcing techniques with allowed to construct new models of ZFC.
#+LATEX: \end{center}}

* Categorical logic
** Lawvere algebraic theories
# I should join the two first sections and exaplain the first definition
# only as an introduction.

*** Algebraic theories
**** First definition of algebraic theory                         :ignore:
#+ATTR_LATEX: :options [Algebraic theory]
#+BEGIN_definition
An *algebraic theory* is given by

  * a /signature/, a family of sets $\left\{ \Sigma_k \right\}_{k \in \mathbb{N}}$ whose elements are called
    *k-ary operations*. The *terms* of a signature are defined inductively
    as variables or k-ary operations applied to k-tuples of terms.

  * a set of *axioms*, which are equations between terms.
#+END_definition

# Example: groups
**** Interpretations of algebraic theories                        :ignore:
#+ATTR_LATEX: :options [Interpretation of an algebraic theory]
#+BEGIN_definition
An *interpretation* $I$ of a theory $\mathbb{A}$ on a category ${\cal C}$ is given by

  * an object on the category, $I\mathbb{A} \in {\cal C}$;
  * a morphism $If \colon (I\mathbb{A})^k \to I\mathbb{A}$ for every k-ary operation.
#+END_definition

Given an interpretation of the theory we can given an interpretation
to every term of the signature on a given variable context. A term $t$
can be given in the variable context $x_1,\dots,x_n$ if all variables that
appear on $t$ are among the variables of the context; we write that as
\[
x_1,x_2,\dots,x_n \mid t.
\]

Given $I$, the *interpretation of a term* $x_1,\dots,x_n \mid t$ is a morphism
$It \colon (I\mathbb{A})^n \to I\mathbb{A}$ defined inductively as

  * $I(x_i) = \pi_i : (I\mathbb{A})^n \to I\mathbb{A}$, the i-th projection.

  * $I(f\pair{t_1,\dots,t_k}) = If \circ \pair{It_1,\dots,It_k} \colon (I\mathbb{A})^n \to \mathbb{A}$, 
    where $\pair{It_1,\dots,It_k} \colon (I\mathbb{A})^n \to (I\mathbb{A})^n$ is the componentwise 
    interpretation of subterms.

Note that the interpretation of variables depends on the context.
An interpretation *satisfies* an equation $u=v$ in a context if
$Iu = Iv$.

**** Models of algebraic theories                                 :ignore:
#+ATTR_LATEX: :options [Model of an algebraic theory]
#+BEGIN_definition
A *model of an algebraic theory* is an interpretation that satisfies
all the axioms of the theory.
#+END_definition

*** Algebraic theories as categories
**** TODO Notion of representation-free theories                  :ignore:
# This notion is not representation free
# Example
# Category of an algebraic theory

**** Second definition of algebraic theory                        :ignore:
#+ATTR_LATEX: :options [Lawvere algebraic theory]
#+BEGIN_definition
An *algebraic theory* is a category with finite products and objects
forming a sequence $A^0,A^1,A^2,\dots$ such that $A^m \times A^n = A^{m+n}$ for
any $m,n$.
#+END_definition

From this definition, it follows that $A^0$ must be the terminal object.

# This notion corresponds with the previous one.

**** Models as functors                                           :ignore:
Every model $M$ in the sense of Definition [TODO] can be seen as a functor
from the category of $\mathbb{A}$ to a given category ${\cal C}$; defined on 
objects as 
\[ 
M[x_1,\dots,x_n] = (M\mathbb{A})^{k}
\]
and inductively defined on morphisms as

  * $M\pair{x_i} = \pi_i \colon (M\mathbb{A})^k \to M\mathbb{{A}}$, for any morphism $\pair{x_i}$;
  * $M\pair{t_1,\dots,t_m} = \pair{Mt_1,\dots,Mt_m} \colon (M\mathbb{A})^m \to M\mathbb{A}$, the 
    componentwise interpretation of subterms;
  * $M\pair{f\pair{t_1,\dots,t_m}} = Mf \circ \pair{Mt_1,\dots,Mt_{m}} \colon (M\mathbb{A})^m\to M\mathbb{A}$.

The fact that $M$ is a well-defined functor follows from the assumption
that it is a model.

#+ATTR_LATEX: :options [Model]
#+BEGIN_definition
A *model* of an algebraic theory $\mathbb{A}$ in a category ${\cal C}$ is a functor
$M \colon \mathbb{A} \to {\cal C}$ preserving all finite products.
#+END_definition

#+ATTR_LATEX: :options [Category of models of a theory]
#+BEGIN_definition
The *category of models* $\mathtt{Mod}_{{\cal C}}(\mathbb{A})$ is is the full subcategory of functor category
${\cal C}^{\mathbb{A}}$ given by the functors preserving all finite products.
#+END_definition

*** Completeness for algebraic theories
#+ATTR_LATEX: :options [Completeness for algebraic theories]
#+BEGIN_theorem
Given $\mathbb{A}$ an algebraic theory, there exists a category ${\cal A}$ 
with a model $U \in \mathtt{Mod}_{{\cal A}}(\mathbb{A})$ such that, for every terms $u,v$,
\[
U \text{ satisfies } u = v 
\iff
\mathbb{A} \text{ proves } u = v 
\] 
this is called the *universal model* for $\mathbb{A}$. That is,
categorical semantics of algebraic theories are complete.
#+END_theorem
#+BEGIN_proof
# TODO
#+END_proof

The universal model needs not to be set-theoretic, but we can
always find a universal model in a presheaf category via the
Yoneda embedding.

#+ATTR_LATEX: :options [Yoneda embedding as a universal model]
#+BEGIN_proposition
The Yoneda embedding $y \colon \mathbb{A} \to \widehat{\mathbb{A}}$ is a universal model for $\mathbb{A}$.
#+END_proposition
#+BEGIN_proof
# TODO
#+END_proof
** Heyting algebras
In this section, we develop the notion of a *Heyting algebra* and show
its differences with a Boolean algebra.

There is a correlation between classical propositional calculus and
the Boolean algebra of the subsets of a given set. If we interpret
a proposition $p$ as a subset of a given universal set $P \subset U$ and
fix an element $u \in U$, propositions can be translated to $u \in P$, 
logical connectives can be translated as

\[\begin{tabular}{cc|cc}
logic &  & &  subsets \\
\hline
$P \land Q$ & and & intersection & $P \cap Q$  \\
$P \lor Q$ & or & union  & $P \cup Q$ \\
$\neg P$ & not & complement & $\overline{P}$ \\
$P \to Q$ & implication & complement union & $\overline{P} \cup Q$
\end{tabular}\]

using crucially that $\neg P \land Q \equiv P \to Q$.

In the same way that Boolean algebras correspond to classical
propositional logic, Heyting algebras correspond to intuitionistic
propositional calculus. Its model on a set-like theory is not the subsets
of a given set, but instead, only the /open/ sets of a given
topological space

\[\begin{tabular}{cc|cc}
logic &  & & open sets \\
\hline
$P \land Q$ & and & intersection & $P \cap Q$  \\
$P \lor Q$ & or & union  & $P \cup Q$ \\
$\neg P$ & not & interior of the complement & $\mathrm{int}\left(\overline{P}\right)$ \\
$P \to Q$ & implication & interior of complement and consequent & $\mathrm{int}(\overline{P} \cup Q)$
\end{tabular}\]

where $\mathrm{int}$ is the topological interior of a set.
# TODO: This definition of the implication does not match with the used
# on Moerdijk. It is equivalent but different.

*** Boolean algebras
**** Lattices, definition                                         :ignore:
#+attr_latex: :options [Lattice]
#+begin_definition
A *lattice* is a partially ordered set with all binary products and
coproducts. It is a *bounded lattice* if it has all binary products
and coproducts.
#+end_definition

We will usually work with bounded lattices.
A bounded lattice can also be defined as a set with $0,1$ and two binary
operations $\land,\lor$ satisfying

  * $1 \land x = x$, and $0 \lor x = x$;
  * $x \land x = x$, and $x \lor x = x$;
  * $x \land (y \lor x) = x = (x \land y) \lor x$.

This perspective allows us also to define a lattice object in any
category as an object $L$ with morphisms

\[
\land \colon L \times L \to L,
\quad
\lor \colon L \times L \to L,
\quad
0,1 \colon \mathrm{I} \to L,
\]

where $\mathrm{I}$ is the terminal object of the category; and commutative
diagrams encoding the previous equations.

**** Distributive lattices                                        :ignore:
#+attr_latex: :options [Distributive lattice]
#+begin_definition
A *distributive lattice* is a lattice where
\[
x \land (y \lor z) = (x \land y) \lor (x \land z),
\]

holds for all $x,y,z$.
#+end_definition
# TODO: This implies the dual distributive law

**** Complements                                                  :ignore:
#+attr_latex: :options [Complement]
#+begin_definition
A *complement* of $a$ in a bounded lattice is an element $\overline{a}$ such that
\[
a \land \overline{a} = 0, \qquad
a \lor \overline{a} = 1.
\]
#+end_definition

#+attr_latex: :options [The complement in distributive lattices is unique]
#+begin_proposition
If a complement of an element exists in a distributive lattice, it is unique.
#+end_proposition
# TODO: Commutativity is needed on this proof, but it is not stated elsewhere
#+begin_proof
Given $a$ with two complements $x,y$, we have that
\[
x = x \land (a \lor y) = (x \land a) \lor (x \land y) =
(y \land a) \lor (x \land y) = y \lor (x \land a) = y.
\]
#+end_proof

**** Boolean algebras                                             :ignore:
#+attr_latex: :options [Boolean algebra]
#+begin_definition
A *Boolean algebra* is a distributive bounded lattice in which every element
has a complement.
#+end_definition

Boolean algebras satisfy certain known properties such as the DeMorgan laws
and the double negation elimination rule.

# TODO: Every Boolean algebra is the algebra of subsets of some set.
# This is a result by M.H.Stone, cited in pag 50 Moerdijk.

*** Heyting algebras
**** Definition of Heyting algebras                               :ignore:
#+attr_latex: :options [Heyting algebra]
#+begin_definition
A *Heyting algebra*, also called *Brouwerian lattice*, is a bounded lattice
which is cartesian closed as a category; i.e., for every pair of elements
$x,y$, the exponential $y^x$ exists.
#+end_definition

The exponential in Heyting algebras is usually written as $x \Rightarrow y$ and is 
characterized by its adjunction with the product
\[
z \leq (x \Rightarrow y) \text{ if and only if } z \land x \leq y.
\]

# TODO: Any complete distributive lattice is a Heyting algebra

**** Boolean algebras are Heyting algebras                        :ignore:
#+attr_latex: :options [Boolean algebras are Heyting algebras]
#+begin_proposition
Every Boolean algebra is a Heyting algebra with exponentials given by
\[
(x \Rightarrow y) = \overline{x} \land y.
\]
#+end_proposition
#+begin_proof
We will prove that
\[
z \leq (\overline{x} \lor y) \text{ if and only if } z \land x \leq y.
\]
If $z \leq (\overline{x} \lor y)$,
\[
z \land x \leq
(\overline{x} \lor y) \land x \leq
(\overline{x} \land x) \lor (y \land x) \leq
y \land x \leq
y;
\]
and if $z \land x \leq y$,
\[
z = 
z \land 1 =
z \land (\overline{x} \lor x) =
(z \land \overline{x}) \lor (z \land x) \leq 
(z \land \overline{x}) \lor y \leq
z \lor y.
\]
#+end_proof

**** Negation on Heyting algebras                                 :ignore:
#+attr_latex: :options [Negation]
#+begin_definition
The *negation* of $x$ in a Heyting algebra is defined as
\[
\neg x = (x \Rightarrow 0).
\]
#+end_definition

**** Properties of a Heyting algebra                              :ignore:
#+begin_proposition
In any Heyting algebra,

  1) $x \leq \neg \neg x$,
  2) $x \leq y$ implies $\neg y \leq \neg x$,
  3) $\neg x = \neg\neg\neg x$,
  4) $\neg\neg (x \land y) = \neg \neg x \land \neg \neg y$,
  5) $(x \impl x) = 1$,
  6) $x \land (x \impl y) = x \land y$,
  7) $y \land (x \impl y) = y$,
  8) $x \impl (y \land z) = (x \impl y) \land (x \impl z)$.

Any bounded lattice $L$ with an operation satisfying the last four properties
is a Heyting algebra with this operation as implication.
#+end_proposition
#+begin_proof
We can prove the inequalities using the definition of implication.

  1) By definition, $x \land (x \Rightarrow \bot) \leq \bot$.
  2) Again, by definition, $\neg y \land x \leq \neg y \land y \leq \bot$.
  3) Is a consequence of the first two inequalities.
  4) We know that $x \land y \leq x,y$, and therefore $\neg\neg (x\land y) \leq \neg \neg x \land \neg \neg y$.
     We can prove $\neg\neg x \land \neg\neg y \leq \neg\neg (x \land y)$ using the definition of
     negation to get $\neg\neg x \land \neg\neg y \land \neg (x \land y) \leq \bot$, and then by reversing
     the definition of implication $\neg\neg y \land \neg (x \land y) \leq \neg\neg\neg x = x$. Applying
     the same reasoning to $y$, we finally get $x \land y \land \neg (x \land y) \leq \bot$.
  5) Follows from $x \land 1 \leq x$.
  6) Using the evaluation morphism, we know that $x \land (x \impl y) \leq y \leq x \land y$.
  7) Using the definition of implication $y = y \land y \leq y \land (x \Rightarrow y)$.
  8) The exponential $x \Rightarrow -$ is a right adjoint and it preserves products.
#+end_proof

**** Characterization of Boolean algebras                         :ignore:
#+attr_latex: :options [Complements are negations in Heyting algebras]
#+begin_proposition
If an element has a complement on a Heyting algebra, it must be $\neg x$.
#+end_proposition
#+begin_proof
Let $a$ a complement of $x$. By definition, $x \land a = \bot$ and therefore $a \leq \neg x$.
The reverse inequality can be proven using the lattice properties as
\[
\neg x = \neg x \land (x \lor a) = \neg x \land a.
\]
#+end_proof

#+attr_latex: :options [Characterization of Boolean algebras]
#+begin_proposition
A Heyting algebra is Boolean if and only if $\neg\neg x = x$ for every $x$; 
and if and only if $x \lor \neg x = 1$ for every $x$.
#+end_proposition
#+begin_proof
In a Boolean algebra the complement is unique and $\neg\neg x = x$. Now, 
if $\neg\neg y = y$ for every $y$,
\[
x \lor \neg x = \neg\neg (x \lor \neg x) = \neg (\neg x \land \neg\neg x) = \top;
\]
and then, as $x \lor \neg x = \bot$, $\neg x$ must be the complement of $x$. We have
used the fact that $\neg (x \lor y) = (\neg x) \land (\neg y)$ in any Heyting algebra.
#+end_proof

*** Quantifiers as adjoints
#+begin_definition
Given a relation between sets $S \subseteq X \times Y$, the functors $\forall_p, \exists_p \colon {\cal P}(X \times Y) \to {\cal P}(Y)$
are defined as

  * $\forall_p S = \left\{ y \mid \forall x: \pair{x,y} \in S \right\}$, and
  * $\exists_p S = \left\{ y \mid \exists x: \pair{x,y} \in S \right\}$.
#+end_definition

#+begin_theorem
The functors $\exists_p,\forall_p$ are the left and right adjoints to the inverse image of the
projection functor, $p^{\ast} \colon {\cal P}(Y) \to {\cal P}(X \times Y)$.
#+end_theorem
#+begin_proof
# TODO: Proof
#+end_proof

#+begin_theorem
Given any function on sets $f \colon Z \to Y$, the inverse image functor $f^{\ast} \colon {\cal P}Y \to {\cal P}Z$
has left and right adjoints, called $\exists_f$ and $\forall_f$.
#+end_theorem
** Cartesian closed categories
*** Cartesian category
#+ATTR_LATEX: :options [Cartesian category]
#+BEGIN_definition
A *cartesian category* is a category with all finite products.
#+END_definition
#+ATTR_LATEX: :options [Cartesian closed category]
#+BEGIN_definition
A *cartesian closed category* is a category with all finite products
and exponentials.
#+END_definition

The definition of cartesian closed category can be written in terms of
existence of adjoints.

#+ATTR_LATEX: :options [Cartesian closed categories and adjoints]
#+BEGIN_proposition
Any category ${\cal C}$ is cartesian closed if and only if there exist
right adjoints for the following functors

  * $! \colon {\cal C} \to 1$, the unique functor to the terminal category;
  * $\Delta \colon {\cal C} \to {\cal C} \times {\cal C}$, the diagonal functor;
  * $(- \times A) \colon {\cal C} \to {\cal C}$, the product functor, for each $A \in {\cal C}$.
#+END_proposition
#+BEGIN_proof
# TODO
#+END_proof

*** Frames
#+ATTR_LATEX: :options [Completeness for posets]
#+BEGIN_proposition
Complete posets are cocomplete. Cocomplete posets are complete.
#+END_proposition
#+BEGIN_proof
# TODO
#+END_proof

#+ATTR_LATEX: :options [Frames]
#+BEGIN_definition
*Frames* are complete cartesian closed posets.
#+END_definition

** TODO Models of \lambda-calculus
*** TODO Types and sets
# From the Hott book
*** TODO Models
# From Selinger
*** TODO Continuous partial orderings
# From Selinger
** Topoi
*** Subobject classifier
#+attr_latex: :options [Subobject classifier]
#+begin_definition
A *subobject classifier* is an object $\Omega$ with a monomorphism $\mathrm{true} \colon 1 \to \Omega$
such that, for every monomorphism $S \to X$, there exists a unique $\phi$ such that
\[\begin{tikzcd}
S\rar{} \dar[swap]{} & 1 \dar{\mathrm{true}} \\
X\rar[dashed]{\phi} & \Omega
\end{tikzcd}\]
forms a pullback square.
#+end_definition

*** Definition of a topos
#+attr_latex: :options [Topos]
#+begin_definition
An *elementary topos* (plural /topoi/) is a cartesian closed category
with all finite limits and a subobject classifier.
#+end_definition
* Type theory
# to conduct my thoughts in an orderly fashion, by commencing with those
# objects that are simplest and easiest to know, in order to ascend
# little by little, as by degrees, to the knowledge of the most
# composite things, and by supposing an order even among those things
# that do not naturally precede one another.

** Intuitionistic logic
*** Constructive mathematics
*** Agda example
In intuitionistic logic, the double negation of the LEM holds for every
proposition, that is,

\[
\forall A\colon \neg \neg (A \lor \neg A)
\]

**** Machine proof
#+latex: \ExecuteMetaData[latex/Ctlc.tex]{id}

** TODO Martin-Löf Type Theory, dependent types
*** Type in type
# Russell's paradox code
# Girard paradox in agda: https://github.com/nzl-nott/PhD-of-nzl/blob/master/Exercise/Ex4/girard.agda
# Hurkens' paradox: https://github.com/agda/agda/blob/master/test/Succeed/Hurkens.agda
# Hurkens: https://www.cs.cmu.edu/~kw/scans/hurkens95tlca.pdf
# Burali-Forti paradox

** TODO Type theory as a foundation of mathematics
** TODO Constructive mathematics
*** TODO Axiom of choice implies excluded middle
# In Agda or Coq!?
** TODO Homotopy Type Theory
* Conclusions
* Appendices
bibliographystyle:alpha
bibliography:Bibliography.bib

** TODO Mikrokosmos complete code
** TODO Mikrokosmos user's guide
** TODO Quotes
# These quotes can be placed on the partial abstracts or in the general one.
