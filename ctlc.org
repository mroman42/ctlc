#+TITLE: Category Theory and Lambda Calculus
#+AUTHOR: Mario Román
#+OPTIONS: broken-links:mark toc:t tasks:nil num:3
#+SETUPFILE: ctlc.setup
#+LATEX_HEADER_EXTRA: \usepackage[conor]{agda}
#+LATEX_HEADER_EXTRA: \usepackage{catchfilebetweentags}
#+LATEX_HEADER_EXTRA: % \input{titlepage}


** Abstract
:PROPERTIES:
:UNNUMBERED: t
:END:

This is the abstract. It should not be written until the end.

** Acknowlegments                                                 :noexport:
This document has been written with Emacs26 and org-mode 8, using the
=org= file format and LaTeX as intermediate format. The document follows
the =classicthesis= [[http://www.latextemplates.com/templates/theses/2/thesis_2.pdf][template]] by André Miede. The =minted= package
has been used for code listings.

* Category theory
** Categories
*** Definition of category
#+begin_definition
A *category* ${\cal C}$, as defined in cite:maclane78, is given by

 * ${\cal C}_0$, a collection[fn:collection] whose elements are called *objets*, and
 * ${\cal C}_1$, a collection whose elements are called *morphisms*.

Every morphism $f \in {\cal C}_1$ has two objects assigned: a
*domain*, written as $\mathrm{dom}(f) \in {\cal C}_0$, and a
*codominio*, written as $\mathrm{cod}(f) \in {\cal C}_0$; a common
notation for such morphism is

\[
f \colon \mathrm{dom}(f) \to \mathrm{cod}(f).
\]

Given two morphisms $f \colon A \to B$ and $g \colon B \to C$ there
exists a *composition morphism*, written as $g \circ f \colon A \to C$. 
Morphism composition is a binary associative operation with
identity elements $\mathrm{id}_{A}\colon A \to A$, that is

\[
h \circ (g \circ f) = (h \circ g) \circ f
\quad\text{ and }\quad
f \circ \mathrm{id}_A = f = \mathrm{id}_B \circ f.
\]
#+end_definition

[fn:collection]: We use the term /collection/ to denote
some unspecified formal notion of compilation of "things" that could
be given by sets or proper classes. We will want to define categories
whose objects are all the possible sets and we will need the objects
to form a proper class.
* Lambda calculus
** Untyped \lambda-calculus
The *\lambda-calculus* is a collection of formal systems, all of them based
on the lambda notation discovered by Alonzo Church in the 1930s while trying
to develop a foundational notion of function on mathematics.

The *untyped* or *pure lambda calculus* is, syntactically, the simplest of those
formal systems. This presentation of the untyped lambda calculus will follow
cite:Hindley08 and cite:selinger13.

*** Definition
#+begin_definition
The *\lambda-terms* are defined inductively as

  * every /variable/, taken from an infinite and numerable set ${\cal V}$ of
    variables, and usually written as lowercase single letters
    (x,y,z,...), is a \lambda-term.

  * given two \lambda-terms $M,N$; its /application/, $MN$ is a \lambda-term.

  * given a \lambda-term $M$ and a variable $x$, its /abstraction/, $\lambda x.M$
    is a lambda term.

They can be also defined by the following BNF
\[ \mathtt{Exp} ::= x \mid (\mathtt{Exp}\ \mathtt{Exp}) \mid (\lambda x.\mathtt{Exp})
\]
where $x \in {\cal V}$ is any variable.
#+end_definition

By convention, we omit outermost parentheses and assume
left-associativity, i.e., $MNP$ will mean $(MN)P$. Multiple
\lambda-abstractions can be also contracted to a single multivariate
abstraction; thus $\lambda x.\lambda y.M$ can become $\lambda x,y.M$.

*** Free and bound variables, substitution
Any ocurrence of a variable $x$ inside the /scope/ of a lambda is said to be
bound; and any not bound variable is said to be free. We can define formally
the set of free variables as follows.

#+begin_definition
The *set of free variables* of a term $M$ is defined inductively as
\[\begin{aligned}
FV(x) &= \{x\}, \\
FV(MN) &= FV(M) \cup FV(N), \\
FV(\lambda x.M) &= FV(M) \setminus \{x\}.
\end{aligned}\]
#+end_definition

A free ocurrence of a variable can be substituted by a term. This should
be done avoiding the unintended bounding of free variables which happens
when a variable is substituted inside of the scope of a binder with the
same name, as in the following example, where we substitute $y$ by $(\lambda z.xz)$
on $(\lambda x.yx)$ and the second free variable $x$ gets bounded by the first binder

\[ (\lambda x.yx) \overset{y \mapsto (\lambda z.xz)}\longrightarrow (\lambda x.(\lambda z.xz)x).
\]

To avoid this, the $x$ should be renamed before the substitution.

#+begin_definition
The *substitution* of a variable $x$ by a term $N$ on $M$ is defined
inductively as
\[\begin{aligned}
x[N/x] &\equiv N,\\
y[N/x] &\equiv y,\\
(MP)[N/x] &\equiv (M[N/x])(P[N/x]),\\
(\lambda x.P)[N/x] &\equiv \lambda x.P,\\
(\lambda y.P)[N/x] &\equiv \lambda y.P[N/x] & \text{ if } y \notin FV(N), \\
(\lambda y.P)[N/x] &\equiv \lambda z.P[z/y][N/x] & \text{ if } y \in FV(N);
\end{aligned}\]

where, in the last clause, $z$ is a fresh unused variable.
#+end_definition

We could define a criterion for choosing exactly what this new
variable should be, or simply accept that our definition will not be
well-defined, but well-defined up to a change on the name of the
variables. This equivalence relation will be defined formally on the
next section. In practice, it is common to follow the 
/Barendregt's variable convention/ which simply assumes that bound 
variables have been renamed to be distinct.

*** \alpha-equivalence
#+begin_definition
*\alpha-equivalence* is the smallest relation $=_{\alpha}$ on
\lambda-terms which is an equivalence relation, i.e.,

  * it is /reflexive/, $M =_{\alpha} M$;
  * it is /symmetric/, if $M =_{\alpha} N$, then $N =_{\alpha} M$;
  * and it is /transitive/, if $M=_{\alpha}N$ and $N=_{\alpha}P$, then $M=_{\alpha}P$;

and it is compatible with the structure of lambda terms,

  * if $M =_{\alpha} M'$ and $N =_{\alpha} N'$, then $MN =_{\alpha}M'N'$;
  * if $M=_{\alpha}M'$, then $\lambda x.M =_{\alpha} \lambda x.M'$;
  * if $y$ does not appear on $M$, $\lambda x.M =_{\alpha} \lambda y.M[y/x]$.
#+end_definition

\alpha-equivalence formally captures the fact that the name of a bound
variable can be changed without changing the properties of the term. This
idea appears recurrently on mathematics; the renaming of the variable of
integration is an example of \alpha-equivalence.

\[
\int_0^1 x^2\ dx = \int_0^1 y^2\ dy
\]

*** \beta-reduction
The core idea of evaluation in \lambda-calculus is captured by the notion
of \beta-reduction.

#+begin_definition
<<def-betared>>
The *single-step \beta-reduction* is the smallest relation on \lambda-terms
capturing the notion of evaluation 
\[(\lambda x.M)N \to_{\beta}M[N/x],\]

and some congruence rules that preserve the structure of
\lambda-terms, such as

  * $M \to_{\beta} M'$ implies $MN \to_{\beta} M'N$ and $MN \to_{\beta} MN'$;
  * $M \to_{\beta}M'$ implies $\lambda x.M \to_{\beta} \lambda x.M'$.

The reflexive transitive closure of $\to_{\beta}$ is written as $\twoheadrightarrow_{\beta}$. The symmetric
closure of $\twoheadrightarrow_{\beta}$ is called *\beta-equivalence* and written as $=_{\beta}$ or simply $=$.
#+end_definition

*** \eta-reduction
The idea of function extensionality in \lambda-calculus is captured by the
notion of \eta-reduction. Function extensionality implies the equality of
any two terms that define the same function over any argument.

#+begin_definition
The \eta-reduction is the smallest relation on \lambda-terms satisfiying the
same congruence rules as \beta-reduction and the following axiom

\[
\lambda x.Mx \to_{\eta} M,\text{ for any } x \notin \mathrm{FV}(M).
\]
#+end_definition

We define single-step \beta\eta-reduction as the union of \beta-reduction
and \eta-reduction. This will be written as $\to_{\beta\eta}$, and its reflexive transitive
closure will be $\tto_{\beta\eta}$.

*** Confluence
#+begin_definition
A relation $\to$ is *confluent* if, given its reflexive transitive closure
$\tto$, $M \tto N$ and $M \tto P$ imply the existence of some $Z$ such that
$N \tto Z$ and $P \tto Z$.
#+end_definition

Given any binary relation $\to$ of which $\tto$ is its reflexive transitive
closure, we can consider three seemingly related properties

  * the *confluence* or Church-Rosser property we have just defined.
  * the *quasidiamond property*, which assumes $M \to N$ and $M \to P$.
  * the *diamond property*, which is defined substituting $\tto$ by $\to$ on
    the definition on confluence.

Diagrammatically, the three properties can be represented as

\[\begin{tikzcd}[column sep=small]
& 
M \drar[two heads]\dlar[two heads] &&& 
M \drar\dlar &&& 
M \drar\dlar &\\
N \drar[dashed,two heads] && 
P \dlar[dashed,two heads] & 
N \drar[dashed,two heads] &&
P \dlar[dashed,two heads] &
N \drar[dashed] && 
P \dlar[dashed] \\& 
Z &&&
Z &&&
Z &\\
\end{tikzcd}\]

and the implication relation between them is that the diamond relation
implies confluence; while the quasidiamond does not. Both claims are
easy to prove, and they show us that, in order to prove confluence for
a given relation, we need to prove the diamond property instead of try
to prove it from the quasidiamond property, as a naive attempt of proof
would try.

The statement of $\tto_{\beta}$ and $\tto_{\beta\eta}$ being confluent is what we are going to
call the Church-Rosser theorem. The definition of a relation satisfying
the diamond property and whose reflexive transitive closure is $\tto_{\beta\eta}$ will
be the core of our proof.

*** The Church-Rosser theorem
The proof presented here is due to Tait and Per Martin-Löf; an earlier
but more convoluted proof was discovered by Alonzo Church and Barkley 
Rosser in 1935. It is based on the idea of parallel one-step reduction.

#+attr_latex: :options [Parallel one-step reduction]
#+begin_definition
We define the *parallel one-step reduction* relation, $\rhd$ as the smallest
relation satisfying that, assuming $P \rhd P'$ and $N \rhd N'$, the following
properties of

  * reflexivity, $x \rhd x$;
  * parallel application, $PN \rhd P'N'$;
  * congruence to \lambda-abstraction, $\lambda x.N \rhd \lambda x.N'$;
  * parallel substitution, $(\lambda x.P)N \rhd P'[N'/x]$;
  * and extensionality, $\lambda x.P x \rhd P'$, if $x \not\in \mathrm{FV}(P)$,

hold.
#+end_definition

Using the first three rules, it is trivial to show that this relation
is in fact reflexive.

#+begin_lemma
<<lemma-transclosureparallel>>
The reflexive transitive closure of $\rhd$ is $\tto_{\beta\eta}$.
In particular, given any $M,M'$,

  1) if $M \to_{\beta\eta} M'$, then $M \rhd M'$.
  2) if $M \rhd M'$, then $M \tto_{\beta\eta} M'$;
#+end_lemma
#+begin_proof
  1) We can prove this by exhaustion and structural induction on
     \lambda-terms, the possible ways in which we arrive at $M \to M'$
     are

     * $(\lambda x.M)N \to M[N/x]$; where we know that, by parallel substitution
       and reflexivity $(\lambda x.M)N \rhd M[N/x]$.

     * $MN \to M'N$ and $NM \to NM'$; where we know that, by
       induction $M \rhd M'$, and by parallel application and reflexivity, $MN \rhd M'N$
       and $NM \rhd NM'$.

     * congruence to \lambda-abstraction, which is a shared property between
       the two relations where we can apply structural induction again.

     * $\lambda x. Px \to P$, where $x \not\in \mathrm{FV}(P)$ and we can apply extensionality for $\rhd$
       and reflexivity.

  2) We can prove this by induction on any derivation of $M \rhd M'$. The
     possible ways in which we arrive at this are
     
     * the trivial one, reflexivity.

     * parallel application $NP \rhd N'P'$, where, by induction, we have $P \tto P'$ 
       and $N \tto N'$. Using two steps, $NP \tto N'P \tto N'P'$ we prove $NP \tto N'P'$.

     * congruence to \lambda-abstraction $\lambda x.N \rhd \lambda x.N'$, where, by induction,
       we know that $N \tto N'$, so $\lambda x.N \tto \lambda x.N'$.

     * parallel substitution, $(\lambda x.P)N \rhd P'[N'/x]$, where, by induction,
       we know that $P \tto P'$ and $N\tto N'$. Using multiple steps,
       $(\lambda x.P)N \tto (\lambda x.P')N \tto (\lambda x.P')N' \to P'[N'/x]$.

     * extensionality, $\lambda x.P x \rhd P'$, where by induction $P \tto P'$, and trivially,
       $\lambda x.Px \tto \lambda x.P'x$.

Because of this, the reflexive transitive closure of $\rhd$ should be a subset and a
superset of $\tto$ at the same time.
#+end_proof

#+attr_latex: :options [Substitution Lemma]
#+begin_lemma
<<lemma-subsl>>
Assuming $M \rhd M'$ and $U \rhd U'$, $M[U/y] \rhd M'[U'/y]$.
#+end_lemma
#+begin_proof
We apply structural induction on derivations of $M \rhd M'$, depending
on what the last rule we used to derive it was.

  * Reflexivity, $M = x$. If $x=y$, we simply use $U \rhd U'$; if $x \neq y$,
    we use reflexivity on $x$ to get $x \rhd x$.

  * Parallel application. By induction hypothesis, $P[U/y] \rhd P'[U'/y]$ and
    $N[U/y]\rhd N'[U'/y]$, hence $(PN)[U/y] \rhd (P'N')[U'/y]$.

  * Congruence. By induction, $N[U/y] \rhd N'[U'/y]$ and $\lambda x.N[U/y] \rhd \lambda x.N'[U'/y]$.

  * Parallel substitution. By induction, $P[U/y] \rhd P'[U'/y]$ and $N[U/y] \rhd N[U'/y]$,
    hence $((\lambda x.P)N)[U/y] \rhd P'[U'/y][N'[U'/y]/x] = P'[N'/x][U'/y]$.

  * Extensionality, given $x \notin \mathrm{FV}(P)$. By induction, $P \rhd P'$, hence
    $\lambda x.P[U/y]x \rhd P'[U'/y]$.

Note that we are implicitely assuming the Barendregt's variable convention; all
variables have been renamed to avoid clashes.
#+end_proof

#+attr_latex: :options [Maximal parallel one-step reduct]
#+begin_definition 
The *maximal parallel one-step reduct* $M^{\ast}$ of a \lambda-term $M$ is defined
inductively as

  * $x^{\ast} = x$;
  * $(PN)^{\ast} = P^{\ast}N^{\ast}$;
  * $((\lambda x.P)N)^{\ast} = P^{\ast}[N^{\ast}/x]$;
  * $(\lambda x.N)^{\ast} = \lambda x.N^{\ast}$;
  * $(\lambda x.Px)^{\ast} = P^{\ast}$, given $x \notin \mathrm{FV}(P)$.
#+end_definition

#+attr_latex: :options [Diamond property of parallel reduction]
#+begin_lemma
<<lemma-paralleldiamond>>
Given any $M'$ such that $M \rhd M'$, $M' \rhd M^{\ast}$. Parallel one-step reduction 
has the diamond property.
#+end_lemma
#+begin_proof
We apply again structural induction on the derivation of $M \rhd M'$.

  * Reflexivity gives us $M' = x = M^{\ast}$.

  * Parallel application. By induction, we have $P \rhd P^\ast$ and $N \rhd N^{\ast}$; depending
    on the form of $P$, we have

    - $P$ is not a \lambda-abstraction and $P'N' \rhd P^{\ast}N^{\ast} = (PN)^{\ast}$.

    - $P = \lambda x.Q$ and $P \rhd P'$ could be derived using congruence to \lambda-abstraction
      or extensionality. On the first case we know by induction hypothesis that $Q'\rhd Q^{\ast}$
      and $(\lambda x.Q')N' \rhd Q^{\ast}[N^{\ast}/x]$. On the second case, we can take $P = \lambda x.Rx$, where,
      $R \rhd R'$. By induction, $(R'x) \rhd (Rx)^{\ast}$ and now we apply the substitution lemma
      to have $R'N' = (R'x)[N'/x] \rhd (Rx)^{\ast}[N^{\ast}/x]$.

  * Congruence. Given $N \rhd N'$; by induction $N' \rhd N^{\ast}$, and depending on the form of
    $N$ we have two cases

    - $N$ is not of the form $Px$ where $x \not\in \mathrm{FV}(P)$; we can apply congruence to 
      \lambda-abstraction.

    - $N = Px$ where $x \notin \mathrm{FV}(P)$; and $N \rhd N'$ could be derived by parallel application
      or parallel substitution. On the first case, given $P \rhd P'$, we know that $P' \rhd P^{\ast}$
      by induction hypothesis and $\lambda x.P'x \rhd P^{\ast}$ by extensionality. On the second case,
      $N = (\lambda y.Q)x$ and $N' = Q'[x/y]$, where $Q \rhd Q'$. Hence $P \rhd \lambda y.Q'$, and by
      induction hypothesis, $\lambda y.Q' \rhd P^{\ast}$.

  * Parallel substitution, with $N \rhd N'$ and $Q \rhd Q'$; we know that $M^{\ast} = Q^{\ast}[N^{\ast}/x]$
    and we can apply the substitution lemma (lemma [[lemma-subsl]]) to get $M' \rhd M^{\ast}$.

  * Extensionality. We know that $P \rhd P'$ and $x \notin \mathrm{FV}(P)$, so by induction hypothesis
    we know that $P' \rhd P^{\ast} = M^{\ast}$.
#+end_proof

#+attr_latex: :options [Church-Rosser Theorem]
#+begin_theorem
The relation $\tto_{\beta\eta}$ is confluent.
#+end_theorem
#+begin_proof
Parallel reduction, $\rhd$, satisfies the diamond property (lemma [[lemma-paralleldiamond]]), 
which implies the Church-Rosser property. Its reflexive transitive closure is $\tto_{\beta\eta}$
(lemma [[lemma-transclosureparallel]]),
whose diamond property implies confluence for $\to_{\beta\eta}$.
#+end_proof

*** TODO Normalization                                           :noexport:
*** TODO Evaluation, call by name and call by value              :noexport:
*** TODO SKI combinators                                         :noexport:
** Simply typed lambda calculus
We will give now a presentation of the *simply-typed lambda calculus*
based on cite:selinger13.

*** TODO Types and sets
# From the Hott book

*** Simple types
# Are basic types necessary?

We start assuming that a set of *basic types* exists. Those basic
types would correspond, in a programming language interpretation, with
things like the type of strings or the type of integers. We will also
assume that a *unit* type, $1$ exists; the unit type will have only
one inhabitant.

#+begin_definition
The set of *simple types* is given by the following Backus-Naur form
\[\mathtt{Type} ::= 
1 \mid
\iota \mid 
\mathtt{Type} \to \mathtt{Type} \mid
\mathtt{Type} \times \mathtt{Type} \]

where $1$ is a one-element type and $\iota$ is any /basic type/.
#+end_definition

That is to say that, for every two types $A,B$, there exist a *function type*
$A \to B$ and a *pair type* $A \times B$.

*** Raw typed lambda terms
We will now define the terms of the typed lambda calculus. 

#+begin_definition
The set of *typed lambda terms* is given by the BNF
\[ \mathtt{Term} ::=
\ast \mid
x \mid
\mathtt{Term}\mathtt{Term} \mid
\lambda x^{\mathtt{Type}}. \mathtt{Term} \mid
\left\langle \mathtt{Term},\mathtt{Term} \right\rangle \mid
\pi_1 \mathtt{Term} \mid
\pi_2\mathtt{Term}
\]
#+end_definition

Besides the previously considered term application and a special
element $\ast$ which will be the unique inhabitant of the type $1$; we
now introduce a typed lambda abstraction and an explicit construction
of the pair element with its projections.

*** Typing rules for the simply-typed lambda calculus
The set of raw typed lambda terms contains some meaningless terms
under our type interpretation, such as $\pi_1(\lambda x^A.M)$. *Typing rules*
will give them the desired semantics; only a subset of these raw
lambda terms will be typeable.

#+begin_definition
A *typing context* is a sequence of typing assumptions
$x_1:A_1,\dots,x_n:A_n$, where no variable appears more than once.
#+end_definition

Every typing rule assumes a typing context, usually denoted by $\Gamma$ 
or by a concatenation of typing contexts written as $\Gamma,\Gamma'$; and 
a consequence from that context, separated by the $\vdash$ symbol.

 1) The type of $\ast$ is $1$, the rule $(\ast)$ builds this element.

   \begin{prooftree}
   \LeftLabel{($\ast$)}
   \AxiomC{}
   \UnaryInfC{$\Gamma \vdash \ast : 1$}
   \end{prooftree}

 2) The $(var)$ rule simply makes explicit the type of a variable from
    the context.

    \begin{prooftree}
    \LeftLabel{($var$)}
    \AxiomC{}
    \UnaryInfC{$\Gamma, x:A \vdash x:A$}
    \end{prooftree}

 3) The $(pair)$ rule allow us to build pairs by their components. It acts
   as a constructor of pairs.

   \begin{prooftree}
   \LeftLabel{$(pair)$}
   \AxiomC{$\Gamma \vdash a : A$}
   \AxiomC{$\Gamma \vdash b : B$}
   \BinaryInfC{$\Gamma \vdash \left\langle a,b \right\rangle : A \times B$}
   \end{prooftree}

 4) The $(\pi_1)$ and $(\pi_2)$ rules give the semantics of a product
    with two projections to the pair terms. If we have a pair $m : A \times B$, then
    $\pi_1m : A$ and $\pi_2m : B$. They act as two different destructors of pairs.

    \begin{prooftree}
    \LeftLabel{($\pi_1$)}
    \AxiomC{$\Gamma \vdash m : A \times B$}
    \UnaryInfC{$\Gamma \vdash \pi_1m : A$}
    \LeftLabel{($\pi_2$)}
    \AxiomC{$\Gamma \vdash m : A \times B$}
    \UnaryInfC{$\Gamma \vdash \pi_2m : B$}
    \noLine\BinaryInfC{}
    \end{prooftree}

 5) The $(abs)$ introduces a well-typed lambda abstraction. If we have a
    $h : B$ term depending on $x : A$, we can create a lambda abstraction
    from this term. It acts as a constructor of function terms.

    \begin{prooftree}
    \LeftLabel{($abs$)}
    \AxiomC{$\Gamma, x : A \vdash h : B$}
    \UnaryInfC{$\Gamma \vdash \lambda x^A.h : A \to B$}
    \end{prooftree}

 6) The $(app)$ rule gives the type of a well-typed application of a
    lambda term. A term $f : A \to B$ applied to a term $a : A$ is a term
    of type $B$. It acts as a destructor of function terms.

    \begin{prooftree}
    \LeftLabel{$(app)$}
    \AxiomC{$\Gamma \vdash f : A \to B$}
    \AxiomC{$\Gamma \vdash a : A$}
    \BinaryInfC{$\Gamma \vdash f a : B$}
    \end{prooftree}

#+begin_definition
A term is *typable* if we can assign types to all its variables in
such a way that a typing judgment for the type is derivable.
#+end_definition

# Examples of typable and non-typable terms.

# It is easy to check if a term is typable because there is only
# one way to type it.

# If we want to derive term bottom-up, there is only one possible
# choice at each step. Has this to do with the natural deduction
# properties?

*** TODO Natural deduction
# Every derivation in natural deduction is exactly a lambda term.

** TODO Hindley-Milner
** TODO System F                                                  :noexport:
*** TODO System F is strongly normalizing
* Mikrokosmos (abstract)                                             :ignore:
#+LATEX: \ctparttext{\color{black}\begin{center}
We have developed *Mikrokosmos*, a lambda calculus interpreter
written in the purely functional programming language Haskell cite:hudak07_haskell.
It aims to provide students with a tool to learn and understand lambda calculus.
#+LATEX: \end{center}}

* Mikrokosmos
** Lambda expressions
*** De Bruijn indexes
Nicolaas Govert *De Bruijn* proposed in cite:debruijn81 a way of defining \lambda-terms modulo
\alpha-conversion based on indices.  The main idea of De Bruijn
indices is to remove all variables from binders and replace every
variable on the body of an expression with a number, called /index/,
representing the number of \lambda-abstractions in scope between the
ocurrence and its binder.

Consider the following example, the \lambda-term
\[ \lambda x.(\lambda y.\ y (\lambda z.\ y z)) (\lambda t.\lambda z.\ t x)
\]
can be written with de Bruijn indices as
\[
\lambda\ (\lambda(1 \lambda(2 1))\ \lambda\lambda(2 3)\ ).
\]

De Bruijn also proposed a notation for the \lambda-calculus
changing the order of binders and \lambda-applications.  A review on
the syntax of this notation, its advantages and De Bruijn indexes, can be found in
cite:kamareddine01. In this section, we are going to describe De Bruijn
indexes but preserve the usual notation of \lambda-terms; that is, /De Bruijn/
/indexes/ and /De Bruijn notation/ are different concepts and we are going to
use only the former.

#+attr_latex: :options [De Bruijn indexed terms]
#+begin_definition
We define recursively the set of \lambda-terms using de Bruijn notation
following this BNF
\[ \mathtt{Exp} ::= \mathbb{N}
 \mid (\lambda\ \mathtt{Exp})
 \mid (\mathtt{Exp}\ \mathtt{Exp})
\]
#+end_definition

Our internal definition closely matches the formal one. The names of
the constructors here are =Var=, =Lambda= and =App=:

#+BEGIN_SRC haskell
-- | A lambda expression using DeBruijn indexes.
data Exp = Var Integer -- ^ integer indexing the variable.
         | Lambda Exp  -- ^ lambda abstraction
         | App Exp Exp -- ^ function application
         deriving (Eq, Ord)
#+END_SRC

This notation avoids the need for the Barendregt's variable convention and
the \alpha-reductions. It will be useful to implement \lambda-calculus without
having to worry about the specific names of variables.

*** Substitution
We define the [[*Free and bound variables, substitution][substitution]] operation needed for the [[*\beta-reduction][\beta-reduction]] on
de Bruijn indices. In order to define the substitution of the n-th
variable by a \lambda-term $P$ on a given term, we must

 * find all the ocurrences of the variable. At each level of scope
   we are looking for the successor of the number we were looking
   for before.

 * decrease the higher variables to reflect the disappearance of
   a lambda.

 * replace the ocurrences of the variables by the new term, taking
   into account that free variables must be increased to avoid them
   getting captured by the outermost lambda terms. 

In our code, we apply =subs= to any expression. When it is applied to
a \lambda-abstraction, the index and the free variables of the
replaced term are increased with =incrementFreeVars=; whenever it is
applied to a variable, the previous cases are taken into consideration.

#+BEGIN_SRC haskell
-- | Substitutes an index for a lambda expression
subs :: Integer -> Exp -> Exp -> Exp
subs n p (Lambda e) = Lambda (subs (n+1) (incrementFreeVars 0 p) e)
subs n p (App f g)  = App (subs n p f) (subs n p g)
subs n p (Var m)
  | n == m    = p         -- The lambda is replaced directly  
  | n <  m    = Var (m-1) -- A more exterior lambda decreases a number
  | otherwise = Var m     -- An unrelated variable remains untouched
#+END_SRC

Then \beta-reduction can be then defined using this =subs= function.

#+BEGIN_SRC haskell
betared :: Exp -> Exp
betared (App (Lambda e) x) = substitute 1 x e
betared e = e
#+END_SRC

*** De Bruijn-terms and \lambda-terms
The internal language of the interpreter uses de Bruijn expressions,
while the user interacts with it using lambda expressions with alphanumeric
variables. Our definition of a \lambda-expression with variables will be
used in parsing and output formatting.

#+BEGIN_SRC haskell
data NamedLambda = LambdaVariable String                    
                 | LambdaAbstraction String NamedLambda     
                 | LambdaApplication NamedLambda NamedLambda
#+END_SRC

The translation from a natural \lambda-expression to de Bruijn notation
is done using a dictionary which keeps track of the bounded variables

#+BEGIN_SRC haskell
tobruijn :: Map.Map String Integer -- ^ names of the variables used
         -> Context                -- ^ names already binded on the scope
         -> NamedLambda            -- ^ initial expression
         -> Exp
-- Every lambda abstraction is inserted in the variable dictionary,
-- and every number in the dictionary increases to reflect we are entering
-- into a deeper context.
tobruijn d context (LambdaAbstraction c e) = 
     Lambda $ tobruijn newdict context e
        where newdict = Map.insert c 1 (Map.map succ d)

-- Translation of applications is trivial.
tobruijn d context (LambdaApplication f g) = 
     App (tobruijn d context f) (tobruijn d context g)

-- We look for every variable on the local dictionary and the current scope.
tobruijn d context (LambdaVariable c) =
  case Map.lookup c d of
    Just n  -> Var n
    Nothing -> fromMaybe (Var 0) (MultiBimap.lookupR c context)
#+END_SRC

while the translation from a de Bruijn expression to a natural one is done
considering an infinite list of possible variable names and keeping a list
of currently-on-scope variables to name the indices.

#+BEGIN_SRC haskell
-- | An infinite list of all possible variable names 
-- in lexicographical order.
variableNames :: [String]
variableNames = concatMap (`replicateM` ['a'..'z']) [1..]

-- | A function translating a deBruijn expression into a 
-- natural lambda expression.
nameIndexes :: [String] -> [String] -> Exp -> NamedLambda
nameIndexes _    _   (Var 0) = LambdaVariable "undefined"
nameIndexes used _   (Var n) = LambdaVariable (used !! pred (fromInteger n))
nameIndexes used new (Lambda e) = 
  LambdaAbstraction (head new) (nameIndexes (head new:used) (tail new) e)
nameIndexes used new (App f g) = 
  LambdaApplication (nameIndexes used new f) (nameIndexes used new g)
#+END_SRC

*** TODO Evaluation
** Output formatting :noexport:
*** Verbose mode
*** SKI mode
** Haskell                                                        :noexport:
** Parsing
*** Monadic parser combinators
A common approach to building parsers in functional programming is to
model parsers as functions. Higher-order functions on parsers act as
/combinators/, which are used to implement complex parsers in a
modular way from a set of primitive ones. In this setting, parsers
exhibit a monad algebraic structure, which can be used to simplify
the combination of parsers. A technical report on *monadic parser combinators*
can be found on cite:hutton96.

The use of monads for parsing is discussed firstly in cite:Wadler85,
and later in cite:Wadler90 and cite:hutton98. The parser type is
defined as a function taking a =String= and returning a list of pairs,
representing a successful parse each. The first component of the pair
is the parsed value and the second component is the remaining
input. The Haskell code for this definition is

#+BEGIN_SRC haskell
newtype Parser a = Parser (String -> [(a,String)])

parse :: Parser a -> String -> [(a,String)]
parse (Parser p) = p

instance Monad Parser where
  return x = Parser (\s -> [(x,s)])
  p >>= q  = Parser (\s -> 
               concat [parse (q x) s' | (x,s') <- parse p s ])
#+END_SRC

where the monadic structure is defined by =bind= and =return=. Given a
value, the =return= function creates a monad that consumes no input
and simply returns the given value. The =>>== function acts as a sequencing
operator for parsers. It takes two parsers and applies the second one
over the remaining inputs of the first one, using the parsed values on
the first parsing as arguments.

An example of primitive *parser* is the =item= parser, which consumes a
character from a non-empty string. It is written in Haskell code as

#+BEGIN_SRC haskell
item :: Parser Char
item = Parser (\s -> case s of 
                       "" -> []
                       (c:s') -> [(c,s')])
#+END_SRC

and an example of *parser combinator* is the =many= function, which
allows one or more applications of the parser given as an argument

#+BEGIN_SRC haskell
many :: Paser a -> Parser [a]
many p = do
  a  <- p
  as <- many p
  return (a:as)
#+END_SRC

in this example =many item= would be a parser consuming all characters
from the input string.

*** Parsec
*Parsec* is a monadic parser combinator Haskell library described in
cite:leijen2001. We have chosen to use it due to its simplicity and
extensive documentation. As we expect to use it to parse user live
input, which will tend to be short, performance is not a critical
concern. A high-performace library supporting incremental parsing,
such as *Attoparsec* cite:attoparsec, would be suitable otherwise.
 
** Usage
*** Mikrokosmos interpreter :noexport:
*** Jupyter kernel
The *Jupyter Project* cite:jupyter is an open source project providing
support for interactive scientific computing. Specifically, the
Jupyter Notebook provides a web application for creating interactive
documents with live code and visualizations. 

We have developed a Mikrokosmos kernel for the Jupyter Notebook,
allowing the user to write and execute arbitrary Mikrokosmos code
on this web application.

# Image of the mikrokosmos jupyter notebook

*** CodeMirror lexer
** TODO Programming on the untyped \lambda-calculus               :noexport:
# Lecture notes on lambda calculus chapter 3
*** TODO Church encoding                                         :noexport:
* Type theory                                                     
** Intuitionistic logic
*** Constructive mathematics
*** The double negation of LEM is provable
In intuitionistic logic, the double negation of the LEM holds for every
proposition, that is,

\[
\forall A\colon \neg \neg (A \vee \neg A)
\]

**** Proof
Suppose $\neg (A \vee \neg A)$. We firstly are going to prove that, under this
specific assumption, $\neg A$ holds. If $A$ were true, $A \vee \neg A$ would be true and we
would arrive to a contradition, so $\neg A$. But then, if we have $\neg A$ we also have
$A \vee \neg A$ and we arrive to a contradiction with the assumption. We should conclude
that $\neg \neg (A \vee \neg A)$.

**** Machine proof
#+latex: \ExecuteMetaData[latex/Ctlc.tex]{id}

** TODO Propositions as types
** TODO Martin-Löf Type Theory
** TODO Type theory as a foundation of mathematics
** TODO Constructive mathematics
*** TODO Proof by contradiction and proof of a negation
# They are fundamentally different
#
# [[https://www.youtube.com/watch?v=21qPOReu4FI][Five stages of accepting constructive mathematics]]
*** TODO Axiom of choice implies excluded middle
# In Agda or Coq!?
** TODO Homotopy Type Theory
* Conclusions
bibliographystyle:alpha
bibliography:Bibliography.bib
