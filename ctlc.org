#+TITLE: Category Theory and Lambda Calculus
#+AUTHOR: Mario Román
#+OPTIONS: broken-links:mark toc:t tasks:nil num:3
#+SETUPFILE: ctlc.setup
#+LATEX_HEADER_EXTRA: \usepackage[conor]{agda}
#+LATEX_HEADER_EXTRA: \usepackage{catchfilebetweentags}
#+LATEX_HEADER_EXTRA: % \input{titlepage}


** Abstract
:PROPERTIES:
:UNNUMBERED: t
:END:

This is the abstract. It should not be written until the end.

** Acknowlegments                                                 :noexport:
This document has been written with Emacs26 and org-mode 9, using the
=org= file format and LaTeX as intermediate format. The document follows
the =classicthesis= [[http://www.latextemplates.com/templates/theses/2/thesis_2.pdf][template]] by André Miede. The =minted= package
has been used for code listings.
* Category theory
** Categories
*** Definition of category
#+begin_definition
A *category* ${\cal C}$, as defined in cite:maclane78, is given by

 * ${\cal C}_0$, a collection[fn:collection] whose elements are called *objets*, and
 * ${\cal C}_1$, a collection whose elements are called *morphisms*.

Every morphism $f \in {\cal C}_1$ has two objects assigned: a
*domain*, written as $\mathrm{dom}(f) \in {\cal C}_0$, and a
*codomain*, written as $\mathrm{cod}(f) \in {\cal C}_0$; a common
notation for such morphism is
\[
f \colon \mathrm{dom}(f) \to \mathrm{cod}(f).
\]

Given two morphisms $f \colon A \to B$ and $g \colon B \to C$ there
exists a *composition morphism*, written as $g \circ f \colon A \to C$. 
Morphism composition is a binary associative operation with
identity elements $\mathrm{id}_{A}\colon A \to A$, that is
\[
h \circ (g \circ f) = (h \circ g) \circ f
\quad\text{ and }\quad
f \circ \mathrm{id}_A = f = \mathrm{id}_B \circ f.
\]
#+end_definition

[fn:collection]: We use the term /collection/ to denote
some unspecified formal notion of compilation of "things" that could
be given by sets or proper classes. We will want to define categories
whose objects are all the possible sets and we will need the objects
to form a proper class.
* Lambda calculus
** Untyped \lambda-calculus
The *\lambda-calculus* is a collection of formal systems, all of them based
on the lambda notation discovered by Alonzo Church in the 1930s while trying
to develop a foundational notion of function on mathematics.

The *untyped* or *pure lambda calculus* is, syntactically, the simplest of those
formal systems. This presentation of the untyped lambda calculus will follow
cite:Hindley08 and cite:selinger13.

*** Definition
#+begin_definition
The *\lambda-terms* are defined inductively as

  * every /variable/, taken from an infinite and numerable set ${\cal V}$ of
    variables, and usually written as lowercase single letters
    (x,y,z,...), is a \lambda-term.

  * given two \lambda-terms $M,N$; its /application/, $MN$ is a \lambda-term.

  * given a \lambda-term $M$ and a variable $x$, its /abstraction/, $\lambda x.M$
    is a lambda term.

They can be also defined by the following BNF
\[ \mathtt{Exp} ::= x \mid (\mathtt{Exp}\ \mathtt{Exp}) \mid (\lambda x.\mathtt{Exp})
\]
where $x \in {\cal V}$ is any variable.
#+end_definition

By convention, we omit outermost parentheses and assume
left-associativity, i.e., $MNP$ will mean $(MN)P$. Multiple
\lambda-abstractions can be also contracted to a single multivariate
abstraction; thus $\lambda x.\lambda y.M$ can become $\lambda x,y.M$.

*** Free and bound variables, substitution
Any ocurrence of a variable $x$ inside the /scope/ of a lambda is said to be
bound; and any not bound variable is said to be free. We can define formally
the set of free variables as follows.

#+begin_definition
The *set of free variables* of a term $M$ is defined inductively as
\[\begin{aligned}
FV(x) &= \{x\}, \\
FV(MN) &= FV(M) \cup FV(N), \\
FV(\lambda x.M) &= FV(M) \setminus \{x\}.
\end{aligned}\]
#+end_definition

A free ocurrence of a variable can be substituted by a term. This should
be done avoiding the unintended bounding of free variables which happens
when a variable is substituted inside of the scope of a binder with the
same name, as in the following example, where we substitute $y$ by $(\lambda z.xz)$
on $(\lambda x.yx)$ and the second free variable $x$ gets bounded by the first binder
\[
(\lambda x.yx) \overset{y \mapsto (\lambda z.xz)}\longrightarrow (\lambda x.(\lambda z.xz)x).
\]

To avoid this, the $x$ should be renamed before the substitution.

#+begin_definition
The *substitution* of a variable $x$ by a term $N$ on $M$ is defined
inductively as
\[\begin{aligned}
x[N/x] &\equiv N,\\
y[N/x] &\equiv y,\\
(MP)[N/x] &\equiv (M[N/x])(P[N/x]),\\
(\lambda x.P)[N/x] &\equiv \lambda x.P,\\
(\lambda y.P)[N/x] &\equiv \lambda y.P[N/x] & \text{ if } y \notin FV(N), \\
(\lambda y.P)[N/x] &\equiv \lambda z.P[z/y][N/x] & \text{ if } y \in FV(N);
\end{aligned}\]

where, in the last clause, $z$ is a fresh unused variable.
#+end_definition

We could define a criterion for choosing exactly what this new
variable should be, or simply accept that our definition will not be
well-defined, but well-defined up to a change on the name of the
variables. This equivalence relation will be defined formally on the
next section. In practice, it is common to follow the 
/Barendregt's variable convention/ which simply assumes that bound 
variables have been renamed to be distinct.

*** \alpha-equivalence
#+begin_definition
*\alpha-equivalence* is the smallest relation $=_{\alpha}$ on
\lambda-terms which is an equivalence relation, i.e.,

  * it is /reflexive/, $M =_{\alpha} M$;
  * it is /symmetric/, if $M =_{\alpha} N$, then $N =_{\alpha} M$;
  * and it is /transitive/, if $M=_{\alpha}N$ and $N=_{\alpha}P$, then $M=_{\alpha}P$;

and it is compatible with the structure of lambda terms,

  * if $M =_{\alpha} M'$ and $N =_{\alpha} N'$, then $MN =_{\alpha}M'N'$;
  * if $M=_{\alpha}M'$, then $\lambda x.M =_{\alpha} \lambda x.M'$;
  * if $y$ does not appear on $M$, $\lambda x.M =_{\alpha} \lambda y.M[y/x]$.
#+end_definition

\alpha-equivalence formally captures the fact that the name of a bound
variable can be changed without changing the properties of the term. This
idea appears recurrently on mathematics; e.g., the renaming of the variable of
integration is an example of \alpha-equivalence.
\[
\int_0^1 x^2\ dx = \int_0^1 y^2\ dy
\]

*** \beta-reduction
The core idea of evaluation in \lambda-calculus is captured by the notion
of \beta-reduction.

#+begin_definition
<<def-betared>>
The *single-step \beta-reduction* is the smallest relation on \lambda-terms
capturing the notion of evaluation 
\[(\lambda x.M)N \to_{\beta}M[N/x],\]

and some congruence rules that preserve the structure of
\lambda-terms, such as

  * $M \to_{\beta} M'$ implies $MN \to_{\beta} M'N$ and $MN \to_{\beta} MN'$;
  * $M \to_{\beta}M'$ implies $\lambda x.M \to_{\beta} \lambda x.M'$.

The reflexive transitive closure of $\to_{\beta}$ is written as $\twoheadrightarrow_{\beta}$. The symmetric
closure of $\twoheadrightarrow_{\beta}$ is called *\beta-equivalence* and written as $=_{\beta}$ or simply $=$.
#+end_definition

*** \eta-reduction
The idea of function extensionality in \lambda-calculus is captured by the
notion of \eta-reduction. Function extensionality implies the equality of
any two terms that define the same function over any argument.

#+begin_definition
The \eta-reduction is the smallest relation on \lambda-terms satisfiying the
same congruence rules as \beta-reduction and the following axiom
\[
\lambda x.Mx \to_{\eta} M,\text{ for any } x \notin \mathrm{FV}(M).
\]
#+end_definition

We define single-step \beta\eta-reduction as the union of \beta-reduction
and \eta-reduction. This will be written as $\to_{\beta\eta}$, and its reflexive transitive
closure will be $\tto_{\beta\eta}$.

*** Confluence
#+begin_definition
A relation $\to$ is *confluent* if, given its reflexive transitive closure
$\tto$, $M \tto N$ and $M \tto P$ imply the existence of some $Z$ such that
$N \tto Z$ and $P \tto Z$.
#+end_definition

Given any binary relation $\to$ of which $\tto$ is its reflexive transitive
closure, we can consider three seemingly related properties

  * the *confluence* or Church-Rosser property we have just defined.
  * the *quasidiamond property*, which assumes $M \to N$ and $M \to P$.
  * the *diamond property*, which is defined substituting $\tto$ by $\to$ on
    the definition on confluence.

Diagrammatically, the three properties can be represented as

\[\begin{tikzcd}[column sep=small]
& 
M \drar[two heads]\dlar[two heads] &&& 
M \drar\dlar &&& 
M \drar\dlar &\\
N \drar[dashed,two heads] && 
P \dlar[dashed,two heads] & 
N \drar[dashed,two heads] &&
P \dlar[dashed,two heads] &
N \drar[dashed] && 
P \dlar[dashed] \\& 
Z &&&
Z &&&
Z &\\
\end{tikzcd}\]

and the implication relation between them is that the diamond relation
implies confluence; while the quasidiamond does not. Both claims are
easy to prove, and they show us that, in order to prove confluence for
a given relation, we need to prove the diamond property instead of try
to prove it from the quasidiamond property, as a naive attempt of proof
would try.

The statement of $\tto_{\beta}$ and $\tto_{\beta\eta}$ being confluent is what we are going to
call the Church-Rosser theorem. The definition of a relation satisfying
the diamond property and whose reflexive transitive closure is $\tto_{\beta\eta}$ will
be the core of our proof.

*** The Church-Rosser theorem
The proof presented here is due to Tait and Per Martin-Löf; an earlier
but more convoluted proof was discovered by Alonzo Church and Barkley 
Rosser in 1935. It is based on the idea of parallel one-step reduction.

#+attr_latex: :options [Parallel one-step reduction]
#+begin_definition
We define the *parallel one-step reduction* relation, $\rhd$ as the smallest
relation satisfying that, assuming $P \rhd P'$ and $N \rhd N'$, the following
properties of

  * reflexivity, $x \rhd x$;
  * parallel application, $PN \rhd P'N'$;
  * congruence to \lambda-abstraction, $\lambda x.N \rhd \lambda x.N'$;
  * parallel substitution, $(\lambda x.P)N \rhd P'[N'/x]$;
  * and extensionality, $\lambda x.P x \rhd P'$, if $x \not\in \mathrm{FV}(P)$,

hold.
#+end_definition

Using the first three rules, it is trivial to show that this relation
is in fact reflexive.

#+begin_lemma
<<lemma-transclosureparallel>>
The reflexive transitive closure of $\rhd$ is $\tto_{\beta\eta}$.
In particular, given any $M,M'$,

  1) if $M \to_{\beta\eta} M'$, then $M \rhd M'$.
  2) if $M \rhd M'$, then $M \tto_{\beta\eta} M'$;
#+end_lemma
#+begin_proof
  1) We can prove this by exhaustion and structural induction on
     \lambda-terms, the possible ways in which we arrive at $M \to M'$
     are

     * $(\lambda x.M)N \to M[N/x]$; where we know that, by parallel substitution
       and reflexivity $(\lambda x.M)N \rhd M[N/x]$.

     * $MN \to M'N$ and $NM \to NM'$; where we know that, by
       induction $M \rhd M'$, and by parallel application and reflexivity, $MN \rhd M'N$
       and $NM \rhd NM'$.

     * congruence to \lambda-abstraction, which is a shared property between
       the two relations where we can apply structural induction again.

     * $\lambda x. Px \to P$, where $x \not\in \mathrm{FV}(P)$ and we can apply extensionality for $\rhd$
       and reflexivity.

  2) We can prove this by induction on any derivation of $M \rhd M'$. The
     possible ways in which we arrive at this are
     
     * the trivial one, reflexivity.

     * parallel application $NP \rhd N'P'$, where, by induction, we have $P \tto P'$ 
       and $N \tto N'$. Using two steps, $NP \tto N'P \tto N'P'$ we prove $NP \tto N'P'$.

     * congruence to \lambda-abstraction $\lambda x.N \rhd \lambda x.N'$, where, by induction,
       we know that $N \tto N'$, so $\lambda x.N \tto \lambda x.N'$.

     * parallel substitution, $(\lambda x.P)N \rhd P'[N'/x]$, where, by induction,
       we know that $P \tto P'$ and $N\tto N'$. Using multiple steps,
       $(\lambda x.P)N \tto (\lambda x.P')N \tto (\lambda x.P')N' \to P'[N'/x]$.

     * extensionality, $\lambda x.P x \rhd P'$, where by induction $P \tto P'$, and trivially,
       $\lambda x.Px \tto \lambda x.P'x$.

Because of this, the reflexive transitive closure of $\rhd$ should be a subset and a
superset of $\tto$ at the same time.
#+end_proof

#+attr_latex: :options [Substitution Lemma]
#+begin_lemma
<<lemma-subsl>>
Assuming $M \rhd M'$ and $U \rhd U'$, $M[U/y] \rhd M'[U'/y]$.
#+end_lemma
#+begin_proof
We apply structural induction on derivations of $M \rhd M'$, depending
on what the last rule we used to derive it was.

  * Reflexivity, $M = x$. If $x=y$, we simply use $U \rhd U'$; if $x \neq y$,
    we use reflexivity on $x$ to get $x \rhd x$.

  * Parallel application. By induction hypothesis, $P[U/y] \rhd P'[U'/y]$ and
    $N[U/y]\rhd N'[U'/y]$, hence $(PN)[U/y] \rhd (P'N')[U'/y]$.

  * Congruence. By induction, $N[U/y] \rhd N'[U'/y]$ and $\lambda x.N[U/y] \rhd \lambda x.N'[U'/y]$.

  * Parallel substitution. By induction, $P[U/y] \rhd P'[U'/y]$ and $N[U/y] \rhd N[U'/y]$,
    hence $((\lambda x.P)N)[U/y] \rhd P'[U'/y][N'[U'/y]/x] = P'[N'/x][U'/y]$.

  * Extensionality, given $x \notin \mathrm{FV}(P)$. By induction, $P \rhd P'$, hence
    $\lambda x.P[U/y]x \rhd P'[U'/y]$.

Note that we are implicitely assuming the Barendregt's variable convention; all
variables have been renamed to avoid clashes.
#+end_proof

#+attr_latex: :options [Maximal parallel one-step reduct]
#+begin_definition 
The *maximal parallel one-step reduct* $M^{\ast}$ of a \lambda-term $M$ is defined
inductively as

  * $x^{\ast} = x$;
  * $(PN)^{\ast} = P^{\ast}N^{\ast}$;
  * $((\lambda x.P)N)^{\ast} = P^{\ast}[N^{\ast}/x]$;
  * $(\lambda x.N)^{\ast} = \lambda x.N^{\ast}$;
  * $(\lambda x.Px)^{\ast} = P^{\ast}$, given $x \notin \mathrm{FV}(P)$.
#+end_definition

#+attr_latex: :options [Diamond property of parallel reduction]
#+begin_lemma
<<lemma-paralleldiamond>>
Given any $M'$ such that $M \rhd M'$, $M' \rhd M^{\ast}$. Parallel one-step reduction 
has the diamond property.
#+end_lemma
#+begin_proof
We apply again structural induction on the derivation of $M \rhd M'$.

  * Reflexivity gives us $M' = x = M^{\ast}$.

  * Parallel application. By induction, we have $P \rhd P^\ast$ and $N \rhd N^{\ast}$; depending
    on the form of $P$, we have

    - $P$ is not a \lambda-abstraction and $P'N' \rhd P^{\ast}N^{\ast} = (PN)^{\ast}$.

    - $P = \lambda x.Q$ and $P \rhd P'$ could be derived using congruence to \lambda-abstraction
      or extensionality. On the first case we know by induction hypothesis that $Q'\rhd Q^{\ast}$
      and $(\lambda x.Q')N' \rhd Q^{\ast}[N^{\ast}/x]$. On the second case, we can take $P = \lambda x.Rx$, where,
      $R \rhd R'$. By induction, $(R'x) \rhd (Rx)^{\ast}$ and now we apply the substitution lemma
      to have $R'N' = (R'x)[N'/x] \rhd (Rx)^{\ast}[N^{\ast}/x]$.

  * Congruence. Given $N \rhd N'$; by induction $N' \rhd N^{\ast}$, and depending on the form of
    $N$ we have two cases

    - $N$ is not of the form $Px$ where $x \not\in \mathrm{FV}(P)$; we can apply congruence to 
      \lambda-abstraction.

    - $N = Px$ where $x \notin \mathrm{FV}(P)$; and $N \rhd N'$ could be derived by parallel application
      or parallel substitution. On the first case, given $P \rhd P'$, we know that $P' \rhd P^{\ast}$
      by induction hypothesis and $\lambda x.P'x \rhd P^{\ast}$ by extensionality. On the second case,
      $N = (\lambda y.Q)x$ and $N' = Q'[x/y]$, where $Q \rhd Q'$. Hence $P \rhd \lambda y.Q'$, and by
      induction hypothesis, $\lambda y.Q' \rhd P^{\ast}$.

  * Parallel substitution, with $N \rhd N'$ and $Q \rhd Q'$; we know that $M^{\ast} = Q^{\ast}[N^{\ast}/x]$
    and we can apply the substitution lemma (lemma [[lemma-subsl]]) to get $M' \rhd M^{\ast}$.

  * Extensionality. We know that $P \rhd P'$ and $x \notin \mathrm{FV}(P)$, so by induction hypothesis
    we know that $P' \rhd P^{\ast} = M^{\ast}$.
#+end_proof

#+attr_latex: :options [Church-Rosser Theorem]
#+begin_theorem
The relation $\tto_{\beta\eta}$ is confluent.
#+end_theorem
#+begin_proof
Parallel reduction, $\rhd$, satisfies the diamond property (lemma [[lemma-paralleldiamond]]), 
which implies the Church-Rosser property. Its reflexive transitive closure is $\tto_{\beta\eta}$
(lemma [[lemma-transclosureparallel]]),
whose diamond property implies confluence for $\to_{\beta\eta}$.
#+end_proof

*** TODO Normalization                                           :noexport:
*** TODO Evaluation, call by name and call by value              :noexport:
*** TODO SKI combinators                                         :noexport:
** Simply typed \lambda-calculus
We will give now a presentation of the *simply-typed \lambda-calculus*
(STLC) based on cite:Hindley08. Our presentation will rely only on the
/arrow type/ $\to$; while other presentations of simply typed
\lambda-calculus extend this definition with type constructors such as
pairs or union types, as it is done in cite:selinger13.

It seems clearer to present a first minimal version of the
\lambda-calculus. Such extensions will be explained later, profiting
from the logical interpretation of propositions as types.

*** Simple types
We start assuming that a set of *basic types* exists. Those basic
types would correspond, in a programming language interpretation, with
things like the type of strings or the type of integers.

#+attr_latex: :options [Simple types]
#+begin_definition
The set of *simple types* is given by the following Backus-Naur form
\[\mathtt{Type} ::= 
\iota \mid 
\mathtt{Type} \to \mathtt{Type} \]
where $\iota$ would be any /basic type/.
#+end_definition

That is to say that, for every two types $A,B$, there exists a *function type*
$A \to B$ between them.

*** Typing rules for the simply typed \lambda-calculus
We will now define the terms of the simply typed \lambda-calculus (STLC) using the
same constructors we used on the untyped version. Those are the *raw typed \lambda-terms*.

#+attr_latex: :options [Raw typed lambda terms]
#+begin_definition
The set of *typed lambda terms* is given by the BNF
\[ \mathtt{Term} ::=
x \mid
\mathtt{Term}\mathtt{Term} \mid
\lambda x^{\mathtt{Type}}. \mathtt{Term} \mid
\]
#+end_definition

The set of raw typed \lambda-terms contains some meaningless terms
under our type interpretation, such as $\pi_1(\lambda x^A.M)$. *Typing rules*
will give them the desired semantics; only a subset of these raw
lambda terms will be typeable.

#+attr_latex: :options [Typing context]
#+begin_definition
A *typing context* is a sequence of typing assumptions
$x_1:A_1,\dots,x_n:A_n$, where no variable appears more than once.
#+end_definition

Every typing rule assumes a typing context, usually denoted by $\Gamma$ 
or by a concatenation of typing contexts written as $\Gamma,\Gamma'$; and 
a consequence from that context, separated by the $\vdash$ symbol.

 1) The $(var)$ rule simply makes explicit the type of a variable from
    the context.

    \begin{prooftree}
    \LeftLabel{($var$)}
    \AxiomC{}
    \UnaryInfC{$\Gamma, x:A \vdash x:A$}
    \end{prooftree}

 2) The $(abs)$ gives the type of a \lambda-abstraction as the type of
    functions from the variable type to the result type. It acts as
    a constructor of function terms.

   \begin{prooftree}
   \LeftLabel{$(abs)$}
   \AxiomC{$\Gamma, x:A \vdash M : B$}
   \UnaryInfC{$\Gamma \vdash \lambda x.M : A \to B$}
   \end{prooftree}

 3) The $(app)$ rule gives the type of a well-typed application of a
    lambda term. A term $f : A \to B$ applied to a term $a : A$ is a term
    of type $B$. It acts as a destructor of function terms.

    \begin{prooftree}
    \LeftLabel{$(app)$}
    \AxiomC{$\Gamma \vdash f : A \to B$}
    \AxiomC{$\Gamma \vdash a : A$}
    \BinaryInfC{$\Gamma \vdash f a : B$}
    \end{prooftree}

#+begin_definition
A term is *typable* if we can assign types to all its variables in
such a way that a typing judgment for the type is derivable.
#+end_definition

From now on, we only consider typable terms on the STLC to be used as
real terms. As a consequence, the set of \lambda-terms of the STLC is
only a subset of the terms of the untyped \lambda-calculus.

# Examples of typable and non-typable terms.

# It is easy to check if a term is typable because there is only
# one way to type it.

# If we want to derive term bottom-up, there is only one possible
# choice at each step. Has this to do with the natural deduction
# properties?

*** Curry-style types
Two different approaches to typing \lambda-terms are commonly used.

 * *Church-style* typing, also known as /explicit typing/ originated from
   the work of Alonzo Church in cite:church40, where he described a STLC
   with two basic types. The term's type is defined as an intrinsic property
   of the term; and the same term has to be interpreted always with the same
   type.

 * *Curry-style* typing, also known as /implicit typing/; which creates a
   formalism where every single term can be given an infinite number of types.
   This technique is called *polymorphism* in general; and here it is only used
   to tell the kinds of combinations that are allowed with any given term.

As an example, we can consider the identity term $I = \lambda x.x$. It would have to be 
defined for each possible type. That is, we should consider a family of different 
identity terms $I_A = \lambda x.x : A \to A$. Curry-style typing allow us to consider 
parametric types with type variables, and to type the identity as 
$I = \lambda x.x : \sigma \to \sigma$ where $\sigma$ is a type variable.

#+attr_latex: :options [Type variables]
#+begin_definition
Given a infinite numerable set of /type variables/, we define *parametric types*
or /type-schemes/ inductively as
\[\mathtt{PType} ::= 
\iota \mid
\mathtt{Tvar} \mid 
\mathtt{PType} \to \mathtt{PType}, \]
that is, all basic types and type variables are atomic parametric types; and we also
consider the arrow type between two parametric types.
#+end_definition

The difference between the two typing styles is then not a mere notational
convention, but a difference on the expressive power that we assign to each
term. The interesting property of type variables is that they can act as
placeholders for other type templates. This is formalized with the notion
of type substitution.

#+attr_latex: :options [Type substitution]
#+begin_definition
A *substitution* $\psi$ is any function from type variables to type templates. It can
be applied to a type template as $\overline{\psi}$ by recursion and knowing that

   * $\overline{\psi} \iota = \iota$,
   * $\overline{\psi} \sigma = \psi \sigma$,
   * $\overline{\psi} (A \to B) = \overline{\psi} A \to \overline{\psi} B$.

That is, the type template $\overline{\psi} A$ is the same as $A$ but with every type variable
replaced according to the substitution $\sigma$.
#+end_definition

We consider a type to be /more general/ than other if the latter can be obtained by
applying a substitution to the former. In this case, the latter is called an /instance/
of the former. For instance, $A \to B$ is more general than its instance
$(C \to D) \to B$, where $A$ is a type variable affected by the substitution. An
interesting property of STLC is that every type has a most general type, called
its /principal type/.

#+attr_latex: :options [Principal type]
#+begin_definition
A closed \lambda-term $M$ has a *principal type* $\pi$ if $M : \pi$ and given any
$M : \tau$, we can obtain $\tau$ as an instance of $\pi$, that is, $\overline{\sigma} \pi = \tau$.
#+end_definition

*** Unification and type inference
The unification of two type templates is the construction of two substitutions
making them equal as type templates; i.e., the construction of a type that
is a particular instance of both at the same time. We will not only aim for
an unifier but for the most general one between them.

#+attr_latex: :options [Most general unifier]
#+begin_definition
A substitution $\psi$ is called an *unifier* of two sequences of type templates
$\left\{ A_i \right\}_{i = 1,\dots,n}$ and $\left\{ B_i \right\}_{i=1,\dots,n}$ if $\overline{\psi} A_i = \overline{\psi} B_i$ for any $i$. We say that it
is the *most general unifier* if given any other unifier $\phi$ exists a substitution
$\varphi$ such that $\phi = \overline{\varphi} \circ \psi$.
#+end_definition

#+begin_lemma
<<lemma-unification>>
If an unifier of $\left\{ A_i \right\}_{i = 1,\dots,n}$ and $\left\{ B_i \right\}_{i=1,\dots,n}$ exists, the most general unifier
can be found using the following recursive definition of $\mathtt{unify}(A_1,\dots,A_n;B_1,\dots,B_n)$.

  1) $\mathtt{unify}(x;x) = \mathrm{id}$ and $\mathtt{unify}(\iota,\iota) = \mathrm{id}$;
  2) $\mathtt{unify}(x;B) = (x \mapsto B)$, the substitution that only changes $x$ by $B$;
     if $x$ does not occur in $B$. The algorithm *fails* if $x$ occurs in $B$;
  3) $\mathtt{unify}(A;x)$ is defined symmetrically;
  4) $\mathtt{unify}(A \to A'; B \to B') = \mathtt{unify}(A,A';B,B')$;
  5) $\mathtt{unify}(A,A_1,\dots; B,B_1,\dots) = \overline{\psi} \circ \rho$ where $\rho = \mathtt{unify}(A_1,\dots;B_1,\dots)$ 
     and $\psi = \mathtt{unify}(\overline{\rho}A; \overline{\rho}B)$;
  6) $\mathtt{unify}$ fails in any other case.

Where $x$ is any type variable. The two sequences of types have no unifier if and only
if $\mathtt{unify}(A,B)$ fails.
#+end_lemma
#+begin_proof
It is easy to notice that, by structural induction, if
$\mathtt{unify}(A;B)$ exists, it is in fact an unifier.

If the unifier fails in clause 2, there is obviously no possible unifier: the number
of constructors on the first type template will be always smaller than the second one.
If the unifier fails in clause 6, the type templates are fundamentally different, they
have different head constructors and this is invariant to substitutions. This proves
that the failure of the algorithm implies the non existence of an unifier.

We now prove that, if $A$ and $B$ can be unified, $\mathtt{unify}(A,B)$ is the most general unifier.
For instance, in the clause 2, if we call $\psi = (x \mapsto B)$ and, if $\eta$ were another unifier,
then $\eta x = \overline{\eta}x = \overline{\eta} B = \overline{\eta}(\psi(x))$; hence $\overline{\eta} \circ \psi = \eta$ by definition of $\psi$. A similar argument can 
be applied to clauses 3 and 4. In the clause 5, we suppose the existence of some unifier $\psi'$. 
The recursive call gives us the most general unifier $\rho$ of $A_1,\dots,A_n$ and $B_1,\dots,B_{n}$; and 
since it is more general than $\psi'$, there exists an $\alpha$ such that $\overline{\alpha} \circ \rho = \psi'$. Now,
$\overline{\alpha}(\overline{\rho}A) = \psi'(A) = \psi'(B) = \overline{\alpha}(\overline{\rho} B)$, hence $\alpha$ is a unifier of $\overline{\rho}A$ and $\overline{\rho}B$; we can take the 
most general unifier to be $\psi$, so $\overline{\beta} \circ \psi = \overline{\alpha}$; and finally, $\overline{\beta} \circ (\overline{\psi} \circ \rho) = \overline{\alpha} \circ \rho = \psi'$.

We also need to prove that the unification algorithm terminates. Firstly, we note that
every substitution generated by the algorithm is either the identity or it removes at least
one type variable. We can perform induction on the size of the argument on all clauses except
for clause 5, where a substitution is applied and the number of type variables is reduced.
Therefore, we need to apply induction on the number of type variables and only then apply
induction on the size of the arguments.
#+end_proof

Using unification, we can define type inference.

#+attr_latex: :options [Type inference]
#+begin_theorem
<<thm-typeinfer>>
The algorithm $\mathtt{typeinfer}(M,B)$, defined as follows, finds the most general substitution $\sigma$
such that $x_1 : \sigma A_1, \dots, x_n : \sigma A_n \vdash M : \overline{\sigma} B$ is a valid typing judgment if it exists;
and fails otherwise.

  1) $\mathtt{typeinfer}(x_i:A_i,\Gamma \vdash x_i : B) = \mathtt{unify}(A_i,B)$;
  2) $\mathtt{typeinfer}(\Gamma \vdash MN : B) = \overline{\varphi} \circ \psi$, where $\psi = \mathtt{typeinfer}(\Gamma \vdash M : x \to B)$ and
     $\varphi = \mathtt{typeinfer}(\overline{\psi}\Gamma \vdash N : \overline{\psi}x)$ for a fresh type variable $x$.
  3) $\mathtt{typeinfer}(\Gamma \vdash \lambda x.M : B) = \overline{\varphi} \circ \psi$ where $\psi = \mathtt{unify}(B; z \to z')$ and
     $\varphi = \mathtt{typeinfer}(\overline{\psi}\Gamma, x:\overline{\psi}z \vdash M : \overline{\psi}z')$ for fresh type variables $z,z'$.

Note that the existence of fresh type variables is always asserted by the set of
type variables being infinite. The output of this algorithm is defined up to
a permutation of type variables.
#+end_theorem
#+begin_proof
The algorithm terminates by induction on the size of $M$. It is easy to check
by structural induction that the inferred type judgments are in fact valid.
If the algorithm fails, by Lemma [[lemma-unification]], it is also clear that the
type inference is not possible.

On the first case, the type is obviously the most general substitution
by virtue of the previous Lemma [[lemma-unification]].  On the second
case, if $\alpha$ were another possible substitution, in particular, it should
be less general than $\psi$, so $\alpha = \beta \circ \psi$. As $\beta$ would be then a possible substitution
making $\overline{\psi}\Gamma \vdash N : \overline{\psi}x$ valid, it should be less general than $\varphi$, so 
$\alpha = \overline{\beta} \circ \psi = \overline{\gamma} \circ \overline{\varphi} \circ \beta$.
On the third case, if $\alpha$ were another possible substitution, it should unify
$B$ to a function type, so $\alpha = \overline{\beta} \circ \psi$. Then $\beta$ should make the type inference
$\overline{\psi}\Gamma, x:\overline{\psi}z \vdash M : \overline{\psi}z'$ possible, so $\beta = \overline{\gamma} \circ \varphi$.
We have proved that the inferred type is in general the most general one.
#+end_proof

#+attr_latex: :options [Principal type property]
#+begin_corollary
Every typable pure \lambda-term has a principal type.
#+end_corollary
#+begin_proof
Given a typable term $M$, we can compute $\mathtt{typeinfer}(x_1:A_1,\dots,x_n:A_n \vdash M : B)$,
where $x_1,\dots,x_n$ are the free variables on $M$ and $A_1,\dots,A_n,B$ are fresh type
variables. By virtue of Theorem [[thm-typeinfer]], the result is the most general type of $M$
if we assume the variables to have the given types.
#+end_proof
*** TODO Subject reduction
# 6.1 on Selinger
# Maybe it's better to write the proof after adding all the
# constructors.
*** TODO Normalization
#+begin_theorem
In the STLC, all terms are strongly normalizing.
#+end_theorem
# Proof in Girard, Lafont, Taylor

** The Curry-Howard correspondence
# Tutorial on Curry-Howard http://purelytheoretical.com/papers/ATCHC.pdf
# Local soundness and completeness http://www.cs.cmu.edu/~fp/courses/15816-s10/lectures/01-judgments.pdf

*** Extending the simply typed \lambda-calculus
We will add now special syntax for some terms and types, such as
pairs, unions and unit types. This syntax will make our \lambda-calculus
more expressive, but the unification and type inference algorithms
will continue to work. The previous proofs and algorithms can be extended to cover
all the new cases.
# And this is done on the mikrokosmos implementation

#+attr_latex: :options [Simple types II]
#+begin_definition
The new set of *simple types* is given by the following BNF
\[\mathtt{Type} ::= \iota \mid 
\mathtt{Type} \to \mathtt{Type} \mid
\mathtt{Type} \times \mathtt{Type} \mid
\mathtt{Type} + \mathtt{Type} \mid
1 \mid
0,\]
where $\iota$ would be any /basic type/.
#+end_definition

That is to say that, for any given types $A,B$, there exists a product
type $A \times B$, consisting of the pairs of elements where the first
one is of type $A$ and the second one of type $B$; there exists the
union type $A + B$, consisting of a disjoint union of tagged terms
from $A$ or $B$; an unit type $1$ with only an element, and an empty
or void type $0$ without inhabitants. The raw typed \lambda-terms are
extended to use these new types.

#+attr_latex: :options [Raw typed lambda terms II]
#+begin_definition
The new set of raw *typed lambda terms* is given by the BNF
\[\begin{aligned} 
\mathtt{Term} ::=\ &
x \mid
\mathtt{Term}\mathtt{Term} \mid
\lambda x. \mathtt{Term} \mid \\&
\left\langle \mathtt{Term},\mathtt{Term} \right\rangle \mid
\pi_1 \mathtt{Term} \mid
\pi_2 \mathtt{Term} \mid \\&
\textrm{inl}\ \mathtt{Term} \mid
\textrm{inr}\ \mathtt{Term} \mid
\textrm{case}\ \mathtt{Term}\ \textrm{of}\ \mathtt{Term}; \mathtt{Term} \mid \\&
\textrm{abort}\ \mathtt{Term} \mid \ast
\end{aligned}\]
#+end_definition

The use of these new terms is formalized by the following extended set
of typing rules.

 1) The $(var)$ rule simply makes explicit the type of a variable from
    the context.

    \begin{prooftree}
    \LeftLabel{($var$)}
    \AxiomC{}
    \UnaryInfC{$\Gamma, x:A \vdash x:A$}
    \end{prooftree}

 2) The $(abs)$ gives the type of a \lambda-abstraction as the type of
    functions from the variable type to the result type. It acts as
    a constructor of function terms.

   \begin{prooftree}
   \LeftLabel{$(abs)$}
   \AxiomC{$\Gamma, x:A \vdash M : B$}
   \UnaryInfC{$\Gamma \vdash \lambda x.M : A \to B$}
   \end{prooftree}

 3) The $(app)$ rule gives the type of a well-typed application of a
    lambda term. A term $f : A \to B$ applied to a term $a : A$ is a term
    of type $B$. It acts as a destructor of function terms.

    \begin{prooftree}
    \LeftLabel{$(app)$}
    \AxiomC{$\Gamma \vdash f : A \to B$}
    \AxiomC{$\Gamma \vdash a : A$}
    \BinaryInfC{$\Gamma \vdash f a : B$}
    \end{prooftree}

 4) The $(pair)$ rule gives the type of a pair of elements. It acts as
    a constructor of pair terms.

    \begin{prooftree}
    \LeftLabel{$(pair)$}
    \AxiomC{$\Gamma \vdash a : A$}
    \AxiomC{$\Gamma \vdash b :  B$}
    \BinaryInfC{$\Gamma \vdash \pair{a,b} : A \times B$}
    \end{prooftree}

 5) The $(\pi_1)$ rule extracts the first element from a pair. It acts as
    a destructor of pair terms.

    \begin{prooftree}
    \LeftLabel{$(\pi_1)$}
    \AxiomC{$\Gamma \vdash m : A \times B$}
    \UnaryInfC{$\Gamma \vdash \pi_1\ m : A$}
    \end{prooftree}

 6) The $(\pi_1)$ rule extracts the second element from a pair. It acts as
    a destructor of pair terms.

    \begin{prooftree}
    \LeftLabel{$(\pi_2)$}
    \AxiomC{$\Gamma \vdash m : A \times B$}
    \UnaryInfC{$\Gamma \vdash \pi_2\ m : B$}
    \end{prooftree}

 7) The $(inl)$ rule creates a union type from the left side type of
    the sum. It acts as a constructor of union terms.

    \begin{prooftree}
    \LeftLabel{$(inl)$}
    \AxiomC{$\Gamma \vdash a : A$}
    \UnaryInfC{$\Gamma \vdash \mathrm{inl}\ a : A + B$}
    \end{prooftree}

 8) The $(inr)$ rule creates a union type from the right side type of
    the sum. It acts as a constructor of union terms.

    \begin{prooftree}
    \LeftLabel{$(inr)$}
    \AxiomC{$\Gamma \vdash b : B$}
    \UnaryInfC{$\Gamma \vdash \mathrm{inr}\ b : A + B$}
    \end{prooftree}

 9) The $(case)$ rule extracts a term from an union and uses on any of the
    two cases

    \begin{prooftree}
    \LeftLabel{$(case)$}
    \AxiomC{$\Gamma \vdash m : A + B$}
    \AxiomC{$\Gamma, a:A \vdash n : C$}
    \AxiomC{$\Gamma, b:B \vdash p : C$}
    \TrinaryInfC{$\Gamma \vdash (\mathrm{case}\ m\ \mathrm{of}\ [a].N;\ [b].P) : C$}
    \end{prooftree}

 10) The $(\ast)$ rule simply creates the only element of $1$. It is a constructor
     of the unit type.

     \begin{prooftree}
     \LeftLabel{$(\ast)$}
     \AxiomC{$$}
     \UnaryInfC{$\Gamma \vdash \ast : 1$}
     \end{prooftree}


 11) The $(abort)$ rule extracts a term of any type from the void type.

     \begin{prooftree}
     \LeftLabel{$(abort)$}
     \AxiomC{$\Gamma \vdash M : 0$}
     \UnaryInfC{$\Gamma \vdash \mathrm{abort}_A\ M : A$}
     \end{prooftree} 

     The abort function must be understood as the unique function going
     from the empty set to any given set.

The \beta-reduction of terms is defined the same way as for the untyped
\lambda-calculus; except for the inclusion of \beta-rules governing the
new terms, one for every destruction rule.

  1) Function application, $(\lambda x.M)N \to_{\beta} M[N/x]$.
  2) First projection, $\pi_1 \left\langle M,N \right\rangle \to_{\beta} M$.
  3) Second projection, $\pi_2 \left\langle M,N \right\rangle \to_{\beta} N$.
  4) Case rule, $(\mathrm{case}\ m\ \mathrm{of}\ [a].N;\ [b].P) \to_{\beta} N a$ if $m$ is of the form $m = \mathrm{inl}\ a$; and
     $(\mathrm{case}\ m\ \mathrm{of}\ [a].N;\ [b].P) \to_{\beta} P b$ if $m$ is of the form $m = \mathrm{inr}\ b$.

On the other side, \eta-rules are defined one for every construction rule.

  1) Function extensionality, $\lambda x.M x \to_{\eta} M$.
  2) Definition of product, $\langle \pi_1 M, \pi_{2} M \rangle \to_{\eta} M$.
  3) Uniqueness of unit, $M \to_{\eta} \ast$.
  4) Case rule, $(\mathrm{case}\ m\ \mathrm{of}\ [a].P[ \mathrm{inl}\ a/c ];\ [b].P[ \mathrm{inr}\ b/c ]) \to_{\eta} P[m/c]$.

*** Natural deduction
The natural deduction is a logical system due to Gentzen. We introduce
it here following cite:selinger13 and cite:wadler15. It relationship
with the STLC will be made explicit on the [[*Propositions as types][next section]].

We will use the logical binary connectives $\to,\wedge,\vee$, and two
given propositions, $\top,\bot$ representing truth and falsity. The
rules defining natural deduction come in pairs; there are introductors
and eliminators for every connective. Every introductor uses a set of
assumptions to generate a formula and every eliminator gives a way to
extract precisely that set of assumptions.

 1) Every axiom on the context can be used.

    \begin{prooftree}
    \RightLabel{(Ax)}
    \AxiomC{}
    \UnaryInfC{$\Gamma,A \vdash A$}
    \end{prooftree}

 2) Introduction and elimination of the $\to$ connective. Note that the
    elimination rule corresponds to /modus ponens/.

    \begin{prooftree}
    \RightLabel{($I_{\to}$)}
    \AxiomC{$\Gamma, A \vdash B$}
    \UnaryInfC{$\Gamma \vdash A \to B$}
    \RightLabel{($E_{\to}$)}
    \AxiomC{$\Gamma \vdash A \to B$}
    \AxiomC{$\Gamma \vdash A$}
    \BinaryInfC{$\Gamma \vdash B$}
    \noLine
    \BinaryInfC{}
    \end{prooftree}

 3) Introduction and elimination of the $\wedge$ connective. Note that the
    introduction in this case takes two assumptions, and there are
    two different elimination rules.

    \begin{prooftree}
    \RightLabel{($I_{\wedge}$)}
    \AxiomC{$\Gamma \vdash A$}
    \AxiomC{$\Gamma \vdash B$}
    \BinaryInfC{$\Gamma \vdash A \wedge B$}
    \RightLabel{($E_{\wedge}^1$)}
    \AxiomC{$\Gamma \vdash A \wedge B$}
    \UnaryInfC{$\Gamma \vdash A$}
    \RightLabel{($E_{\wedge}^2$)}
    \AxiomC{$\Gamma \vdash A \wedge B$}
    \UnaryInfC{$\Gamma \vdash B$}
    \noLine
    \TrinaryInfC{}
    \end{prooftree}

 4) Introduction and elimination of the $\vee$ connective. Here, we need
    two introduction rules to match the two assumptions we use on the
    eliminator.

    \begin{prooftree}
    \RightLabel{($I_{\vee}^1$)}
    \AxiomC{$\Gamma \vdash A$}
    \UnaryInfC{$\Gamma \vdash A \vee B$}
    \RightLabel{($I_{\vee}^2$)}
    \AxiomC{$\Gamma \vdash B$}
    \UnaryInfC{$\Gamma \vdash A \vee B$}
    \RightLabel{($E_{\vee}$)}
    \AxiomC{$\Gamma \vdash A \vee B$}
    \AxiomC{$\Gamma,A \vdash C$}
    \AxiomC{$\Gamma,B \vdash C$}
    \TrinaryInfC{$\Gamma \vdash C$}
    \noLine
    \TrinaryInfC{}
    \end{prooftree}

 5) Introduction for $\top$. It needs no assumptions and, consequently,
    there is no elimination rule for it.

    \begin{prooftree}
    \RightLabel{($I_{\top}$)}
    \AxiomC{}
    \UnaryInfC{$\Gamma \vdash \top$}
    \end{prooftree}

 6) Elimination for $\bot$. It can be eliminated in all generality, and,
    consequently, there are no introduction rules for it. This elimination
    rule represents the /"ex falsum quodlibet"/ principle that says that
    falsity implies anything.

    \begin{prooftree}
    \RightLabel{($E_{\bot}$)}
    \AxiomC{$\Gamma \vdash \bot$}
    \UnaryInfC{$\Gamma \vdash C$}
    \end{prooftree}


*** Propositions as types
In 1934, Curry observed in cite:Curry34 that the type of a function
$(A \to B)$ could be read as an implication and that the existence of a
function of that type was equivalent to the provability of the proposition.
Previously, the *Brouwer-Heyting-Kolmogorov interpretation* of intuitionistic
logic had given a definition of what it meant to be a proof of an intuinistic
formula, where a proof of the implication $(A \to B)$ was a function converting
a proof of $A$ into a proof of $B$. It was not until 1969 that Howard pointed
a deep correspondence between the simply-typed \lambda-calculus and the
natural deduction at three levels

  1) propositions are types.
  2) proofs are programs.
  3) simplification of proofs is the evaluation of programs.

*** TODO Type algebra
** TODO Models of \lambda-calculus
*** TODO Types and sets
# From the Hott book
*** TODO Models
# From Selinger
** TODO Hindley-Milner
** TODO System F                                                  :noexport:
*** TODO System F is strongly normalizing
* Mikrokosmos (abstract)                                             :ignore:
#+LATEX: \ctparttext{\color{black}\begin{center}
We have developed *Mikrokosmos*, a lambda calculus interpreter
written in the purely functional programming language Haskell cite:hudak07_haskell.
It aims to provide students with a tool to learn and understand lambda calculus.
#+LATEX: \end{center}}

* Mikrokosmos
** Lambda expressions
*** De Bruijn indexes
Nicolaas Govert *De Bruijn* proposed in cite:debruijn81 a way of defining \lambda-terms modulo
\alpha-conversion based on indices.  The main idea of De Bruijn
indices is to remove all variables from binders and replace every
variable on the body of an expression with a number, called /index/,
representing the number of \lambda-abstractions in scope between the
ocurrence and its binder.

Consider the following example, the \lambda-term
\[ \lambda x.(\lambda y.\ y (\lambda z.\ y z)) (\lambda t.\lambda z.\ t x)
\]
can be written with de Bruijn indices as
\[
\lambda\ (\lambda(1 \lambda(2 1))\ \lambda\lambda(2 3)\ ).
\]

De Bruijn also proposed a notation for the \lambda-calculus
changing the order of binders and \lambda-applications.  A review on
the syntax of this notation, its advantages and De Bruijn indexes, can be found in
cite:kamareddine01. In this section, we are going to describe De Bruijn
indexes but preserve the usual notation of \lambda-terms; that is, /De Bruijn/
/indexes/ and /De Bruijn notation/ are different concepts and we are going to
use only the former.

#+attr_latex: :options [De Bruijn indexed terms]
#+begin_definition
We define recursively the set of \lambda-terms using de Bruijn notation
following this BNF
\[ \mathtt{Exp} ::= \mathbb{N}
 \mid (\lambda\ \mathtt{Exp})
 \mid (\mathtt{Exp}\ \mathtt{Exp})
\]
#+end_definition

Our internal definition closely matches the formal one. The names of
the constructors here are =Var=, =Lambda= and =App=:

#+BEGIN_SRC haskell
-- | A lambda expression using DeBruijn indexes.
data Exp = Var Integer -- ^ integer indexing the variable.
         | Lambda Exp  -- ^ lambda abstraction
         | App Exp Exp -- ^ function application
         deriving (Eq, Ord)
#+END_SRC

This notation avoids the need for the Barendregt's variable convention and
the \alpha-reductions. It will be useful to implement \lambda-calculus without
having to worry about the specific names of variables.

*** Substitution
We define the [[*Free and bound variables, substitution][substitution]] operation needed for the [[*\beta-reduction][\beta-reduction]] on
de Bruijn indices. In order to define the substitution of the n-th
variable by a \lambda-term $P$ on a given term, we must

 * find all the ocurrences of the variable. At each level of scope
   we are looking for the successor of the number we were looking
   for before.

 * decrease the higher variables to reflect the disappearance of
   a lambda.

 * replace the ocurrences of the variables by the new term, taking
   into account that free variables must be increased to avoid them
   getting captured by the outermost lambda terms. 

In our code, we apply =subs= to any expression. When it is applied to
a \lambda-abstraction, the index and the free variables of the
replaced term are increased with =incrementFreeVars=; whenever it is
applied to a variable, the previous cases are taken into consideration.

#+BEGIN_SRC haskell
-- | Substitutes an index for a lambda expression
subs :: Integer -> Exp -> Exp -> Exp
subs n p (Lambda e) = Lambda (subs (n+1) (incrementFreeVars 0 p) e)
subs n p (App f g)  = App (subs n p f) (subs n p g)
subs n p (Var m)
  | n == m    = p         -- The lambda is replaced directly  
  | n <  m    = Var (m-1) -- A more exterior lambda decreases a number
  | otherwise = Var m     -- An unrelated variable remains untouched
#+END_SRC

Then \beta-reduction can be then defined using this =subs= function.

#+BEGIN_SRC haskell
betared :: Exp -> Exp
betared (App (Lambda e) x) = substitute 1 x e
betared e = e
#+END_SRC

*** De Bruijn-terms and \lambda-terms
The internal language of the interpreter uses de Bruijn expressions,
while the user interacts with it using lambda expressions with alphanumeric
variables. Our definition of a \lambda-expression with variables will be
used in parsing and output formatting.

#+BEGIN_SRC haskell
data NamedLambda = LambdaVariable String                    
                 | LambdaAbstraction String NamedLambda     
                 | LambdaApplication NamedLambda NamedLambda
#+END_SRC

The translation from a natural \lambda-expression to de Bruijn notation
is done using a dictionary which keeps track of the bounded variables

#+BEGIN_SRC haskell
tobruijn :: Map.Map String Integer -- ^ names of the variables used
         -> Context                -- ^ names already binded on the scope
         -> NamedLambda            -- ^ initial expression
         -> Exp
-- Every lambda abstraction is inserted in the variable dictionary,
-- and every number in the dictionary increases to reflect we are entering
-- into a deeper context.
tobruijn d context (LambdaAbstraction c e) = 
     Lambda $ tobruijn newdict context e
        where newdict = Map.insert c 1 (Map.map succ d)

-- Translation of applications is trivial.
tobruijn d context (LambdaApplication f g) = 
     App (tobruijn d context f) (tobruijn d context g)

-- We look for every variable on the local dictionary and the current scope.
tobruijn d context (LambdaVariable c) =
  case Map.lookup c d of
    Just n  -> Var n
    Nothing -> fromMaybe (Var 0) (MultiBimap.lookupR c context)
#+END_SRC

while the translation from a de Bruijn expression to a natural one is done
considering an infinite list of possible variable names and keeping a list
of currently-on-scope variables to name the indices.

#+BEGIN_SRC haskell
-- | An infinite list of all possible variable names 
-- in lexicographical order.
variableNames :: [String]
variableNames = concatMap (`replicateM` ['a'..'z']) [1..]

-- | A function translating a deBruijn expression into a 
-- natural lambda expression.
nameIndexes :: [String] -> [String] -> Exp -> NamedLambda
nameIndexes _    _   (Var 0) = LambdaVariable "undefined"
nameIndexes used _   (Var n) = LambdaVariable (used !! pred (fromInteger n))
nameIndexes used new (Lambda e) = 
  LambdaAbstraction (head new) (nameIndexes (head new:used) (tail new) e)
nameIndexes used new (App f g) = 
  LambdaApplication (nameIndexes used new f) (nameIndexes used new g)
#+END_SRC

*** TODO Evaluation
*** TODO Principal type inference
** Output formatting :noexport:
*** Verbose mode
*** SKI mode
** Haskell                                                        :noexport:
** Parsing
*** Monadic parser combinators
A common approach to building parsers in functional programming is to
model parsers as functions. Higher-order functions on parsers act as
/combinators/, which are used to implement complex parsers in a
modular way from a set of primitive ones. In this setting, parsers
exhibit a monad algebraic structure, which can be used to simplify
the combination of parsers. A technical report on *monadic parser combinators*
can be found on cite:hutton96.

The use of monads for parsing is discussed firstly in cite:Wadler85,
and later in cite:Wadler90 and cite:hutton98. The parser type is
defined as a function taking a =String= and returning a list of pairs,
representing a successful parse each. The first component of the pair
is the parsed value and the second component is the remaining
input. The Haskell code for this definition is

#+BEGIN_SRC haskell
newtype Parser a = Parser (String -> [(a,String)])

parse :: Parser a -> String -> [(a,String)]
parse (Parser p) = p

instance Monad Parser where
  return x = Parser (\s -> [(x,s)])
  p >>= q  = Parser (\s -> 
               concat [parse (q x) s' | (x,s') <- parse p s ])
#+END_SRC

where the monadic structure is defined by =bind= and =return=. Given a
value, the =return= function creates a monad that consumes no input
and simply returns the given value. The =>>== function acts as a sequencing
operator for parsers. It takes two parsers and applies the second one
over the remaining inputs of the first one, using the parsed values on
the first parsing as arguments.

An example of primitive *parser* is the =item= parser, which consumes a
character from a non-empty string. It is written in Haskell code as

#+BEGIN_SRC haskell
item :: Parser Char
item = Parser (\s -> case s of 
                       "" -> []
                       (c:s') -> [(c,s')])
#+END_SRC

and an example of *parser combinator* is the =many= function, which
allows one or more applications of the parser given as an argument

#+BEGIN_SRC haskell
many :: Paser a -> Parser [a]
many p = do
  a  <- p
  as <- many p
  return (a:as)
#+END_SRC

in this example =many item= would be a parser consuming all characters
from the input string.

*** Parsec
*Parsec* is a monadic parser combinator Haskell library described in
cite:leijen2001. We have chosen to use it due to its simplicity and
extensive documentation. As we expect to use it to parse user live
input, which will tend to be short, performance is not a critical
concern. A high-performace library supporting incremental parsing,
such as *Attoparsec* cite:attoparsec, would be suitable otherwise.
 
** Usage
*** Mikrokosmos interpreter :noexport:
*** Jupyter kernel
The *Jupyter Project* cite:jupyter is an open source project providing
support for interactive scientific computing. Specifically, the
Jupyter Notebook provides a web application for creating interactive
documents with live code and visualizations. 

We have developed a Mikrokosmos kernel for the Jupyter Notebook,
allowing the user to write and execute arbitrary Mikrokosmos code
on this web application.

# Image of the mikrokosmos jupyter notebook

*** CodeMirror lexer
** Programming in the untyped \lambda-calculus
This section explains how to use the untyped \lambda-calculus to
encode data structures and useful data, such as booleans, linked lists,
natural numbers or binary trees. All this is done on pure \lambda-calculus
avoiding the addition of any new syntax or axioms.

This presentation follows the Mikrokosmos tutorial on \lambda-calculus, which
aims to teach how it is possible to program using untyped \lambda-calculus
without discussing technical topics such as those we have discussed on
the chapter on [[*Untyped \lambda-calculus][untyped \lambda-calculus]]. It also follows
the exposition on cite:selinger13 of the usual Church encodings.

All the code on this section is valid Mikrokosmos code.

*** Basic syntax
In the interpreter, \lambda-abstractions are written with the symbol =\=,
representing a \lambda. This is a convention used on some functional languages
such as Haskell or Agda. Any alphanumeric string can be a variable and
can be defined to represent a particular \lambda-term using the === operator.

As a first example, we define the identity function (=id=), function 
composition (=compose=) and a constant function on two arguments which
always returns the first one untouched (=const=).

#+BEGIN_SRC haskell
id = \x.x
compose = \f.\g.\x.f (g x)
const = \x.\y.x
#+END_SRC

Evaluation of terms will be denoted with the ==>= symbol, as in

#+BEGIN_SRC haskell
compose id id
--- => id
#+END_SRC

It is important to notice that multiple argument functions are defined as
higher one-argument functions which return another functions as arguments.
These intermediate functions are also valid \lambda-terms. For example

#+BEGIN_SRC haskell
alwaysid = const id
#+END_SRC

is a function that discards one argument and returns the identity =id=.
This way of defining multiple argument functions is called the *currying*
of a function in honor to the american logician Haskell Curry in cite:haskell58.
It is a particular instance of a deeper fact, the *hom-tensor adjunction*
\[
\mathrm{Hom}(A \times B, C) \cong \mathrm{Hom}(A, \mathrm{Hom}(B,C))
\]
or the definition of exponentials.

*** A technique on inductive data encoding
Over this presentation, we will implicitly use a technique on the
majority of our data encodings which allows us to write an encoding
for any algebraically inductive generated data. This technique is
used without comment on cite:selinger13 and is the basic of what is
called the *Church encoding*.

We start considering the usual inductive representation of
the data type with data constructors, as we do when we represent a
syntax with a BNF, for example,
\[ \mathtt{Nat} ::= \mathtt{Zero} \mid \mathtt{Succ}\ \mathtt{Nat}. \]

Or, in general
\[
\mathtt{T} ::= C_1 \mid C_2 \mid C_3 \mid \dots
\]

We do not have any possibility of encoding constructors on
\lambda-calculus. Even if we had, they would have, in theory, no
computational content; the application of constructors would not
be reduced under any \lambda-term, and we would need at least the 
ability to pattern match on the constructors to define functions
on them. The \lambda-calculus would need to be extended with
additional syntax for every new type.

This technique, instead, defines a data term as a function on
multiple variables representing the constructors. In our example, the number $2$, which
would be written as $\mathtt{Succ}(\mathtt{Succ}(\mathtt{Zero}))$, would be encoded as
\[
\lambda s.\ \lambda z.\ s (s (z)).
\]

In general, any instance of the type $\mathtt{T}$ would be encoded as a
\lambda-expression depending on all its constuctors
\[
\lambda c_{1}.\ \lambda c_{2}.\ \lambda c_{3}.\ \dots\ \lambda c_{n}. (\textit{term}).
\]

This acts as the definition of an initial algebra over the
constructors and lets us compute by instantiating this algebra on
particular cases. Particular examples are described on the following
sections.
# Link to categories

*** Booleans
Booleans can be defined as the data generated by a pair of constuctors
\[\mathtt{Bool} ::= \mathtt{True} \mid \mathtt{False}.
\]

Consequently, the Church encoding of booleans takes these constructors as
arguments and defines

#+BEGIN_SRC haskell
true  = \t.\f.t
false = \t.\f.f
#+END_SRC

Note that =true= and =const= are exactly the same term up to
\alpha-conversion. The same thing happens with =false= and =alwaysid=.
The absence of types prevents us to make any effort to discriminate
between these two uses of the same \lambda-term. Another side-effect
of this definition is that our =true= and =false= terms can be interpreted
as binary functions choosing between two arguments, i.e.,

  * $\mathtt{true}(a,b) = a$
  * $\mathtt{false}(a,b) = b$

We can test this interpretation on the interpreter to get

#+BEGIN_SRC haskell
true id const
--- => id

false id const
--- => const
#+END_SRC

This inspires the definition of an =ifelse= combinator as the identity

#+BEGIN_SRC haskell
ifelse = \b.b
(ifelse true) id const
--- => id
(ifelse false) id const
--- => false
#+END_SRC

The usual logic gates can be defined profiting from this interpretation
of booleans

#+BEGIN_SRC haskell
and = \p.\q.p q p
or = \p.\q.p p q
not = \b.b false true
xor = \a.\b.a (not b) b
implies = \p.\q.or (not p) q

xor true true
--- => false
#+END_SRC

*** Natural numbers
Our definition of natural numbers is inspired by the Peano natural numbers.
We use two constructors

 * the zero is a natural number, written as Z;
 * the successor of a natural number is a natural number, written as S;

and the BNF we defined when discussing how to [[*A technique on inductive data encoding][encode inductive data]].

#+BEGIN_SRC haskell
0    = \s.\z.z
succ = \n.\s.\z.s (n s z)
#+END_SRC

This definition of =0= is trivial: given a successor function and a
zero, return the zero. The successor function seems more complex, but
it uses the same underlying idea: given a number, a successor and a
zero, apply the successor to the interpretation of that number using
the same successor and zero.

We can then name some natural numbers as

#+BEGIN_SRC haskell
1 = succ 0
2 = succ 1
3 = succ 2
4 = succ 3
5 = succ 4
6 = succ 5
...
#+END_SRC

even if we can not define an infinite number of terms as we might wish.

The interpretation the natural number $n$ as a higher order function
is a function taking an argument =f= and applying them $n$ times over
the second argument.

#+BEGIN_SRC haskell
5 not true
--- => false
4 not true
--- => true

double = \n.\s.\z.n (compose s s) z
double 3
--- => 6
#+END_SRC

Addition $n+m$ applies the successor $m$ times to $n$; and multiplication
$nm$ applies the $n\text{-fold}$ application of the successor $m$ times to $0$.

#+BEGIN_SRC haskell
plus = \m.\n.\s.\z.m s (n s z)
mult = \m.\n.\s.\z.m (n s) z

plus 2 1
--- => 3
mult 2 4
--- => 8
#+END_SRC

*** TODO The predecessor function and predicates on numbers
From the definition of =pred=, some predicates on number can be
defined. The first predicate will be a function distinguishing a
successor from a zero. It will be user later to build more complex
ones. It is built by appliying a =const false= function =n= times to a
true constant. Only if it is applied =0= times, it will return a true
value.

#+BEGIN_SRC haskell
iszero = \n.(n (const false) true)
iszero 0
--- => true
iszero 2
--- => false
#+END_SRC

From this predicate, we can derive predicates on equality and ordering.

#+BEGIN_SRC haskell
leq = \m.\n.(iszero (minus m n))
eq  = \m.\n.(and (leq m n) (leq n m))
#+END_SRC

*** Lists
We would need two constructors to represent a list: a =nil= signaling
the end of the list and a =cons=, joining an element to the head of
the list. An example of list would be
\[\mathtt{cons}\ 1\ (\mathtt{cons}\ 2\ (\mathtt{cons}\ 3\ \mathtt{nil})).\]

Our definition takes those two constructors into account
#+BEGIN_SRC haskell
nil  = \c.\n.n
cons = \h.\t.\c.\n.(c h (t c n))
#+END_SRC
and the interpretation of a list as a higher-order function is its
=fold= function, a function taking a binary operation and an initial
element and appliying the operation repeteadly to every element on
the list.

\[\mathtt{cons}\ 1\ (\mathtt{cons}\ 2\ (\mathtt{cons}\ 3\ \mathtt{nil}))
\overset{fold\ plus\ 0}\longrightarrow 
\mathtt{plus}\ 1\ (\mathtt{plus}\ 2\ (\mathtt{plus}\ 3\ 0))\]

The =fold= operation and some operations on lists can be defined
explicitely as

#+BEGIN_SRC haskell
fold = \c.\n.\l.(l c n)
sum  = fold plus 0
prod = fold mult 1
all  = fold and true
any  = fold or false
length = foldr (\h.\t.succ t) 0

sum (cons 1 (cons 2 (cons 3 nil)))
--- => 6
all (cons true (cons true (cons true nil)))
--- => true
#+END_SRC

The two most commonly used particular cases of fold and frequent examples
of the functional programming paradigm are =map= and =filter=.

  - The *map* function applies a function =f= to every element on a
    list.
  - The *filter* function removes the elements of the list that do not
    satisfy a given predicate. It /filters/ the list, leaving only
    elements that satisfy the predicate.

They can be defined as follows.

#+BEGIN_SRC haskell
map    = \f.(fold (\h.\t.cons (f h) t) nil)
filter = \p.(foldr (\h.\t.((p h) (cons h t) t)) nil)
#+END_SRC

On =map=, given a =cons h t=, we return a =cons (f h) t=; and given a
=nil=, we return a =nil=. On =filter=, we use a boolean to decide at
each step whether to return a list with a head or return the tail
ignoring the head.

#+BEGIN_SRC haskell
mylist = cons 1 (cons 2 (cons 3 nil))
sum (map succ mylist)
--- => 9
length (filter (leq 2) mylist)
--- => 2
#+END_SRC

**** TODO The universal properties of fold, map and filter
*** TODO Binary trees
*** TODO Fixed points                                            :noexport:
* Type theory                                                     
** Intuitionistic logic
*** Constructive mathematics
*** The double negation of LEM is provable
In intuitionistic logic, the double negation of the LEM holds for every
proposition, that is,

\[
\forall A\colon \neg \neg (A \vee \neg A)
\]

**** Proof
Suppose $\neg (A \vee \neg A)$. We firstly are going to prove that, under this
specific assumption, $\neg A$ holds. If $A$ were true, $A \vee \neg A$ would be true and we
would arrive to a contradition, so $\neg A$. But then, if we have $\neg A$ we also have
$A \vee \neg A$ and we arrive to a contradiction with the assumption. We should conclude
that $\neg \neg (A \vee \neg A)$.

**** Machine proof
#+latex: \ExecuteMetaData[latex/Ctlc.tex]{id}

** TODO Propositions as types
** TODO Martin-Löf Type Theory
** TODO Type theory as a foundation of mathematics
** TODO Constructive mathematics
*** TODO Proof by contradiction and proof of a negation
# They are fundamentally different
#
# [[https://www.youtube.com/watch?v=21qPOReu4FI][Five stages of accepting constructive mathematics]]
*** TODO Axiom of choice implies excluded middle
# In Agda or Coq!?
** TODO Homotopy Type Theory
* Conclusions
* Appendices
bibliographystyle:alpha
bibliography:Bibliography.bib

** TODO Mikrokosmos complete code
** TODO Mikrokosmos user's guide
