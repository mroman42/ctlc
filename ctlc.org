#+TITLE: Category Theory and Lambda Calculus
#+AUTHOR: Mario Román
#+OPTIONS: broken-links:mark toc:t tasks:nil num:3
#+SETUPFILE: ctlc.setup
#+LATEX_HEADER_EXTRA: %\input{titlepage}

** Abstract
:PROPERTIES:
:UNNUMBERED: t
:END:

This is the abstract.

** Acknowlegments                                                 :noexport:
# David Charte and Ignacio Cordón, testing the first versions of
# Mikrokosmos.

# Alejandro García, Adrián Ranea and David Charte, template usage.

This document has been written with Emacs26 and org-mode 9, using the
=org= file format and LaTeX as intermediate format. The document follows
the =classicthesis= [[http://www.latextemplates.com/templates/theses/2/thesis_2.pdf][template]] by André Miede. The =minted= package
has been used for code listings and the =tikzcd= package has been used
for commutative diagrams.
* Lambda calculus (abstract)                                         :ignore:
#+LATEX: \ctparttext{\color{black}\begin{center}
The \lambda-calculus is a collection of systems formalizing the notion
of computation. They can be seen as programming languages and formal
logics at the same time. We discuss the properties of the untyped
\lambda-calculus and simply typed \lambda-calculus and its relation to
logic.
#+LATEX: \end{center}}

* Lambda calculus
** Untyped \lambda-calculus
How should we define functions? Classically in mathematics, /functions are graphs/.
A function from a
domain to a codomain, $f \colon X \to Y$, is seen as a subset of the product
space: $f \subset X \times Y$.
Any two functions are equal if they map equal inputs to equal outputs;
and a function its completely determined by what its outputs are.
This vision is called */extensional/*.

From a computational point of view, this perspective could be incomplete
in some cases. Computationally, we usually care not only about the result but, crucially,
about /how/ can it be computed. 
Classically in computer science, /functions are formulae/; and two functions 
mapping equal inputs to equal outputs need not to be equal. E.g., two sorting algorithms
can have a different efficiency or different memory requisites, even if 
they output the same sorted list. This vision, where two functions are
equal if and only if they are given by essentially the same formula is
called */intensional/*.

The *\lambda-calculus* is a collection of formal systems, all of them
based on the lambda notation discovered by Alonzo Church in the 1930s
while trying to develop a foundational notion of functions (/as formulae/)
on mathematics. It is a logical theory of functions, where application and
abstraction are primitive notions; and, at the same time, it
is also one of the simplest programming languages, in which many other
full-fledged languages are based, as we will detail later.

The *untyped* or *pure \lambda-calculus* is, syntactically, the
simplest of those formal systems. In it, a function does not need a
domain nor a codomain; every function is a formula which can be
directly applied to any expression. It even allows functions to be
applied to themselves, a notion which would be troublesome[fn:lambdaregularityaxiom]
in our usual set-theoretical foundations. In exchange, it presents
other problems such as non-terminating functions.

This presentation of the untyped lambda calculus will follow
cite:Hindley08 and cite:selinger13.

[fn:lambdaregularityaxiom]: In particular, if $f$ is a member of its own
domain, the infinite descending sequence
\[
f \ni \{f,f(f)\} \ni f \ni \{f,f(f)\} \ni \dots,
\]
would exist, thus contradicting the *regularity axiom* of Zermelo-Fraenkel
set theory. See, for example, cite:kunen11.

*** The untyped \lambda-calculus
As a language, the untyped \lambda-calculus is given by a set of
expressions, called /\lambda-terms/, and some manipulation rules.
These terms can stand for functions or arguments indistinctly:
they all use the same \lambda-notation in order to define function
abstractions and applications.

*\lambda-notation* allows a function to be written and inlined as any other element
of the language, identifying it with the formula it represents and 
admitting a more compact notation. For example, the polynomial function
$p(x) = x^2 + x$
would be written in \lambda-calculus as
$\lambda x.\ x^2 + x$; and $p(2)$ would be written as $(\lambda x.\ x^2+x)(2)$. In general,
$\lambda x.M$ would be a function taking $x$ as an argument and returning $M$,
which would be a term where $x$ could appear in.

\lambda-notation also eases the writing of *higher-order functions*, functions whose arguments or
outputs are functions themselves. For instance,
\[
\lambda f.(\ \lambda y.\ f(f(y))\ )
\]
would be a function taking $f$ as an argument and returning $\lambda y.\ f(f(y))$,
which is itself a function; most commonly written as $f \circ f$. In particular,
\[
\Big( \big( \lambda f.(\ \lambda y.\ f(f(y))\ ) \big)
\big( \lambda x.\ x^2 + x \big) \Big) (1)
\]
evaluates to $6$.

#+attr_latex: :options [Lambda terms]
#+begin_definition
<<def-lambdaterms>>
The *\lambda-terms* are defined inductively knowing that

  * every */variable/*, taken from an infinite countable set of
    variables and usually written as lowercase single letters
    $(x, y, z, \dots)$, is a \lambda-term.

  * given two \lambda-terms $M,N$; its */application/*, $MN$, is a \lambda-term.

  * given a \lambda-term $M$ and a variable $x$, its */abstraction/*, $\lambda x.M$,
    is a \lambda-term.

Equivalently, they can be also defined by the following Backus-Naur form,
\[
\mathtt{Term} ::= x \mid (\mathtt{Term}\ \mathtt{Term}) \mid (\lambda x.\mathtt{Term})\quad,
\]
where $x$ can be any variable.
#+end_definition

By convention, we omit outermost parentheses and assume
left-associativity, i.e., $MNP$ will always mean $(MN)P$.
We also take the \lambda-abstraction as having the lowest
precedence, e.g., $\lambda x. M N$ should be read as $\lambda x.(MN)$
instead of $(\lambda x.M) N$.

# NOTE: This is not necessary.
# Multiple \lambda-abstractions can be also contracted to a single 
# multivariate abstraction; thus $\lambda x.\lambda y.M$ can 
# become $\lambda x,y.M$.

*** Free and bound variables, substitution
In \lambda-calculus, the scope of a variable restricts to the \lambda-abstraction
where it appeared, if any. Thus, the same variable can be used multiple
times on the same term independently. For example, in 
$(\lambda x.x)(\lambda x.x)$, which evaluates to $(\lambda x.x)$, $x$ appears two times
as a variable with two different meanings.

Any ocurrence of a variable $x$ inside the /scope/ of a lambda is said
to be */bound/*; and any not bound variable is said to be
*/free/*. Formally, we can define the set of free variables on a given
term as follows.

#+attr_latex: :options [Free variables]
#+begin_definition
<<def-freevariables>>
The *set of free variables* of a term $M$ is defined inductively as
\[\begin{aligned}
\freevars(x) &= \{x\}, \\
\freevars(MN) &= \freevars(M) \cup \freevars(N), \\
\freevars(\lambda x.M) &= \freevars(M) \setminus \{x\}.
\end{aligned}\]
#+end_definition

Evaluation in \lambda-calculus relies in the notion of */substitution/*.
Any free ocurrence of a variable can be substituted by a term, as we do
when we are evaluating terms. For instance, in the previous example, we
evaluated $(\lambda x.\ x^2+x)(2)$ by substituting $2$ in $x^{2} + x$ in the place of $x$;
as in
\[\begin{tikzcd}
(\lambda x.\ x^2+x)(2) \rar{x \mapsto 2}
&
2^{2} + 2.
\end{tikzcd}\]
This, however, should
be done avoiding the unintended binding which happens
when a variable is substituted inside the scope of a binder with the
same name, as in the following example: if we were to evaluate the expression
$(\lambda x.\ y x)(\lambda z.\ xz)$,
where $x$ appears two times (once bound and once free), we should substitute $y$ by $(\lambda z.xz)$
on $(\lambda x.yx)$ and $x$ (the free variable) would get tied to $x$ (the bounded variable)
\[\begin{tikzcd}
(\lambda x.yx)(\lambda z.\ xz) \ar{rr}{y \mapsto (\lambda z.xz)} && (\lambda x.(\lambda z.xz)x).
\end{tikzcd}\]

To avoid this, the bounded $x$ should be given a new name before the
substitution, which should be carried as
\[\begin{tikzcd}
(\lambda u.y u)(\lambda z.\ xz) \ar{rr}{y \mapsto (\lambda z.xz)} & & (\lambda u.(\lambda z.xz)u),
\end{tikzcd}\]
keeping the free character of $x$.

#+attr_latex: :options [Substitution on lambda terms]
#+begin_definition
The *substitution* of a variable $x$ by a term $N$ on $M$ is
written as $M[N/x]$ and is defined inductively as
\[\begin{aligned}
x[N/x] &\equiv N,\\
y[N/x] &\equiv y,\\
(MP)[N/x] &\equiv (M[N/x])(P[N/x]),\\
(\lambda x.P)[N/x] &\equiv \lambda x.P,\\
(\lambda y.P)[N/x] &\equiv \lambda y.P[N/x] & \text{ if } y \notin \freevars(N), \\
(\lambda y.P)[N/x] &\equiv \lambda z.P[z/y][N/x] & \text{ if } y \in \freevars(N);
\end{aligned}\]

where, in the last clause, $z$ is a fresh unused variable.
#+end_definition

We could define a criterion for choosing exactly what this new
variable should be, or simply accept that our definition will not be
exactly well-defined, but only
/well-defined up to a change on the name of the variables/.
This equivalence relation will be defined 
formally on the next section. In practice, it is common to follow the 
/Barendregt's variable convention/, which simply assumes that bound 
variables have been renamed to be distinct.

*** \alpha-equivalence
In \lambda-terms, variables are only placeholders and its name, as we have
seen before, is not relevant. Two \lambda-terms whose only difference is
the naming of the variables are called \alpha-equivalent. For example,
\[
(\lambda x.\lambda y. x\ y) \quad\text{ is $\alpha$-equivalent to }\quad (\lambda f.\lambda x. f\ x).
\]

*\alpha-equivalence* formally captures the fact that the name of a bound
variable can be changed without changing the properties of the term. This
idea appears recurrently on mathematics; e.g., the renaming of variables of
integration or the variable on a limit are a examples of \alpha-equivalence.
\[
\int_0^1 x^2\ dx = \int_0^1 y^2\ dy;
\qquad
\lim_{x \to \infty} \frac{1}{x} = \lim_{y \to \infty} \frac{1}{y}.
\]

#+attr_latex: :options [\alpha-equivalence]
#+begin_definition
*\alpha-equivalence* is the smallest relation $=_{\alpha}$ on
\lambda-terms which is an equivalence relation, i.e.,

  * it is /reflexive/, $M =_{\alpha} M$;
  * it is /symmetric/, if $M =_{\alpha} N$, then $N =_{\alpha} M$;
  * and it is /transitive/, if $M=_{\alpha}N$ and $N=_{\alpha}P$, then $M=_{\alpha}P$;

and it is compatible with the structure of lambda terms,

  * if $M =_{\alpha} M'$ and $N =_{\alpha} N'$, then $MN =_{\alpha}M'N'$;
  * if $M=_{\alpha}M'$, then $\lambda x.M =_{\alpha} \lambda x.M'$;
  * if $y$ does not appear on $M$, $\lambda x.M =_{\alpha} \lambda y.M[y/x]$.
#+end_definition

*** \beta-reduction
The core idea of evaluation in \lambda-calculus is captured by the notion
of *\beta-reduction*.
Until now, evaluation has been only informally described; it is time
to define it as a relation, $\tto_{\beta}$, going from the initial term to
any of its partial evaluations. We
will firstly consider a /one-step reduction/ relationship, called
$\to_{\beta}$, which will be extended by transitivity to $\tto_{\beta}$.

Ideally, we would like to define evaluation as a series of reductions
into a canonical form which could not be further reduced.
Unfortunately, as we will see later, it is not possible to find, in
general, that canonical form.

#+attr_latex: :options [\beta-reduction]
#+begin_definition
<<def-betared>>
The *single-step \beta-reduction* is the smallest relation on \lambda-terms
capturing the notion of evaluation 
\[
(\lambda x.M)N \to_{\beta}M[N/x],
\]
and some congruence rules that preserve the structure of
\lambda-terms, such as

  * $M \to_{\beta} M'$ implies $MN \to_{\beta} M'N$ and $NM \to_{\beta} NM'$;
  * $M \to_{\beta}M'$ implies $\lambda x.M \to_{\beta} \lambda x.M'$.

The reflexive transitive closure of $\to_{\beta}$ is written as $\tto_{\beta}$. The symmetric
closure of $\tto_{\beta}$ is called *\beta-equivalence* and written as $=_{\beta}$ or simply $=$.
#+end_definition

*** \eta-reduction
Although we lost the extensional view of functions when we decided to
adopt the /functions as formulae/ perspective, the idea of function
extensionality in \lambda-calculus can be partially recovered by the notion
of \eta-reduction.
This form of /function extensionality for \lambda-terms/ can be captured
by the notion that any term which simply applies a function to the
argument it takes can be reduced to the actual function. That is,
any $\lambda x.M x$ can be reduced to $M$.

#+attr_latex: :options [\eta-reduction]
#+begin_definition
The *\eta-reduction* is the smallest relation on \lambda-terms satisfiying the
same congruence rules as \beta-reduction and the following axiom
\[
\lambda x.Mx \to_{\eta} M,\text{ for any } x \notin \mathrm{FV}(M).
\]
We define single-step \beta\eta-reduction as the union of \beta-reduction
and \eta-reduction. This will be written as $\to_{\beta\eta}$, and its reflexive transitive
closure will be $\tto_{\beta\eta}$.
#+end_definition

Note that, in the particular case where $M$ is itself a \lambda-abstraction,
\eta-reduction is simply a particular case of \beta-reduction.

# TODO: Comments in https://cstheory.stackexchange.com/a/8261/28986
# suggest a theorem in Urzyczyn, Sorensen which might be relevant

*** Confluence
# What is confluence
As we said earlier, it is not possible in general to evaluate a \lambda-term
into a canonical, non-reducible term. However, we will be able to prove
that, in the cases where it exists, it will be unique. This property
is a consequence of a sightly more general one, */confluence/*, which
can be defined in any abstract rewriting system.

#+attr_latex: :options [Confluence]
#+begin_definition
A relation $\to$ is *confluent* if, given its reflexive transitive closure
$\tto$, $M \tto N$ and $M \tto P$ imply the existence of some $Z$ such that
$N \tto Z$ and $P \tto Z$.
#+end_definition

Given any binary relation $\to$ of which $\tto$ is its reflexive transitive
closure, we can consider three seemingly related properties

  * the *confluence* (also called /Church-Rosser property/) we have just defined.
  * the *quasidiamond property*, which assumes $M \to N$ and $M \to P$.
  * the *diamond property*, which is defined substituting $\tto$ by $\to$ on
    the definition on confluence.

Diagrammatically, the three properties can be represented as
\[\begin{tikzcd}[column sep=small]
& 
M \drar[two heads]\dlar[two heads] &&& 
M \drar\dlar &&& 
M \drar\dlar &\\
N \drar[dashed,two heads] && 
P \dlar[dashed,two heads] & 
N \drar[dashed,two heads] &&
P \dlar[dashed,two heads] &
N \drar[dashed] && 
P \dlar[dashed] \\& 
Z &&&
Z &&&
Z &\\
\end{tikzcd}\]
and we can show that the diamond relation implies confluence; while
the quasidiamond does not. Both claims are easy to prove, and they
show us that, in order to prove confluence for a given relation, we
can use the diamond property instead of the quasidiamond property, as
a naive attempt of proof by induction would attempt.

The statement of $\tto_{\beta}$ and $\tto_{\beta\eta}$ being confluent is what we are going to
call the */Church-Rosser theorem/*. The definition of a relation satisfying
the diamond property and whose reflexive transitive closure is $\tto_{\beta\eta}$ will
be the core of our proof.

*** The Church-Rosser theorem
The proof presented here is due to Tait and Per Martin-Löf; an earlier
but more convoluted proof was discovered by Alonzo Church and Barkley 
Rosser in 1935 (see cite:barendregt84 and cite:pollack95).
It is based on the idea of parallel one-step reduction.

#+attr_latex: :options [Parallel one-step reduction]
#+begin_definition
We define the *parallel one-step reduction* relation, $\rhd$ as the smallest
relation satisfying that, assuming $P \rhd P'$ and $N \rhd N'$, the following
properties of

  * reflexivity, $x \rhd x$;
  * parallel application, $PN \rhd P'N'$;
  * congruence to \lambda-abstraction, $\lambda x.N \rhd \lambda x.N'$;
  * parallel substitution, $(\lambda x.P)N \rhd P'[N'/x]$;
  * and extensionality, $\lambda x.P x \rhd P'$, if $x \not\in \mathrm{FV}(P)$,

hold.
#+end_definition

Using the first three rules, it is trivial to show that this relation
is in fact reflexive.

#+begin_lemma
<<lemma-transclosureparallel>>
The reflexive transitive closure of $\rhd$ is $\tto_{\beta\eta}$.
In particular, given any $M,M'$,

  1) if $M \to_{\beta\eta} M'$, then $M \rhd M'$.
  2) if $M \rhd M'$, then $M \tto_{\beta\eta} M'$;
#+end_lemma
#+begin_proof
  1) We can prove this by exhaustion and structural induction on
     \lambda-terms, the possible ways in which we arrive at $M \to M'$
     are

     * $(\lambda x.M)N \to M[N/x]$; where we know that, by parallel substitution
       and reflexivity $(\lambda x.M)N \rhd M[N/x]$.

     * $MN \to M'N$ and $NM \to NM'$; where we know that, by
       induction $M \rhd M'$, and by parallel application and reflexivity, $MN \rhd M'N$
       and $NM \rhd NM'$.

     * congruence to \lambda-abstraction, which is a shared property between
       the two relations where we can apply structural induction again.

     * $\lambda x. Px \to P$, where $x \not\in \mathrm{FV}(P)$ and we can apply extensionality for $\rhd$
       and reflexivity.

  2) We can prove this by induction on any derivation of $M \rhd M'$. The
     possible ways in which we arrive at this are
     
     * the trivial one, reflexivity.

     * parallel application $NP \rhd N'P'$, where, by induction, we have $P \tto P'$ 
       and $N \tto N'$. Using two steps, $NP \tto N'P \tto N'P'$ we prove $NP \tto N'P'$.

     * congruence to \lambda-abstraction $\lambda x.N \rhd \lambda x.N'$, where, by induction,
       we know that $N \tto N'$, so $\lambda x.N \tto \lambda x.N'$.

     * parallel substitution, $(\lambda x.P)N \rhd P'[N'/x]$, where, by induction,
       we know that $P \tto P'$ and $N\tto N'$. Using multiple steps,
       $(\lambda x.P)N \tto (\lambda x.P')N \tto (\lambda x.P')N' \to P'[N'/x]$.

     * extensionality, $\lambda x.P x \rhd P'$, where by induction $P \tto P'$, and trivially,
       $\lambda x.Px \tto \lambda x.P'x$.

Because of this, the reflexive transitive closure of $\rhd$ should be a subset and a
superset of $\tto$ at the same time.
#+end_proof

#+attr_latex: :options [Substitution Lemma]
#+begin_lemma
<<lemma-subsl>>
Assuming $M \rhd M'$ and $U \rhd U'$, $M[U/y] \rhd M'[U'/y]$.
#+end_lemma
#+begin_proof
We apply structural induction on derivations of $M \rhd M'$, depending
on what the last rule we used to derive it was.

  * Reflexivity, $M = x$. If $x=y$, we simply use $U \rhd U'$; if $x \neq y$,
    we use reflexivity on $x$ to get $x \rhd x$.

  * Parallel application. By induction hypothesis, $P[U/y] \rhd P'[U'/y]$ and
    $N[U/y]\rhd N'[U'/y]$, hence $(PN)[U/y] \rhd (P'N')[U'/y]$.

  * Congruence. By induction, $N[U/y] \rhd N'[U'/y]$ and $\lambda x.N[U/y] \rhd \lambda x.N'[U'/y]$.

  * Parallel substitution. By induction, $P[U/y] \rhd P'[U'/y]$ and $N[U/y] \rhd N[U'/y]$,
    hence $((\lambda x.P)N)[U/y] \rhd P'[U'/y][N'[U'/y]/x] = P'[N'/x][U'/y]$.

  * Extensionality, given $x \notin \mathrm{FV}(P)$. By induction, $P \rhd P'$, hence
    $\lambda x.P[U/y]x \rhd P'[U'/y]$.

Note that we are implicitely assuming the Barendregt's variable convention; all
variables have been renamed to avoid clashes.
#+end_proof

#+attr_latex: :options [Maximal parallel one-step reduct]
#+begin_definition 
The *maximal parallel one-step reduct* $M^{\ast}$ of a \lambda-term $M$ is defined
inductively as

  * $x^{\ast} = x$;
  * $(PN)^{\ast} = P^{\ast}N^{\ast}$;
  * $((\lambda x.P)N)^{\ast} = P^{\ast}[N^{\ast}/x]$;
  * $(\lambda x.N)^{\ast} = \lambda x.N^{\ast}$;
  * $(\lambda x.Px)^{\ast} = P^{\ast}$, given $x \notin \mathrm{FV}(P)$.
#+end_definition

#+attr_latex: :options [Diamond property of parallel reduction]
#+begin_lemma
<<lemma-paralleldiamond>>
Given any $M'$ such that $M \rhd M'$, $M' \rhd M^{\ast}$. Parallel one-step reduction 
has the diamond property.
#+end_lemma
#+begin_proof
We apply again structural induction on the derivation of $M \rhd M'$.

  * Reflexivity gives us $M' = x = M^{\ast}$.

  * Parallel application. By induction, we have $P \rhd P^\ast$ and $N \rhd N^{\ast}$; depending
    on the form of $P$, we have

    - $P$ is not a \lambda-abstraction and $P'N' \rhd P^{\ast}N^{\ast} = (PN)^{\ast}$.

    - $P = \lambda x.Q$ and $P \rhd P'$ could be derived using congruence to \lambda-abstraction
      or extensionality. On the first case we know by induction hypothesis that $Q'\rhd Q^{\ast}$
      and $(\lambda x.Q')N' \rhd Q^{\ast}[N^{\ast}/x]$. On the second case, we can take $P = \lambda x.Rx$, where,
      $R \rhd R'$. By induction, $(R'x) \rhd (Rx)^{\ast}$ and now we apply the substitution lemma
      to have $R'N' = (R'x)[N'/x] \rhd (Rx)^{\ast}[N^{\ast}/x]$.

  * Congruence. Given $N \rhd N'$; by induction $N' \rhd N^{\ast}$, and depending on the form of
    $N$ we have two cases

    - $N$ is not of the form $Px$ where $x \not\in \mathrm{FV}(P)$; we can apply congruence to 
      \lambda-abstraction.

    - $N = Px$ where $x \notin \mathrm{FV}(P)$; and $N \rhd N'$ could be derived by parallel application
      or parallel substitution. On the first case, given $P \rhd P'$, we know that $P' \rhd P^{\ast}$
      by induction hypothesis and $\lambda x.P'x \rhd P^{\ast}$ by extensionality. On the second case,
      $N = (\lambda y.Q)x$ and $N' = Q'[x/y]$, where $Q \rhd Q'$. Hence $P \rhd \lambda y.Q'$, and by
      induction hypothesis, $\lambda y.Q' \rhd P^{\ast}$.

  * Parallel substitution, with $N \rhd N'$ and $Q \rhd Q'$; we know that $M^{\ast} = Q^{\ast}[N^{\ast}/x]$
    and we can apply the substitution lemma (lemma [[lemma-subsl]]) to get $M' \rhd M^{\ast}$.

  * Extensionality. We know that $P \rhd P'$ and $x \notin \mathrm{FV}(P)$, so by induction hypothesis
    we know that $P' \rhd P^{\ast} = M^{\ast}$.
#+end_proof

#+attr_latex: :options [Church-Rosser Theorem]
#+begin_theorem
<<theorem-churchrosser>>
The relation $\tto_{\beta\eta}$ is confluent.
#+end_theorem
#+begin_proof
Parallel reduction, $\rhd$, satisfies the diamond property (lemma [[lemma-paralleldiamond]]), 
which implies the Church-Rosser property. Its reflexive transitive closure is $\tto_{\beta\eta}$
(lemma [[lemma-transclosureparallel]]),
whose diamond property implies confluence for $\to_{\beta\eta}$.
#+end_proof

*** Normalization
Once the Church-Rosser theorem is proved, we can formally define the notion
of a normal form as a completely reduced \lambda-term.

#+attr_latex: :options [Normal forms]
#+begin_definition
A \lambda-term is said to be in *\beta-normal form* if \beta-reduction
cannot be applied to it or any of its subformulas. We define *\eta-normal forms*
and *\beta\eta-normal forms* analogously.
#+end_definition

Fully evaluating \lambda-terms usually means to apply reductions to them until
a normal form is reached. We know, by virtue of Theorem [[theorem-churchrosser]], 
that, if a normal form exists, it is unique; but we do not know whether a
normal form actually exists. We say that a term *has* a normal form
when it can be reduced to a normal form.

#+begin_definition
A term is *weakly normalizing* if there exists a sequence of reductions
from it to a normal form. It is *strongly* normalizing if every sequence
of reductions is finite.
#+end_definition

A consequence of Theorem [[theorem-churchrosser]] is that a term is weakly
normalizing if and only if it has a normal form. Strong normalization
implies also weak normalization, but the converse is not true; as an
example, the term
\[
\Omega = (\lambda x.(x x))(\lambda x.(x x))
\]
is neither weakly nor strongly normalizing; and the term
\[
(\lambda x.\lambda y.y)\ \Omega\ (\lambda x.x) \longrightarrow_{\beta} (\lambda x.x)
\]
is weakly normalizing but not strongly normalizing. Its normal form
is
\[
(\lambda x.\lambda y.y)\ \Omega\ (\lambda x.x) \longrightarrow_{\beta} (\lambda x.x).
\]

*** Standarization and evaluation strategies
# Barendregt, 1985, section 13.2

# Leftmost vs Rightmost evaluation
# Leftmost does always normalize if it is possible
# Rightmost only normalizes if it is necessary

# https://cs.stackexchange.com/questions/7702/applicative-order-and-normal-order-in-lambda-calculus
# This case illustrates a more general phenomenon: applicative order
# reduction only ever finds a normal form if the term is strongly
# normalizing, whereas normal order reduction always finds the normal
# form if there is one. This happens because applicative order always
# evaluates fully arguments first, and so misses the opportunity for
# an argument to turn out to be unused; whereas normal order evaluates
# arguments as late as possible, and so always wins if the argument
# turns out to be unused.

# Statement: http://www.nyu.edu/projects/barker/Lambda/barendregt.94.pdf
# Barendregt (1984) Theorem 13.2.2
We would like to find a \beta-reduction strategy such that, if a term
has a normal form, it can be found by following that strategy. Our
basic result will be the *standarization theorem*, which shows that,
if a \beta-reduction to a normal form exists, a sequence of
\beta-reductions from left to right on the \lambda-expression will be
able to find it. From this result, we will be able to prove that the
reduction strategy that always reduces the leftmost \beta-abstraction
will always find a normal form if it exists.

This section follows cite:kashima00, cite:barendsen94 and cite:barendregt84.

#+attr_latex: :options [Leftmost one-step reduction]
#+begin_definition
We define the relation $M \to_{n} N$ when $N$ can be obtained by \beta-reducing
the $n\text{-th}$ leftmost \beta-reducible application of the expression.
We call $\to_{1}$ the *leftmost one-step reduction* and we write it as $\to_{l}$;
$\tto_{l}$ is its reflexive transitive closure.
#+end_definition

#+attr_latex: :options [Standard sequence]
#+begin_definition
A sequence of \beta-reductions $M_0 \to_{n_1} M_1 \to_{n_2} M_2 \to_{n_3} \dots \to_{n_k} M_{k}$ 
is *standard* if $\{n_i\}$ is a non-decreasing sequence.
#+end_definition

We will prove that every term that can be reduced to a normal form can
be reduced to it using a standard sequence, from this theorem, the existence
of an optimal beta reduction strategy, in the sense that it will always find
the normal form if it exists, will follow as a corollary.

#+attr_latex: :options [Standarization theorem]
#+begin_theorem
If $M \tto_{\beta} N$, there exists a standard sequence from $M$ to $N$.
#+end_theorem
#+begin_proof
We start by defining the following two binary relations. The first one
is the relation given by the head reduction of the application and it
is defined by

  * $A \tto_h A$, reflexivity.
  * $(\lambda x.A_0)A_1 \dots \tto_{h} A_0[A_1/x]\dots$, head \beta-reduction.
  * $A \tto_{h} B \tto_{h} C$ implies $A \tto_{h} C$, transitivity.

The second one is the standard reduction and it is defined by

  * $M \tto_h x$ implies $M \tto_s x$, where $x$ is a variable.
  * if $M \tto_h AB$, $A \tto_s C$ and $B \tto_s D$, then $M \tto_s CD$.
  * if $M \tto_h \lambda x.A$ and $A \tto_s B$, then $M \to_s \lambda x.B$.

We can check the following trivial properties by structural induction

  1) $\tto_h$ implies $\tto_{l}$.
  2) $\tto_{s}$ implies the existence of a standard \beta-reduction.
  3) $\tto_{s}$ is reflexive, by induction on the structure of a term.
  4) if $M \tto_{h} N$, then $MP \tto_{h} NP$.
  5) if $M \tto_h N \tto_s P$, then $M \tto_{s} P$.
  6) if $M \tto_h N$, then $M[P/x] \tto_h N[P/x]$.
  7) if $M \tto_s N$ and $P \tto_s Q$, then $M[P/z] \tto_{s} N[Q/z]$.

Now we can prove that $K \tto_{s} (\lambda x.M)N$ implies $K \tto_s M[N/x]$.
From the fact that $K \tto_s (\lambda x.M)$, we know that there must exist $P$ and $Q$ such
that $K \tto_h \lambda PQ$, $P \tto_s \lambda x.M$ and $Q \tto_s N$; and from $P \tto_s \lambda x.M$, we know
that there exists $W$ such that $P \tto_h \lambda x.W$ and $W \tto_s M$. From all this information,
we can conclude that
\[
K \tto_h PQ \tto_{h} (\lambda x.W)Q \tto W[Q/x] \tto_s M[N/x];
\]
which, by (3.), implies $K \tto_s M[N/x]$.

We finally prove that, if $K \tto_s M \to_{\beta} N$, then $K \tto_s N$. This proves the theorem,
as every \beta-reduction $M \tto_s M \tto_\beta N$ implies $M \tto_s N$. We analize the possible
ways in which $M \to_{\beta} N$ can be derived.

  1) If $K \tto_{s} (\lambda x.M)N \to_{\beta} M[N/x]$, it has been
     already showed that $K \tto_s M[N/x]$.
  2) If $K \tto_s MN \to_{\beta} M'N$ with $M \to_{\beta} M'$, we know that there exist $K \tto_h WQ$ 
     such that $W \tto_s M$ and $Q \tto_s N$; by induction $W \tto_s M'$, and then $WQ \tto_s M'N$.
     The case $K \tto_s MN \to_{\beta} MN'$ is entirely analogous.
  3) If $K \tto_s \lambda x.M \to_{\beta} \lambda x.M'$, with $M \to_{\beta} M'$, we know that there exists $W$ such
     that $K \tto_h \lambda x.W$ and $W \tto_s M$. By induction $W \tto_s M'$, and $K \tto_s \lambda x.M'$.
#+end_proof

#+attr_latex: :options [Leftmost reduction theorem]
#+begin_corollary
<<cor-leftmosttheorem>>
We define the *leftmost reduction strategy* as the strategy that
reduces the leftmost \beta-reducible application at each step. If $M$ has a
normal form, the leftmost reduction strategy will lead to it.
#+end_corollary
#+begin_proof
Note that, if $M \to_n N$, where $N$ is in \beta-normal form; $n$ must be exactly
$1$. If $M$ has a normal form and $M \tto_{\beta} N$, there must exist a standard sequence
from $M$ to $N$ whose last step is of the form $\to_{l}$; as the sequence is non-decreasing,
every step has to be of the form $\to_{l}$.
#+end_proof

*** SKI combinators
**** SKI definition                                               :ignore:
As we have seen in previous sections, untyped \lambda-calculus is already
a very syntactically simple system; but it can be further reduced to
a few \lambda-terms without losing its expressiveness. In particular, untyped
\lambda-calculus can be /essentially/ recovered from only two of its terms;
these are

 * $S = \lambda x.\lambda y.\lambda z. xz(yz)$, and
 * $K = \lambda x.\lambda y.x$.

A language can be defined with these combinators and function
application. Every \lambda-term can be translated to this language and recovered up
to $=_{\beta\eta}$ equivalence. For example, the identity \lambda-term, $I$, can be written as
\[
I = \lambda x.x = SKK.
\]
It is common to also add the $I = \lambda x.x$ as a basic term to this language,
even if it can be written in terms of $S$ and $K$, as a
way to ease the writing of long complex terms. Terms written with
these combinators are called */Ski-terms/*.

The language of *Ski-terms* can be defined by the following Backus-Naus form
\[
\mathtt{Ski} ::= x \mid (\mathtt{Ski}\ \mathtt{Ski}) \mid S \mid K \mid I\quad,
\]
where $x$ are free variables.

**** Transformation of SKI combinators                            :ignore:
#+attr_latex: :options [Lambda transform]
#+begin_definition
The *Lambda-transform* of a Ski-term is a \lambda-term defined
recursively as

  * $\lambdatrans(x) = x$, for any variable $x$;
  * $\lambdatrans(I) = (\lambda x.x)$;
  * $\lambdatrans(K) = (\lambda x.\lambda y.x)$;
  * $\lambdatrans(S) = (\lambda x.\lambda y.\lambda z.xz(yz))$;
  * $\lambdatrans(XY) = \lambdatrans(X)\lambdatrans(Y)$.
#+end_definition

#+attr_latex: :options [Bracket abstraction]
#+begin_definition
The *bracket abstraction* of the Ski-term $U$ on the variable $x$ is
written as $[x].U$ and defined recursively as

  * $[x].x = I$;
  * $[x].M = KM$, if $x \notin \freevars(M)$;
  * $[x].Ux = U$, if $x \notin \freevars(U)$;
  * $[x].UV = S([x].U)([x].V)$, otherwise.

where $\freevars$ is the set of free variables; as defined on Definition
[[def-freevariables]].
#+end_definition

#+attr_latex: :options [Ski abstraction]
#+begin_definition
The *SKI abstraction* of a \lambda-term $M$, written as $\skiabs(M)$ is
defined recursively as

  * $\skiabs(x) = x$, for any variable $x$;
  * $\skiabs(MN) = \skiabs(M)\skiabs(N)$;
  * $\skiabs(\lambda x.M) = [x].\skiabs(M)$;

where $[x].U$ is the bracket abstraction of the Ski-term $U$.
#+end_definition

#+attr_latex: :options [Ski combinators and lambda terms]
#+begin_theorem
The Ski-abstraction is a retraction of the Lambda-transform of the term,
that is, for any Ski-term $U$,
\[
\skiabs(\lambdatrans(U)) = U.
\]
#+end_theorem
#+begin_proof
By structural induction on $U$,

  * $\skiabs\lambdatrans(x) = x$, for any variable $x$;
  * $\skiabs\lambdatrans(I) = [x].x = I$;
  * $\skiabs\lambdatrans(K) = [x].[y].x = [x].Kx = K$;
  * $\skiabs\lambdatrans(S) = [x].[y].[z].xz(yz) = [x].[y].Sxy = S$; and
  * $\skiabs\lambdatrans(MN) = MN$.
#+end_proof

In general this translation is not an isomorphism. As an example
\[
\lambdatrans(\skiabs(\lambda u. v u)) = \lambdatrans(v) = v.
\]
However, the \lambda-terms can be essentially recovered if we relax equality
between \lambda-terms to mean $=_{\beta\eta}$.
# This problem could be addressed by using a relaxed form of
# equality containing \eta-equivalence, see cite:Hindley08 for details.

#+ATTR_LATEX: :options [Recovering lambda terms from SKI combinators]
#+BEGIN_theorem
For any \lambda-term $M$,
\[
\lambdatrans(\skiabs(M)) =_{\beta\eta} M.
\]
#+END_theorem
#+BEGIN_proof
We can firstly prove by structural induction that $\lambdatrans([x].M) = \lambda x.\lambdatrans(M)$
for any $M$. In fact, we know that $\lambdatrans([x].x) = \lambda x.x$ for any 
variable $x$; we also know that
\[\begin{aligned}
\lambdatrans([x].MN) &= \lambdatrans(S([x].M)([x].N)) \\
          &= (\lambda x.\lambda y.\lambda z. xz(yz))(\lambda x.\lambdatrans(M))(\lambda x.\lambdatrans(N)) \\
          &= \lambda z.\lambdatrans(M)\lambdatrans(N);
\end{aligned}\]
also, if $x$ is free in $M$,
\[
\lambdatrans([x].M) = \lambdatrans(KM) = (\lambda x.\lambda y.x) \lambdatrans(M) =_{\beta} \lambda x.\lambdatrans(M);
\]
and finally, if $x$ is free in $U$,
\[
\lambdatrans([x].Ux) = \lambdatrans(U) =_{\eta} \lambda x.\lambdatrans(U)x\ .
\]
Now we can use this result to prove the main theorem. Again by
structural induction,

 * $\lambdatrans\skiabs(x) = x$;
 * $\lambdatrans\skiabs(MN) = \lambdatrans\skiabs(M)\lambdatrans\skiabs(N) = MN$;
 * $\lambdatrans\skiabs(\lambda x.M) = \lambdatrans([x].\skiabs(M)) =_{\beta\eta} \lambda x.\lambdatrans\skiabs(M) = \lambda x.M$.
#+END_proof

*** Turing completeness
# Turing, Church and Gödel.
# Papers by Turing, Church and Gödel.
# The lambda calculus as a reasonable machine. Ugo Dal Lago.

# https://en.wikipedia.org/wiki/Entscheidungsproblem

Three different notions of computability were proposed in the 1930s

 * the *general recursive functions* were defined by Herbrand and Gödel.
   They form a class of functions over the natural numbers closed under
   composition, recursion and unbound search.

 * the *\lambda-definable functions* were proposed by Church. They are
   functions on the natural numbers that can be represented by
   \lambda-terms.

 * the *Turing computable functions*, proposed by Alan Turing as the
   functions that can be defined on a theoretical model of a machine,
   the /Turing machines/.

In cite:church36 and cite:turing37, Church and Turing proved the equivalence of
the three definitions. This lead to the metatheoretical */Church-Turing thesis/*,
which postulated the equivalence between these models of computation and the
intuitive notion of /effective calculability/ mathematicians were using.
In practice, this means that the \lambda-calculus, as a programming language, is as
expressive as Turing machines; it can define every computable function.
It is Turing-complete.

# We will informally prove this equivalence: 
# a \lambda-calculus interpreter will be written in chapter ?, proving
# that \lambda-calculus is representable in a Turing machine
# equivalent, namely, our computer;
# general recursive functions will be implemented in \lambda-calculus
# in chapter ? proving that a Turing machine can be represented in it.
# interpreter and implementing general recursive functions on it.

A complete implementation of untyped \lambda-calculus is discussed in the
chapter on [[*Mikrokosmos][Mikrokosmos]]; and a detailed description on how to use the
untyped \lambda-calculus as a programming language is given in the chapter
''[[*Programming in the untyped \lambda-calculus][Programming in the untyped \lambda-calculus]]''.

# Church - An unsolvable problem of elementary number theory
# Corollary 1 pág 362.
# The set of well-formed formulas which have no normal form is not
# recursively enumerable.

** Simply typed \lambda-calculus
*/Types/* were introduced in mathematics as a response to the
Russell's paradox, found in the first naive axiomatizations of set
theory. An attempt to use the untyped \lambda-calculus as a foundational
logical system by Church suffered from the */Rosser-Kleene paradox/*, as
detailed in cite:kleene35 and cite:curry46; and types were a way to avoid it.
Once types are added, a deep connection between the \lambda-calculus and
logic arises. This connection will be discussed in the [[*The Curry-Howard correspondence][next chapter]].

In programming languages, types signal how the programmer intends to
use the data, prevent errors and enforce certain invariants and levels
of abstraction in programs. The role of types in the \lambda-calculus as
a programming language closely matches what we would expect of types
in any common programming language, and typed \lambda-calculus has been the basis
of many modern type systems for programming languages.

The *simply typed \lambda-calculus* (STLC) is a refinment of the untyped
\lambda-calculus. In it, each term has a type, which limits how it can be
combined with other terms. Only a set of basic types and function
types between any to types are considered in this system. If functions
in untyped \lambda-calculus could be applied over any term, now a function
of type $A \to B$ can only be applied over a term of type $A$, to produce
a new term of type $B$, where $A$ and $B$ could be, themselves, function types.

We will give now a presentation of the simply typed \lambda-calculus
based on cite:Hindley08. Our presentation will rely only on the
/arrow type constructor/ $\to$. While other presentations of simply typed
\lambda-calculus extend this definition with type constructors providing
pairs or union types, as it is done in cite:selinger13,
it seems clearer to present a first minimal version of the
\lambda-calculus. Such extensions will be explained later, and its exposition
will profit from the logical interpretation of propositions as types.

*** Simple types
We start assuming a set of *basic types*. Those basic types would
correspond, in a programming language interpretation, with the
fundamental types of the language. Examples would be the type of
strings or the type of integers. Minimal presentations of \lambda-calculus
tend to use only one basic type.

#+attr_latex: :options [Simple types]
#+begin_definition
The set of *simple types* is given by the following Backus-Naur form
\[\mathtt{Type} ::= 
\iota \mid 
\mathtt{Type} \to \mathtt{Type} \]
where $\iota$ would be any /basic type/.
#+end_definition

That is to say that, for every two types $A,B$, there exists a
*function type* $A \to B$ between them.

*** Typing rules for the simply typed \lambda-calculus
We will now define the terms of the simply typed \lambda-calculus
using the same constructors we used on the untyped version. Those are
the */raw typed \lambda-terms/*.

#+attr_latex: :options [Raw typed lambda terms]
#+begin_definition
The set of *typed lambda terms* is given by the BNF
\[ \mathtt{Term} ::=
x \mid
\mathtt{Term}\ \mathtt{Term} \mid
\lambda x^{\mathtt{Type}}. \mathtt{Term} \mid
\]
#+end_definition

The main difference here with Definition [[def-lambdaterms]] is 
that every bound variable has a type, and therefore, every \lambda-abstraction
of the form $(\lambda x^A. M)$ can be applied only over terms type $A$; if $M$ is of
type $B$, this term will be of type $A \to B$. 

However, the set of raw typed \lambda-terms contains some meaningless terms
under this type interpretation, such as $(\lambda x^A. M)(\lambda x^A. M)$.[fn:meaninglesstype]
*Typing rules* will give them the desired expressive power; only a subset
of these raw lambda terms will be typeable, and we will choose to work
only with that subset. When a particular term $M$ has type $A$, we write
this relation as $M : A$, where the $:$ symbol should be read as ''is of type''.

[fn:meaninglesstype]: In particular, we can not apply a term of type $A \to B$ to
a term of type $A \to B$; a term of type $A$ was expected.

**** Typing rules                                                 :ignore:
#+attr_latex: :options [Typing context]
#+begin_definition
A *typing context* is a sequence of typing assumptions
$x_1:A_1,\dots,x_n:A_n$, where no variable $x_{i}$ appears more than once.
We will implicitely assume that the order in which these
assumptions appear does not matter.
#+end_definition

Every typing rule assumes a typing context, usually denoted by $\Gamma$.
Concatenation of typing contexts is written as $\Gamma,\Gamma'$; and
the fact that $\psi$ follows from $\Gamma$ is written as $\Gamma \vdash \psi$.
Typing rules are written as rules of inference; the premises are
listed above and the conclusion is written below the line.

 1) The $(var)$ rule simply makes explicit the type of a variable from
    the context. That is, a context that assumes $x : A$ can
    be written as $\Gamma,x:A$; and we can trivially deduce from it that $x:A$.

    \begin{prooftree}
    \RightLabel{($var$)}
    \AXC{}
    \UIC{$\Gamma, x:A \vdash x:A$}
    \end{prooftree}

 2) The $(abs)$ rule declares that the type of a \lambda-abstraction is the type of
    functions from the variable type to the result type. If a term $M:B$ can
    be built from the assumption that $x:A$, then $\lambda x^{A}. M : A \to B$. It acts as
    a /constructor/ of function terms.

   \begin{prooftree}
   \RightLabel{$(abs)$}
   \AXC{$\Gamma, x:A \vdash M : B$}
   \UIC{$\Gamma \vdash \lambda x.M : A \to B$}
   \end{prooftree}

 3) The $(app)$ rule declares the type of a well-typed application.
    A term $f : A \to B$ applied to a term $a : A$ is a term
    $f\ a : B$. It acts as a /destructor/ of function terms.

    \begin{prooftree}
    \RightLabel{$(app)$}
    \AXC{$\Gamma \vdash f : A \to B$}
    \AXC{$\Gamma \vdash a : A$}
    \BIC{$\Gamma \vdash f a : B$}
    \end{prooftree}

#+begin_definition
A term is *typeable* if a typing judgment for the term is derivable.
#+end_definition

From now on, we only consider typeable terms as the terms of the
STLC. As a consequence, the set of \lambda-terms of the STLC is only a
subset of the terms of the untyped \lambda-calculus.

**** TODO Beta rules for STLC                                     :noexport:ignore:
# These rules are not necessary, we are only using function types.

Typeable terms follow the following set of \beta-reduction rules, which include
the untyped \beta-reduction rule defined in Definition [[def-betared]] and
add explicit reduction rules for the new types. Namely,

 * function application, $(\lambda x.M)N \to_{\beta} M[N/x]$;
 * first projection $\pi_{1} (M,N) \to_{\beta} M$; and
 * second projection $\pi_{2}_{} (M,N) \to_{\beta} M$.

These rules govern how can we compute with pair types. With this
rules $\tto_{\beta}$ and $=_{\beta}$ should be redefined accordingly.

**** Examples of typeable and non-typeable terms                  :ignore:
#+ATTR_LATEX: :options [Typeable and non-typeable terms]
#+BEGIN_exampleth
The term $\lambda f.\lambda x.f (f x)$ is typeable.
If we abbreviate $\Gamma = f:A \to A,\ x:A$, the detailed typing derivation
can be written as
\begin{prooftree}
\AX$\fCenter$
\RightLabel{$(var)$}
\UI$\Gamma\ \fCenter\vdash f : A \to A$
\AX$\fCenter$
\RightLabel{$(var)$}
\UI$\Gamma\ \fCenter\vdash x : A$
\AX$\fCenter$
\RightLabel{$(var)$}
\UI$\Gamma\ \fCenter\vdash f : A \to A$
\RightLabel{$(app)$}
\BI$\Gamma\ \fCenter\vdash f\ x : A$
\RightLabel{$(app)$}
\BI$f : A \to A, x : A\ \fCenter\vdash f (f x) : A$
\RightLabel{$(abs)$}
\UI$f : A \to A\ \fCenter\vdash \lambda x. f (f x) : A \to A$
\RightLabel{$(abs)$}
\UI$\fCenter\vdash \lambda f.\lambda x.f (f x) : (A \to A) \to A \to A$
\end{prooftree}
The term $(\lambda x.x\ x)$, however, is not typeable. If $x$ were of type $\psi$,
it also should be of type $\psi \to \sigma$ for some $\sigma$ in order for $x\ x$ to
be well-typed;
but $\psi \equiv \psi \to \sigma$ is not solvable, at it can be shown by structural
induction on the term $\psi$.
#+END_exampleth

It can be seen that the typing derivation of a term somehow encodes
the complete \lambda-term. If we were to derive the term bottom-up, there
would be only one possible choice at each step on which rule to use.
In the following sections we will discuss a type inference algorithm
which determines if a type is typeable and what its type should be,
and we will make precise these intuitions.

*** Curry-style types
Two different approaches to typing in \lambda-calculus are commonly used.

 * *Church-style* typing, also known as /explicit typing/, originated from
   the work of Alonzo Church in cite:church40, where he described a STLC
   with two basic types. The term's type is defined as an intrinsic property
   of the term; and the same term has to always be interpreted with the same
   type.

 * *Curry-style* typing, also known as /implicit typing/; which
   creates a formalism where every single term can be given an
   infinite number of types.  This technique is called
   */polymorphism/* when it is a formal part of the language; but
   here, it is only used allow us to intermediate terms without having to
   directly specify their type.

As an example, we can consider the identity term $I = \lambda x.x$. It would have to be 
defined for each possible type. That is, we should consider a family of different 
identity terms $I_A = \lambda x.x : A \to A$. Curry-style typing allow us to consider 
parametric types with type variables, and to type the identity as 
$I = \lambda x.x : \sigma \to \sigma$ where $\sigma$ would a free type variable.

#+attr_latex: :options [Type variables]
#+begin_definition
Given a infinite numerable set of /type variables/, we define *parametric types*
or /type-schemes/ inductively as
\[\mathtt{PType} ::= 
\iota \mid
\mathtt{Tvar} \mid 
\mathtt{PType} \to \mathtt{PType}, \]
where $\iota$ is a basic type, $\mathtt{Tvar}$ is a type variable and $\mathtt{PType}$ is
a parametric type.
That is, all basic types and type variables are atomic parametric types; and we also
consider the arrow type between two parametric types.
#+end_definition

The difference between the two typing styles is then not a mere notational
convention, but a difference on the expressive power that we assign to each
term. The interesting property of type variables is that they can act as
placeholders for other type templates. This is formalized with the notion
of type substitution.

#+attr_latex: :options [Type substitution]
#+begin_definition
A *substitution* $\psi$ is any function from type variables to type templates. It can
be applied to a type template as $\overline{\psi}$ by recursion and knowing that

   * $\overline{\psi} \iota = \iota$,
   * $\overline{\psi} \sigma = \psi \sigma$,
   * $\overline{\psi} (A \to B) = \overline{\psi} A \to \overline{\psi} B$.

That is, the type template $\overline{\psi} A$ is the same as $A$ but with every type variable
replaced according to the substitution $\sigma$.
#+end_definition

We consider a type to be /more general/ than other if the latter can be obtained by
applying a substitution to the former. In this case, the latter is called an /instance/
of the former. For instance, $A \to B$ is more general than its instance
$(C \to D) \to B$, where $A$ has been instantiated to $C \to D$. An
interesting property of STLC is that every type has a most general type, called
its /principal type/. This is proved in Theorem [[thm-typeinfer]].

#+attr_latex: :options [Principal type]
#+begin_definition
A closed \lambda-term $M$ has a *principal type* $\pi$ if $M : \pi$ and given any
$M : \tau$, we can obtain $\tau$ as an instance of $\pi$, that is, $\overline{\sigma} \pi = \tau$.
#+end_definition

*** Unification and type inference
**** Unification :ignore:
The unification of two type templates is the construction of two substitutions
making them equal as type templates; i.e., the construction of a type that
is a particular instance of both at the same time. We will not only aim for
an unifier but for the most general one between them.

#+attr_latex: :options [Most general unifier]
#+begin_definition
A substitution $\psi$ is called an *unifier* of two sequences of type templates
$\left\{ A_i \right\}_{i = 1,\dots,n}$ and $\left\{ B_i \right\}_{i=1,\dots,n}$ if $\overline{\psi} A_i = \overline{\psi} B_i$ for any $i$. We say that it
is the *most general unifier* if given any other unifier $\phi$ exists a substitution
$\varphi$ such that $\phi = \overline{\varphi} \circ \psi$.
#+end_definition

#+begin_lemma
<<lemma-unification>>
If an unifier of $\left\{ A_i \right\}_{i = 1,\dots,n}$ and $\left\{ B_i \right\}_{i=1,\dots,n}$ exists, the most general unifier
can be found using the following recursive definition of $\mathtt{unify}(A_1,\dots,A_n;B_1,\dots,B_n)$.

  1) $\mathtt{unify}(x;x) = \id$ and $\mathtt{unify}(\iota,\iota) = \id$;
  2) $\mathtt{unify}(x;B) = (x \mapsto B)$, the substitution that only changes $x$ by $B$;
     if $x$ does not occur in $B$. The algorithm *fails* if $x$ occurs in $B$;
  3) $\mathtt{unify}(A;x)$ is defined symmetrically;
  4) $\mathtt{unify}(A \to A'; B \to B') = \mathtt{unify}(A,A';B,B')$;
  5) $\mathtt{unify}(A,A_1,\dots; B,B_1,\dots) = \overline{\psi} \circ \rho$ where $\rho = \mathtt{unify}(A_1,\dots;B_1,\dots)$ 
     and $\psi = \mathtt{unify}(\overline{\rho}A; \overline{\rho}B)$;
  6) $\mathtt{unify}$ fails in any other case.

Where $x$ is any type variable. The two sequences of types have no unifier if and only
if $\mathtt{unify}(A,B)$ fails.
#+end_lemma
#+begin_proof
It is easy to notice that, by structural induction, if
$\mathtt{unify}(A;B)$ exists, it is in fact an unifier.

If the unifier fails in clause 2, there is obviously no possible unifier: the number
of constructors on the first type template will be always smaller than the second one.
If the unifier fails in clause 6, the type templates are fundamentally different, they
have different head constructors and this is invariant to substitutions. This proves
that the failure of the algorithm implies the non existence of an unifier.

We now prove that, if $A$ and $B$ can be unified, $\mathtt{unify}(A,B)$ is the most general unifier.
For instance, in the clause 2, if we call $\psi = (x \mapsto B)$ and, if $\eta$ were another unifier,
then $\eta x = \overline{\eta}x = \overline{\eta} B = \overline{\eta}(\psi(x))$; hence $\overline{\eta} \circ \psi = \eta$ by definition of $\psi$. A similar argument can 
be applied to clauses 3 and 4. In the clause 5, we suppose the existence of some unifier $\psi'$. 
The recursive call gives us the most general unifier $\rho$ of $A_1,\dots,A_n$ and $B_1,\dots,B_{n}$; and 
since it is more general than $\psi'$, there exists an $\alpha$ such that $\overline{\alpha} \circ \rho = \psi'$. Now,
$\overline{\alpha}(\overline{\rho}A) = \psi'(A) = \psi'(B) = \overline{\alpha}(\overline{\rho} B)$, hence $\alpha$ is a unifier of $\overline{\rho}A$ and $\overline{\rho}B$; we can take the 
most general unifier to be $\psi$, so $\overline{\beta} \circ \psi = \overline{\alpha}$; and finally, $\overline{\beta} \circ (\overline{\psi} \circ \rho) = \overline{\alpha} \circ \rho = \psi'$.

We also need to prove that the unification algorithm terminates. Firstly, we note that
every substitution generated by the algorithm is either the identity or it removes at least
one type variable. We can perform induction on the size of the argument on all clauses except
for clause 5, where a substitution is applied and the number of type variables is reduced.
Therefore, we need to apply induction on the number of type variables and only then apply
induction on the size of the arguments.
#+end_proof

**** Type Inference :ignore:
Using unification, we can define type inference.

#+attr_latex: :options [Type inference]
#+begin_theorem
<<thm-typeinfer>>
The algorithm $\mathtt{typeinfer}(M,B)$, defined as follows, finds the most general substitution $\sigma$
such that $x_1 : \sigma A_1, \dots, x_n : \sigma A_n \vdash M : \overline{\sigma} B$ is a valid typing judgment if it exists;
and fails otherwise.

  1) $\mathtt{typeinfer}(x_i:A_i,\Gamma \vdash x_i : B) = \mathtt{unify}(A_i,B)$;
  2) $\mathtt{typeinfer}(\Gamma \vdash MN : B) = \overline{\varphi} \circ \psi$, where $\psi = \mathtt{typeinfer}(\Gamma \vdash M : x \to B)$ and
     $\varphi = \mathtt{typeinfer}(\overline{\psi}\Gamma \vdash N : \overline{\psi}x)$ for a fresh type variable $x$.
  3) $\mathtt{typeinfer}(\Gamma \vdash \lambda x.M : B) = \overline{\varphi} \circ \psi$ where $\psi = \mathtt{unify}(B; z \to z')$ and
     $\varphi = \mathtt{typeinfer}(\overline{\psi}\Gamma, x:\overline{\psi}z \vdash M : \overline{\psi}z')$ for fresh type variables $z,z'$.

Note that the existence of fresh type variables is always asserted by the set of
type variables being infinite. The output of this algorithm is defined up to
a permutation of type variables.
#+end_theorem
#+begin_proof
The algorithm terminates by induction on the size of $M$. It is easy to check
by structural induction that the inferred type judgments are in fact valid.
If the algorithm fails, by Lemma [[lemma-unification]], it is also clear that the
type inference is not possible.

On the first case, the type is obviously the most general substitution
by virtue of the previous Lemma [[lemma-unification]].  On the second
case, if $\alpha$ were another possible substitution, in particular, it should
be less general than $\psi$, so $\alpha = \beta \circ \psi$. As $\beta$ would be then a possible substitution
making $\overline{\psi}\Gamma \vdash N : \overline{\psi}x$ valid, it should be less general than $\varphi$, so 
$\alpha = \overline{\beta} \circ \psi = \overline{\gamma} \circ \overline{\varphi} \circ \beta$.
On the third case, if $\alpha$ were another possible substitution, it should unify
$B$ to a function type, so $\alpha = \overline{\beta} \circ \psi$. Then $\beta$ should make the type inference
$\overline{\psi}\Gamma, x:\overline{\psi}z \vdash M : \overline{\psi}z'$ possible, so $\beta = \overline{\gamma} \circ \varphi$.
We have proved that the inferred type is in general the most general one.
#+end_proof

#+attr_latex: :options [Principal type property]
#+begin_corollary
Every typeable pure \lambda-term has a principal type.
#+end_corollary
#+begin_proof
Given a typeable term $M$, we can compute $\mathtt{typeinfer}(x_1:A_1,\dots,x_n:A_n \vdash M : B)$,
where $x_1,\dots,x_n$ are the free variables on $M$ and $A_1,\dots,A_n,B$ are fresh type
variables. By virtue of Theorem [[thm-typeinfer]], the result is the most general type of $M$
if we assume the variables to have the given types.
#+end_proof

*** Subject reduction and normalization
A crucial property is that type inference and \beta-reductions do not
interfere with each other. A term can be \beta-reduced without changing
its type.

#+attr_latex: :options [Subject reduction]
#+begin_theorem
The type is preserved on \beta-reductions; that is, if $\Gamma \vdash M : A$ and
and $M \tto_{\beta} M'$, then $\Gamma \vdash M' : A$.
#+end_theorem
#+begin_proof
If $M$ has been derived by \beta-reduction, $M = (\lambda x.P)$
and $M' = P[Q/x]$. $\Gamma \vdash M:A$ implies $\Gamma,x:B \vdash P : A$ and
$\Gamma \vdash Q : B$. Again by structural induction on $P$ (where the only crucial
case uses that $x$ and $Q$ have the same type) we can prove
that substitutions do not alter the type and thus, $\Gamma,Q:B \vdash P[Q/x] : A$.
#+end_proof

We have seen previously that the term $\Omega = (\lambda x.xx)(\lambda x.xx)$ is
not weakly normalizing; but it is also non-typeable. In this section
we will prove that, in fact, every typeable term is strongly normalizing.
We start proving some lemmas about the notion of /reducibility/, which
will lead us to the Strong Normalization Theorem. This proof will
follow cite:girard89.

The notion of */reducibility/* is an abstract concept originally
defined by Tait in cite:tait67 which we will use to ease this
proof. It should not be confused with the notion of \beta-reduction.

#+ATTR_LATEX: :options [Reducibility]
#+BEGIN_definition
We inductively define the set of *reducible* terms of type $T$
for basic and arrow types.

 * If $t : T$ where $T$ a basic type, $t \in \redu_{T}$ if $t$ is strongly
   normalizable.

 * If $t : U \to V$, an arrow type, $t \in \redu_{U \to V}$ if $t\ u \in \redu_{V}$ for all
   $u \in \redu_{U}$.
#+END_definition

Properties of reducibility will be used directly in the Strong
Normalization Theorem. We prove three of them at the same time in
order to use mutual induction.

#+ATTR_LATEX: :options [Properties of reducibility]
#+BEGIN_proposition
<<prop-reducibilityprop>>
The following three properties hold;

  1. if $t \in \redu_{T}$, then $t$ is strongly normalizable;
  2. if $t \in \redu_{T}$ and $t \to_{\beta} t'$, $t' \in \redu_{T}$; and
  3. if $t$ is not a \lambda-abstraction and $t' \in \redu_{T}$ for every $t \to_{\beta} t'$,
     then $t \in \redu_{T}$.
#+END_proposition
#+BEGIN_proof
For basic types,

  1. holds trivially.

  2. holds by the definition of strong normalization.

  3. if any one-step \beta-reduction leads to a strongly normalizing term,
     the term itself must be strongly normalizing.

For arrow types,

  1. if $x : U$ is a variable, we can inductively apply (3) to get $x \in \redu_{U}$;
     then, $t\ x \in \redu_{V}$ is strongly normalizing and $t$ in particular must be 
     strongly normalizing.

  2. if $t \to_{\beta} t'$ then for every $u \in \redu_{U}$, $t\ u \in \redu_{V}$ and $t\ u \to_{\beta} t'\ u$.
     By induction, $t'\ u \in \redu_{V}$.

  3. if $u \in \redu_{U}$, it is strongly normalizable. As $t$ is not a \lambda-abstraction,
     he term $t\ u$ can only be reduced to $t'\ u$ or $t\ u'$. If $t \to_{\beta} t'$; by induction, $t'\ u \in \redu_{V}$.
     If $u \to_{\beta} u'$, we could proceed by induction over the length of the longest
     chain of \beta-reductions starting from $u$ and assume that $t\ u'$ is irreducible.
     In every case, we have proved that $t\ u$ only reduces to already reducible terms;
     thus, $t\ u \in \redu_{U}$.
#+END_proof

#+ATTR_LATEX: :options [Abstraction lemma]
#+BEGIN_lemma
<<lemma-reductionabstraction>>
If $v[u/x] \in \redu_{V}_{}$ for all $u \in \redu_{U}$, then $\lambda x.v \in \redu_{U \to V}_{}_{}$.
#+END_lemma
#+BEGIN_proof
We apply induction over the sum of the lengths of the longest
\beta-reduction sequences from $v[x/x]$ and $u$. The term $(\lambda x.v) u$ can be \beta-reduced to

  * $v[u/x] \in \redu_{U}$; in the base case of induction, this is the only choice.
  * $(\lambda x.v')u$ where $v \to_{\beta }v'$, and, by induction, $(\lambda x.v') u \in \redu_{V}$.
  * $(\lambda x.v)u'$ where $u \to_{\beta} u'$, and, again by induction, $(\lambda x.v) u' \in \redu_{V}$.

Thus, by Proposition [[prop-reducibilityprop]], $(\lambda x.v) \in \redu_{U \to V}$.
#+END_proof

A final lemma is needed before the proof of the Strong Normalization Theorem.
It is a generalization of the main theorem, useful because of the stronger
induction hypothesis it provides.

#+ATTR_LATEX: :options [Strong Normalization lemma]
#+BEGIN_lemma
<<lemma-strongnormalization>>
Given an arbitrary $t : T$ with free variables $x_{1} : U_{1}, \dots, x_{n} : U_{n}$, and reducible
terms $u_{1} \in \redu_{U_1}, \dots, u_{n} \in \redu_{U_{2}}$, we know that
\[
t[u_1 / x_1][u_2 / x_{2}]\dots[u_n / x_n] \in \redu_{T}.
\]
#+END_lemma
#+BEGIN_proof
We call $\tilde{t} = t[u_1 / x_1][u_2 / x_{2}]\dots[u_n / x_n]$ and apply structural induction over $t$,

  * if $t = x_i$, then we simply use that $u_i \in \redu_{U_i}$.

  * if $t = v\ w$, then we apply induction hypothesis to get $\tilde{v} \in \redu_{R \to T},\tilde{w} \in \redu_{R}$ 
    for some type $R$. Then, by definition, $\tilde{t} = \tilde{v}\ \tilde{w} \in \redu_T$.

  * if $t = \lambda y. v : R \to S$, then by induction $\tilde{v}[r/y] \in \redu_S$ for every $r : R$.
    We can then apply Lemma [[lemma-reductionabstraction]] to get that
    $\tilde{t} = \lambda y.\tilde{v} \in \redu_{R \to S}$.
#+END_proof

#+attr_latex: :options [Strong Normalization Theorem]
#+begin_theorem
In the STLC, all terms are strongly normalizing.
#+end_theorem
#+BEGIN_proof
It is the particular case of Lemma [[lemma-strongnormalization]] where we
take $u_i = x_i$.
#+END_proof

# [[https://math.stackexchange.com/questions/1319149/what-breaks-the-turing-completeness-of-simply-typed-lambda-calculus][What breaks turing completeness of STLC]] (link)
Every term normalizes in the STLC and every computation ends.  We
know, however, that the Halting Problem is unsolvable, so the STLC
must be not Turing complete.

** The Curry-Howard correspondence
# Tutorial on Curry-Howard http://purelytheoretical.com/papers/ATCHC.pdf
# Local soundness and completeness http://www.cs.cmu.edu/~fp/courses/15816-s10/lectures/01-judgments.pdf
# https://www.elsevier.com/books/lectures-on-the-curry-howard-isomorphism/sorensen/978-0-444-52077-7

*** Extending the simply typed \lambda-calculus
We will add now special syntax for some terms and types, such as
pairs, unions and unit types. This syntax will make our \lambda-calculus
more expressive, but the unification and type inference algorithms
will continue to work. The previous proofs and algorithms can be extended to cover
all the new cases.
# And this is done on the mikrokosmos implementation

#+attr_latex: :options [Simple types II]
#+begin_definition
The new set of *simple types* is given by the following BNF
\[\mathtt{Type} ::= \iota \mid 
\mathtt{Type} \to \mathtt{Type} \mid
\mathtt{Type} \times \mathtt{Type} \mid
\mathtt{Type} + \mathtt{Type} \mid
1 \mid
0,\]
where $\iota$ would be any /basic type/.
#+end_definition

That is to say that, for any given types $A,B$, there exists a product
type $A \times B$, consisting of the pairs of elements where the first
one is of type $A$ and the second one of type $B$; there exists the
union type $A + B$, consisting of a disjoint union of tagged terms
from $A$ or $B$; an unit type $1$ with only an element, and an empty
or void type $0$ without inhabitants. The raw typed \lambda-terms are
extended to use these new types.

#+attr_latex: :options [Raw typed lambda terms II]
#+begin_definition
The new set of raw *typed lambda terms* is given by the BNF
\[\begin{aligned} 
\mathtt{Term} ::=\ &
x \mid
\mathtt{Term}\mathtt{Term} \mid
\lambda x. \mathtt{Term} \mid \\&
\left\langle \mathtt{Term},\mathtt{Term} \right\rangle \mid
\pi_1 \mathtt{Term} \mid
\pi_2 \mathtt{Term} \mid \\&
\textrm{inl}\ \mathtt{Term} \mid
\textrm{inr}\ \mathtt{Term} \mid
\textrm{case}\ \mathtt{Term}\ \textrm{of}\ \mathtt{Term}; \mathtt{Term} \mid \\&
\textrm{abort}\ \mathtt{Term} \mid \ast
\end{aligned}\]
#+end_definition

The use of these new terms is formalized by the following extended set
of typing rules.

 1) The $(var)$ rule simply makes explicit the type of a variable from
    the context.
    \begin{prooftree}
    \LeftLabel{($var$)}
    \AXC{}
    \UIC{$\Gamma, x:A \vdash x:A$}
    \end{prooftree}

 2) The $(abs)$ gives the type of a \lambda-abstraction as the type of
    functions from the variable type to the result type. It acts as
    a constructor of function terms.
    \begin{prooftree}
    \LeftLabel{$(abs)$}
    \AXC{$\Gamma, x:A \vdash M : B$}
    \UIC{$\Gamma \vdash \lambda x.M : A \to B$}
    \end{prooftree}

 3) The $(app)$ rule gives the type of a well-typed application of a
    lambda term. A term $f : A \to B$ applied to a term $a : A$ is a term
    of type $B$. It acts as a destructor of function terms.
    \begin{prooftree}
    \LeftLabel{$(app)$}
    \AXC{$\Gamma \vdash f : A \to B$}
    \AXC{$\Gamma \vdash a : A$}
    \BIC{$\Gamma \vdash f a : B$}
    \end{prooftree}

 4) The $(pair)$ rule gives the type of a pair of elements. It acts as
    a constructor of pair terms.
    \begin{prooftree}
    \LeftLabel{$(pair)$}
    \AXC{$\Gamma \vdash a : A$}
    \AXC{$\Gamma \vdash b :  B$}
    \BIC{$\Gamma \vdash \pair{a,b} : A \times B$}
    \end{prooftree}

 5) The $(\pi_1)$ rule extracts the first element from a pair. It acts as
    a destructor of pair terms.
    \begin{prooftree}
    \LeftLabel{$(\pi_1)$}
    \AXC{$\Gamma \vdash m : A \times B$}
    \UIC{$\Gamma \vdash \pi_1\ m : A$}
    \end{prooftree}

 6) The $(\pi_1)$ rule extracts the second element from a pair. It acts as
    a destructor of pair terms.
    \begin{prooftree}
    \LeftLabel{$(\pi_2)$}
    \AXC{$\Gamma \vdash m : A \times B$}
    \UIC{$\Gamma \vdash \pi_2\ m : B$}
    \end{prooftree}

 7) The $(inl)$ rule creates a union type from the left side type of
    the sum. It acts as a constructor of union terms.
    \begin{prooftree}
    \LeftLabel{$(inl)$}
    \AXC{$\Gamma \vdash a : A$}
    \UIC{$\Gamma \vdash \mathrm{inl}\ a : A + B$}
    \end{prooftree}

 8) The $(inr)$ rule creates a union type from the right side type of
    the sum. It acts as a constructor of union terms.
    \begin{prooftree}
    \LeftLabel{$(inr)$}
    \AXC{$\Gamma \vdash b : B$}
    \UIC{$\Gamma \vdash \mathrm{inr}\ b : A + B$}
    \end{prooftree}

 9) The $(case)$ rule extracts a term from an union and applies the appropiate
    deduction on any of the two cases
    \begin{prooftree}
    \LeftLabel{$(case)$}
    \AXC{$\Gamma \vdash m : A + B$}
    \AXC{$\Gamma, a:A \vdash n : C$}
    \AXC{$\Gamma, b:B \vdash p : C$}
    \TIC{$\Gamma \vdash (\mathrm{case}\ m\ \mathrm{of}\ [a].n;\ [b].p) : C$}
    \end{prooftree}

 10) The $(\ast)$ rule simply creates the only element of $1$. It is a constructor
     of the unit type.
     \begin{prooftree}
     \LeftLabel{$(\ast)$}
     \AXC{$$}
     \UIC{$\Gamma \vdash \ast : 1$}
     \end{prooftree}

 11) The $(abort)$ rule extracts a term of any type from the void type.
     \begin{prooftree}
     \LeftLabel{$(abort)$}
     \AXC{$\Gamma \vdash M : 0$}
     \UIC{$\Gamma \vdash \mathrm{abort}_A\ M : A$}
     \end{prooftree} 

     The abort function must be understood as the unique function going
     from the empty set to any given set.

The \beta-reduction of terms is defined the same way as for the untyped
\lambda-calculus; except for the inclusion of \beta-rules governing the
new terms, each for every new destruction rule.

  1) Function application, $(\lambda x.M)N \to_{\beta} M[N/x]$.
  2) First projection, $\pi_1 \left\langle M,N \right\rangle \to_{\beta} M$.
  3) Second projection, $\pi_2 \left\langle M,N \right\rangle \to_{\beta} N$.
  4) Case rule, $(\mathrm{case}\ m\ \mathrm{of}\ [a].N;\ [b].P) \to_{\beta} N a$ if $m$ is of the form $m = \mathrm{inl}\ a$; and
     $(\mathrm{case}\ m\ \mathrm{of}\ [a].N;\ [b].P) \to_{\beta} P b$ if $m$ is of the form $m = \mathrm{inr}\ b$.

On the other side, new \eta-rules are defined, each for every new construction rule.

  1) Function extensionality, $\lambda x.M x \to_{\eta} M$.
  2) Definition of product, $\langle \pi_1 M, \pi_{2} M \rangle \to_{\eta} M$.
  3) Uniqueness of unit, $M \to_{\eta} \ast$.
  4) Case rule, $(\mathrm{case}\ m\ \mathrm{of}\ [a].P[ \mathrm{inl}\ a/c ];\ [b].P[ \mathrm{inr}\ b/c ]) \to_{\eta} P[m/c]$.

*** Natural deduction
The natural deduction is a logical system due to Gentzen. We introduce
it here following cite:selinger13 and cite:wadler15. It relationship
with the STLC will be made explicit on the [[*Propositions as types][next section]].

We will use the logical binary connectives $\to,\land,\lor$, and two
given propositions, $\top,\bot$ representing truth and falsity. The
rules defining natural deduction come in pairs; there are introductors
and eliminators for every connective. Every introductor uses a set of
assumptions to generate a formula and every eliminator gives a way to
extract precisely that set of assumptions.

 1) Every axiom on the context can be used.

    \begin{prooftree}
    \RightLabel{(Ax)}
    \AXC{}
    \UIC{$\Gamma,A \vdash A$}
    \end{prooftree}

 2) Introduction and elimination of the $\to$ connective. Note that the
    elimination rule corresponds to /modus ponens/.

    \begin{prooftree}
    \RightLabel{($I_{\to}$)}
    \AXC{$\Gamma, A \vdash B$}
    \UIC{$\Gamma \vdash A \to B$}
    \RightLabel{($E_{\to}$)}
    \AXC{$\Gamma \vdash A \to B$}
    \AXC{$\Gamma \vdash A$}
    \BIC{$\Gamma \vdash B$}
    \noLine
    \BIC{}
    \end{prooftree}

 3) Introduction and elimination of the $\land$ connective. Note that the
    introduction in this case takes two assumptions, and there are
    two different elimination rules.

    \begin{prooftree}
    \RightLabel{($I_{\land}$)}
    \AXC{$\Gamma \vdash A$}
    \AXC{$\Gamma \vdash B$}
    \BIC{$\Gamma \vdash A \land B$}
    \RightLabel{($E_{\land}^1$)}
    \AXC{$\Gamma \vdash A \land B$}
    \UIC{$\Gamma \vdash A$}
    \RightLabel{($E_{\land}^2$)}
    \AXC{$\Gamma \vdash A \land B$}
    \UIC{$\Gamma \vdash B$}
    \noLine
    \TIC{}
    \end{prooftree}

 4) Introduction and elimination of the $\lor$ connective. Here, we need
    two introduction rules to match the two assumptions we use on the
    eliminator.

    \begin{prooftree}
    \RightLabel{($I_{\lor}^1$)}
    \AXC{$\Gamma \vdash A$}
    \UIC{$\Gamma \vdash A \lor B$}
    \RightLabel{($I_{\lor}^2$)}
    \AXC{$\Gamma \vdash B$}
    \UIC{$\Gamma \vdash A \lor B$}
    \RightLabel{($E_{\lor}$)}
    \AXC{$\Gamma \vdash A \lor B$}
    \AXC{$\Gamma,A \vdash C$}
    \AXC{$\Gamma,B \vdash C$}
    \TIC{$\Gamma \vdash C$}
    \noLine
    \TIC{}
    \end{prooftree}

 5) Introduction for $\top$. It needs no assumptions and, consequently,
    there is no elimination rule for it.

    \begin{prooftree}
    \RightLabel{($I_{\top}$)}
    \AXC{}
    \UIC{$\Gamma \vdash \top$}
    \end{prooftree}

 6) Elimination for $\bot$. It can be eliminated in all generality, and,
    consequently, there are no introduction rules for it. This elimination
    rule represents the /"ex falsum quodlibet"/ principle that says that
    falsity implies anything.

    \begin{prooftree}
    \RightLabel{($E_{\bot}$)}
    \AXC{$\Gamma \vdash \bot$}
    \UIC{$\Gamma \vdash C$}
    \end{prooftree}

Proofs on natural deduction are written as deduction trees, and they
can be simplified according to some simplification rules, which can
be applied anywhere on the deduction tree. On these rules, a chain
of dots represents any given part of the deduction tree.

  1) An implication and its antecedent can be simplified using the
     antecedent directly on the implication.

    \begin{prooftree}
    \AXC{$[A]$}\noLine
    \UIC{$\vdots^{1}$}\noLine
    \UIC{$B$}
    \UIC{$A \to B$}
    \AXC{$\vdots^2$}\noLine
    \UIC{$A$}
    \BIC{$B$}

    \UIC{$\vdots$}\noLine
    \AXC{$\Longrightarrow$}
    \UIC{}\noLine\UIC{}\noLine\UIC{}\noLine

    \AXC{$\vdots^{2}$}\noLine
    \UIC{$A$}\noLine
    \UIC{$\vdots^{1}$}\noLine
    \UIC{$B$}
    \UIC{$\vdots$}\noLine
    \noLine
    \TIC{}
    \end{prooftree}

  2) The introduction of an unused conjunction can be simplified
     as

    \begin{prooftree}
    \AXC{$\vdots^{1}$}\noLine
    \UIC{$A$}
    \AXC{$\vdots^{2}$}\noLine
    \UIC{$B$}
    \BIC{$A \land B$}
    \UIC{$A$}
    \UIC{$\vdots$}\noLine
    \AXC{$\Longrightarrow$}
    \UIC{}\noLine\UIC{}\noLine\UIC{}\noLine
    \AXC{$\vdots^{1}$}\noLine
    \UIC{$A$}
    \UIC{$\vdots$}\noLine
    \noLine
    \TIC{}
    \end{prooftree}

    and, similarly, on the other side as

    \begin{prooftree}
    \AXC{$\vdots^{1}$}\noLine
    \UIC{$A$}
    \AXC{$\vdots^{2}$}\noLine
    \UIC{$B$}
    \BIC{$A \land B$}
    \UIC{$B$}
    \UIC{$\vdots$}\noLine
    \AXC{$\Longrightarrow$}
    \UIC{}\noLine\UIC{}\noLine\UIC{}\noLine
    \AXC{$\vdots^{2}$}\noLine
    \UIC{$B$}
    \UIC{$\vdots$}\noLine
    \noLine
    \TIC{}
    \end{prooftree}

  3) The introduction of a disjunction followed by its elimination can
     be also simplified

    \begin{prooftree}
    \AXC{$\vdots^{1}$}\noLine
    \UIC{$A$}
    \UIC{$A+B$}
    \AXC{$[A]$}\noLine
    \UIC{$\vdots^2$}\noLine
    \UIC{$C$}
    \AXC{$[B]$}\noLine
    \UIC{$\vdots^3$}\noLine
    \UIC{$C$}
    \TIC{$C$}

    \UIC{$\vdots$}\noLine
    \AXC{$\Longrightarrow$}
    \UIC{}\noLine\UIC{}\noLine\UIC{}\noLine

    \AXC{$\vdots^{1}$}\noLine
    \UIC{$A$}\noLine
    \UIC{$\vdots^{2}$}\noLine
    \UIC{$C$}
    \UIC{$\vdots$}\noLine
    \noLine
    \TIC{}
    \end{prooftree}

    and a similar pattern is used on the other side of the disjunction

    \begin{prooftree}
    \AXC{$\vdots^{1}$}\noLine
    \UIC{$B$}
    \UIC{$A+B$}
    \AXC{$[A]$}\noLine
    \UIC{$\vdots^2$}\noLine
    \UIC{$C$}
    \AXC{$[B]$}\noLine
    \UIC{$\vdots^3$}\noLine
    \UIC{$C$}
    \TIC{$C$}

    \UIC{$\vdots$}\noLine
    \AXC{$\Longrightarrow$}
    \UIC{}\noLine\UIC{}\noLine\UIC{}\noLine

    \AXC{$\vdots^{1}$}\noLine
    \UIC{$B$}\noLine
    \UIC{$\vdots^{3}$}\noLine
    \UIC{$C$}
    \UIC{$\vdots$}\noLine
    \noLine
    \TIC{}
    \end{prooftree}

*** Propositions as types
In 1934, Curry observed in cite:curry34 that the type of a function
$(A \to B)$ could be read as an implication and that the existence of a
function of that type was equivalent to the provability of the proposition.
Previously, the *Brouwer-Heyting-Kolmogorov interpretation* of intuitionistic
logic had given a definition of what it meant to be a proof of an intuinistic
formula, where a proof of the implication $(A \to B)$ was a function converting
a proof of $A$ into a proof of $B$. It was not until 1969 that Howard pointed
a deep correspondence between the simply-typed \lambda-calculus and the
natural deduction at three levels

  1) propositions are types.
  2) proofs are programs.
  3) simplification of proofs is the evaluation of programs.

In the case of STLC and natural deduction, the correspondence starts when
we describe the following isomorphism between types and propositions.

\begin{center}\begin{tabular}{c|c}
Types & Propositions \\
\hline
Unit type ($1$) & Truth ($\top$) \\
Product type ($\times$) & Conjunction ($\land$) \\
Union type ($+$) & Disjunction ($\lor$) \\
Function type ($\to$) & Implication ($\to$) \\
Empty type ($0$) & False ($\bot$)
\end{tabular}\end{center}

Now it is easy to notice that every [[*Natural deduction][deduction rule]] for a proposition has a
correspondence with a [[*Extending the simply typed \lambda-calculus][typing rule]]. The only distinction between them is the
appearance of \lambda-terms on the first set of rules. As every typing rule
results on the construction of a particular kind of \lambda-term, they can
be interpreted as encodings of proof in the form of derivation trees. That is,
terms are proofs of the propositions represented by their types.

Under this interpretation, 
*/simplification rules are precisely the \beta-reduction rules/*.
This makes execution of \lambda-calculus
programs correspond to proof simplification on natural deduction.
The Curry-Howard correspondence is then not only a simple bijection
between types and propositions, but a deeper isomorphism regarding the
way they are constructed, used in derivations, and simplified.

#+attr_latex: :options [Curry-Howard example]
#+begin_exampleth
As an example of this duality, we will write a proof/term of the proposition/type =A → B + A=
and we are going to simplify/compute it using proof simplification rules/\beta-rules.
Similar examples can be found in cite:wadler15.

We start with the following derivation tree; in which terms are colored in
$\cterms{red}$ and types are colored in $\ctypes{blue}$
\begin{prooftree}\EnableBpAbbreviations
\AXC{$\cterms{b }\ctypes{:: [A+B]}$}
\AXC{$\cterms{c }\ctypes{:: A}$}
\RightLabel{$(inr)$}
\UIC{$\cterms{inr c }\ctypes{:: B+A}$}
\AXC{$\cterms{c }\ctypes{:: B}$}
\RightLabel{$(inl)$}
\UIC{$\cterms{inl c }\ctypes{:: B+A}$}
\RightLabel{$(case)$}
\TIC{$\cterms{case b of [c].inr c; [c].inl c }\ctypes{:: B+A}$}
\RightLabel{$(abs)$}
\UIC{$\cterms{λb.case b of [c].inr c; [c].inl c }\ctypes{:: A+B \to B+A}$}

\AXC{$\cterms{a }\ctypes{:: A}$}
\RightLabel{$(inl)$}
\UIC{$\cterms{inl a }\ctypes{:: A+B}$}
\RightLabel{$(app)$}
\BIC{$\cterms{(λb.case b of [c].inr c; [c].inl c)(inl a) }\ctypes{:: B+A}$}
\RightLabel{$(abs)$}
\UIC{$\cterms{λa.(λb.case b of [c].inr c; [c].inl c) (λa.inl a) }\ctypes{:: A \to B + A}$}
\end{prooftree}

which is encoded by the term =λa.(λc.case c of [a].inr a; [b].inl b) (λz.inl z)=.
We apply the simplification rule/\beta-rule of the implication/function application
to get

\begin{prooftree}\EnableBpAbbreviations
\AXC{$\cterms{z }\ctypes{:: A}$}
\RightLabel{$(inl)$}
\UIC{$\cterms{inl z }\ctypes{:: A+B}$}
\AXC{$\cterms{a }\ctypes{:: A}$}
\RightLabel{$(inr)$}
\UIC{$\cterms{inr a }\ctypes{:: B+A}$}
\AXC{$\cterms{b }\ctypes{:: B}$}
\RightLabel{$(inl)$}
\UIC{$\cterms{inl b }\ctypes{:: B+A}$}
\RightLabel{$(case)$}
\TIC{$\cterms{case (inl z) of [a].inr a; [b].inl b }\ctypes{:: B+A}$}
\RightLabel{$(abs)$}
\UIC{$\cterms{λ z.case (inl z) of [a].inr a; [b].inl b }\ctypes{:: A \to B +A}$}
\end{prooftree}

which is encoded by the term =λa.case (inl a) of (inr) (inl)=. We finally
apply the =case= simplification/reduction rule to get

\begin{prooftree}\EnableBpAbbreviations
\AXC{$\cterms{a }\ctypes{:: A}$}
\RightLabel{$(inr)$}
\UIC{$\cterms{inr a }\ctypes{:: B+A}$}
\RightLabel{$(abs)$}
\UIC{$\cterms{λ a.inr a }\ctypes{:: A \to B + A}$}
\end{prooftree}

which is encoded by =λa.(inr a)=.

On the chapter on [[*Mikrokosmos][Mikrokosmos]], we develop a \lambda-calculus interpreter
which is able to check and simplify proofs on intuitionistic logic.
This example could be checked and simplified by this interpreter as
it is shown in image [[mikrogentzen]]. 

#+caption: Curry-Howard example in Mikrokosmos.
#+name: mikrogentzen
[[./images/mikrogentzen.png]]
#+end_exampleth

# Extending the Curry-Howard correspondence
** Other type systems
*** TODO Hindley-Milner
*** TODO Gödel's System T
*** TODO System F                                                :noexport:
**** TODO System F is strongly normalizing
*** TODO Type algebra
# Type algebra should be studied, at least, on System F.
# Properties of type algebra can be proved in Agda.

**** Lists, trees and generating functions
**** Derivatives and one-hole contexts
**** Seven trees in one
*** \lambda-cube
The *\lambda-cube* is a taxonomy for Church-style type systems given
by Barendregt in cite:barendregt92. It describes eight type systems
based on the \lambda-calculus along three axes, representing three
properties of the systems. These properties are

  1) *parametric polymorphism*, terms that depend on types. This is
     achieved via universal quantification over types. It allows type
     variables and binders for them. An example is the following parametric
     identity function
     \[
     \mathrm{id} \equiv \Lambda \tau . \lambda x . x : \forall \tau . \tau \to \tau, 
     \]
     that can be applied to any particular type $\sigma$ to obtain the 
     specific identity function for that type as
     \[
     \mathrm{id}_{\sigma} \equiv \lambda x.x : \sigma \to \sigma.
     \]

     *System F* is the simplest type system on the cube implementing
     polymorphism.

  2) *type operators*, types that depend on types.

  3) *dependent types*, types that depend on terms.

# Pierce
# Lectures on the Curry-Howard isomorphism
# Introduction to generalized type systems - Barendregt

# https://en.wikipedia.org/wiki/System_F#System_F.CF.89

\[\begin{tikzcd}[column sep=small]
&&& |[label={above:\lcubett{System F$\omega$}}]| \systemfo \ar{rr}
&&  |[label=above:\lcubett{CoC},label=above:\phantom{System Fo}]| \systemcoc 
& \\
\phantom{.}  
&&  |[label={left:\lcubett{System F}}]| \systemf \ar{ur}\ar{rr} 
&&  \systemfp \ar{ur}
&& \\ 
&&& \systemo \ar{rr}\ar{uu} 
&&  |[label=right:\lcubett{wCoC}]| \systemlpo \ar{uu}
&&  \phantom{\lambda}\phantom{PQW}  \\
\ar[\lcred]{uu}[\lcred]{\text{\parbox{2cm}{\centering terms depend on types}}}
&&  |[label=below:\lcubett{STLC}]| \stlc \ar{uu}\ar{rr}\ar{ur} 
&&  |[label=below:\lcubett{DTLC}]| \systemlp \ar{uu}\ar{ur} 
&& \phantom{.} \\ 
&&  \ar[\lcred]{rr}[swap,\lcred]{\text{\parbox{2cm}{\centering types depend on terms}}} 
&& \phantom{.} 
& \ar[\lcred]{ur}[swap,\lcred]{\text{\parbox{2cm}{\centering types depend on types}}} 
&
\end{tikzcd}\]

The following type systems

 * *Simply typed \lambda-calculus* ($\stlc$);
 * *System F* ($\systemf$);
 * typed \lambda-calculus with *dependent types* ($\systemlp$);
 * typed \lambda-calculus with *type operators* ($\systemo$);
 * *System F-omega* ($\systemfo$);
   
The \lambda-cube is generalized by the theory of pure type systems.

All systems on the \lambda-cube are strongly normalizing.

# https://cstheory.stackexchange.com/questions/7561/whats-the-relation-and-difference-between-calculus-of-inductive-constructions-a
A different approach to higher-order type systems will be presented in the
chapter on Type Theory.

*** TODO Pure type systems
In particular *System F* is equivalent to the single-sorted pure system $\lambda 2$.
# https://www.ps.uni-saarland.de/extras/fscd17/

*** TODO Subtyping (?)

*** TODO Inductive and coinductive definitions
* Mikrokosmos (abstract)                                             :ignore:
#+LATEX: \ctparttext{\color{black}\begin{center}
We have developed *Mikrokosmos*, an untyped and simply typed \lambda-calculus interpreter
written in the purely functional programming language Haskell cite:hudak07_haskell.
It aims to provide students with a tool to learn and understand \lambda-calculus
and the relation between logic and types.
#+LATEX: \end{center}}

* Mikrokosmos
** Programming environment
*** The Haskell programming language
**** Haskell as a programming choice                              :ignore:
*Haskell* is the purely functional programming language of our choice
to implement Mikrokosmos, our \lambda-calculus interpreter. Its own
design is heavily influenced by the \lambda-calculus and is a
general-purpose language with a rich ecosystem and plenty of
consolidated libraries[fn:hackagelibs] in areas such as parsing,
testing or system interaction; matching the requisites of our
project. In the following sections, we describe this ecosystem in more
detail.

[fn:hackagelibs]: In the central package archive of the Haskell community,
Hackage, a categorized list of libraries can be found: https://hackage.haskell.org/packages/

**** History of Haskell                                           :ignore:
In the 1980s, many lazy programming languages were independently being
written by researchers such as /Miranda/, /Lazy ML/, /Orwell/, /Clean/
or /Daisy/. All of them were similar in expressive power, but their
differences were holding back the efforts to communicate ideas on
functional programming.  A comitee was created in 1987 with the
mission of designing a common lazy functional language. Several
versions of the language were developed, and the first standarized
reference of the language was published in the *Haskell 98 Report*,
whose revised version can be read on cite:haskell98. Its more popular
implementation is the *Glasgow Haskell Compiler (GHC)*; an open source
compiler written in Haskell and C. The complete history of Haskell and
its design decisions is detailed on cite:hudak07_haskell.

**** Haskell's properties                                         :ignore:
Haskell is

 1. *strongly and statically typed*, meaning that it only compiles
    well-typed programs and it does not allow implicit type
    casting. The compiler will generate an error if a term is
    non-typeable.

 2. *lazy*, with /non-strict semantics/, meaning that it will not
    evaluate a term or the argument of a function until it is needed.
    In cite:hughes89, John Hughes, codesigner of the language, argues
    for the benefits of a lazy functional language, which could solve
    the traditional efficiency problems on functional programming.

 3. *purely functional*. As the evaluation order is demand-driven and
    not explicitly known, it is not possible in practice to perform
    ordered input/output actions or any other side-effects by relying
    on the evaluation order. This helps modularity of the code,
    testing and verfication.

 4. *referentially transparent*. As a consequence of its purity, every
    term on the code could be replaced by its definition without
    changing the global meaning of the program. This allows equational
    reasoning with rules that are directly derived from \lambda-calculus.

 5. based on *System F\omega* with some restrictions. Crucially, it
    implements *System F* adding quantification over type operators
    even if it does not allow abstraction on type operators. The GHC
    Haskell compiler, however, allows the user the ability to activate
    extensions that implement dependent types.
    # https://stackoverflow.com/a/21220357/2552681

#+ATTR_LATEX: :options [A first example in Haskell]
#+BEGIN_exampleth
This example shows the basic syntax and how its type system and
its implicit laziness can be used.

#+BEGIN_SRC haskell
-- The type of the term can be declared.
-- Polymorphic type variables are allowed.
id :: a -> a
-- Functions are defined equationally
id x = x
-- This definition performs short circuit evaluation thanks
-- to laziness. The unused argument can be omitted.
(&&) :: Bool -> Bool -> Bool
True  && x = x
False && _ = False
-- Laziness also allows infinite data structures.
-- We can define the list of all natural numbers.
nats :: [Integer]
nats = 1 : map (+1) nats
#+END_SRC
#+END_exampleth

**** Haskell's syntax                                             :ignore:
Where most imperative languages use semicolons to separate sequential
commands, Haskell has no notion of sequencing, and programs are
written in a purely declarative way. A Haskell program essentially
consist on a series of definitions (of both types and terms) and type
declarations. The following example shows the definition of a binary
tree and its preorder as

#+BEGIN_SRC haskell
-- A tree is either empty or a node with two subtrees.
data Tree a = Empty | Node a (Tree a) (Tree a)
-- The preorder function takes a tree and returns a list
preorder :: Tree a -> [a]
preorder Empty            = []
preorder (Node x lft rgt) = preorder lft ++ [x] ++ preorder rgt
#+END_SRC

We can see on the previous example that function definitions allow
/pattern matching/, that is, data constructors can be used in
definitions to decompose values of the type. This increases readability
when working with algebraic data types.

While infix operators are allowed, function application is
left-associative in general. Definitions using partial application are
allowed, meaning that functions on multiple arguments can use currying
and can be passed only one of its arguments to define a new
function. For example, a function that squares every number on a list
could be written in two ways as

#+BEGIN_SRC haskell
squareList :: [Int] -> [Int]
squareList list = map square list
squareList' :: [Int] -> [Int]
squareList' = map square
#+END_SRC

where the second one, because of its simplicity, is usually
preferred. 

**** Type classes, monads                                         :ignore:
A characteristic piece of Haskell are *type classes*, which allow 
defining common interfaces for different types. In the following
example, we define =Monad= as the type class of types with suitably
typed =return= and =bind= operators.

#+BEGIN_SRC haskell
class Monad m where
  return :: a   -> m a
  (>>=)  :: m a -> (a -> m b) -> m b
#+END_SRC

And lists, for example, are monads in this sense.

#+BEGIN_SRC haskell
instance Monad [] where
  return x = [x]               -- returns a one-element list
  xs >>= f = concat (map f xs) -- map and concatenation
#+END_SRC

Haskell uses monads in varied forms. They are used in I/O, error
propagation and stateful computations. Another characteristical syntax
bit of Haskell is the =do= notation, which provides a nicer, cleaner
way to work with types that happen to be monads. The following example
uses the list monad to compute the list of Pythagorean triples.

#+BEGIN_SRC haskell
pythagorean = do
  a <- [1..]               -- let a be any natural
  b <- [1..a]              -- let b be a natural between 1 and a
  c <- [1..b]              -- let c be a natural between 1 and b
  guard (a^2 == b^2 + c^2) -- filter the list
  return (a,b,c)           -- return matching tuples
#+END_SRC

Note that this list is infinite. As the language is lazy, this does not
represent a problem: the list will be evaluated only on demand.

For a more detailed treatment of monads, and their relation to
categorical monads, see the chapter on Category Theory and the chapter
on Type Theory, where we will program with monads in Agda.

# [[http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.22.2636][CiteSeerX — Faking It: Simulating Dependent Types in Haskell]]
*** Cabal, Stack and Haddock
The Mikrokosmos documentation as a Haskell library is included
in its own code. It uses *Haddock*, a tool that generates documentation
from annotated Haskell code; it is the /de facto/ standard in for Haskell
software.

Dependencies and packaging details for Mikrokosmos are specified in
a file distributed with the source code called =mikrokosmos.cabal=.
It is used by the package managers *stack* and *cabal* to provide the
necessary libraries even if they are not available system-wide. The
*stack* tool is also used to package the software, which is uploaded
to /*Hackage*/.

*** Testing
*Tasty* is the Haskell testing framework of our choice for this
project. It allows the user to create a comprehensive test suite
combining multiple types of tests. The Mikrokosmos code is testing
using the following techniques

  * *unit tests*, in which individual core functions are tested
    independently of the rest of the application;

  * *property-based testing*, in which multiple test cases are
    created automatically in order to verfiy that a specified
    property always holds.

  * *golden tests*, a special case of unit tests in which the expected
    results of an IO action, as described on a file, are checked to
    match the actual ones.
    
We are using the *HUnit* library for unit tests. It tests particular
cases of type inference, unification and parsing. The following is an
example of unit test, as found in =tests.hs=. It checks that the type
inference of the identity term is correct.

#+BEGIN_SRC haskell
-- Checks that the type of λx.x is exactly A → A
testCase "Identity type inference" $
  typeinference (Lambda (Var 1)) @?= Just (Arrow (Tvar 0) (Tvar 0))
#+END_SRC

We are using the *QuickCheck* library for property-based tests. It
tests transformation properties of lambda expressions. In the following
example, it tests that any deBruijn expression keeps is meaning when
is translated into a \lambda-term.

#+BEGIN_SRC haskell
-- Tests that naming lambda expressions does not alter their meaning
QC.testProperty "Expression -> named -> expression" $
  \expr -> toBruijn emptyContext (nameExp expr) == expr
#+END_SRC

We are using the *tasty-golden* package for golden tests.  Mikrokosmos
can be passed a file as an argument to interpret it and show only the
results. This feature is used to create a golden test in which the
correct interpretation of a file.  This file is called =testing.mkr=,
and contains library definitions and multiple tests. Its expected
output is =testing.golden=. For example, the following Mikrokosmos
code can be found on the file

#+BEGIN_SRC haskell
:types on
caseof (inr 3) (plus 2) (mult 2)
#+END_SRC

and the expected output is

#+BEGIN_SRC haskell
-- types: on
-- λa.λb.(a (a (a (a (a (a b)))))) ⇒ 6 :: (A → A) → A → A
#+END_SRC

*** Version control and continuous integration
Mikrokosmos uses *git* as its version control system and the code,
which is licensed under GPLv3, can be publicly accessed on the
following GitHub repository:

#+begin_center
https://github.com/M42/mikrokosmos
#+end_center

Development takes place on the =development= git branch and permanent
changes are released into the =master= branch. Some more minor
repositories have been used in the development; they directly
depend on the main one

 * https://github.com/m42/mikrokosmos-js
 * https://github.com/M42/jupyter-mikrokosmos
 * https://github.com/M42/mikrokosmos-lib

The code uses the *Travis CI* continuous integration system to run
tests and check that the software builds correctly after each change
and in a reproducible way on a fresh Linux installation provided by
the service.
** Implementation of \lambda-expressions
*** De Bruijn indexes
Nicolaas Govert *De Bruijn* proposed in cite:debruijn81 a way of defining \lambda-terms modulo
\alpha-conversion based on indices.  The main idea of De Bruijn
indices is to remove all variables from binders and replace every
variable on the body of an expression with a number, called /index/,
representing the number of \lambda-abstractions in scope between the
ocurrence and its binder.

Consider the following example, the \lambda-term
\[ \lambda x.(\lambda y.\ y (\lambda z.\ y z)) (\lambda t.\lambda z.\ t x)
\]
can be written with de Bruijn indices as
\[
\lambda\ (\lambda(1 \lambda(2 1))\ \lambda\lambda(2 3)\ ).
\]

De Bruijn also proposed a notation for the \lambda-calculus
changing the order of binders and \lambda-applications.  A review on
the syntax of this notation, its advantages and De Bruijn indexes, can be found in
cite:kamareddine01. In this section, we are going to describe De Bruijn
indexes but preserve the usual notation of \lambda-terms; that is, the /De Bruijn/
/indexes/ and the /De Bruijn notation/ are different concepts and we are going to
use only the former.

#+attr_latex: :options [De Bruijn indexed terms]
#+begin_definition
We define recursively the set of \lambda-terms using de Bruijn notation
following this BNF
\[ \mathtt{Exp} ::= \mathbb{N}
 \mid (\lambda\ \mathtt{Exp})
 \mid (\mathtt{Exp}\ \mathtt{Exp})
\]
#+end_definition

Our internal definition closely matches the formal one. The names of
the constructors here are =Var=, =Lambda= and =App=:

#+BEGIN_SRC haskell
-- | A lambda expression using DeBruijn indexes.
data Exp = Var Integer -- ^ integer indexing the variable.
         | Lambda Exp  -- ^ lambda abstraction
         | App Exp Exp -- ^ function application
         deriving (Eq, Ord)
#+END_SRC

This notation avoids the need for the Barendregt's variable convention and
the \alpha-reductions. It will be useful to implement \lambda-calculus without
having to worry about the specific names of variables.

*** Substitution
We define the [[*Free and bound variables, substitution][substitution]] operation needed for the [[*\beta-reduction][\beta-reduction]] on
de Bruijn indices. In order to define the substitution of the n-th
variable by a \lambda-term $P$ on a given term, we must

 * find all the ocurrences of the variable. At each level of scope
   we are looking for the successor of the number we were looking
   for before.

 * decrease the higher variables to reflect the disappearance of
   a lambda.

 * replace the ocurrences of the variables by the new term, taking
   into account that free variables must be increased to avoid them
   getting captured by the outermost lambda terms. 

In our code, we apply =subs= to any expression. When it is applied to
a \lambda-abstraction, the index and the free variables of the
replaced term are increased with =incrementFreeVars=; whenever it is
applied to a variable, the previous cases are taken into consideration.

#+BEGIN_SRC haskell
-- | Substitutes an index for a lambda expression
subs :: Integer -> Exp -> Exp -> Exp
subs n p (Lambda e) = Lambda (subs (n+1) (incrementFreeVars 0 p) e)
subs n p (App f g)  = App (subs n p f) (subs n p g)
subs n p (Var m)
  | n == m    = p         -- The lambda is replaced directly  
  | n <  m    = Var (m-1) -- A more exterior lambda decreases a number
  | otherwise = Var m     -- An unrelated variable remains untouched
#+END_SRC

Then \beta-reduction can be defined using this =subs= function.

#+BEGIN_SRC haskell
betared :: Exp -> Exp
betared (App (Lambda e) x) = substitute 1 x e
betared e = e
#+END_SRC

*** De Bruijn-terms and \lambda-terms
The internal language of the interpreter uses de Bruijn expressions,
while the user interacts with it using lambda expressions with alphanumeric
variables. Our definition of a \lambda-expression with variables will be
used in parsing and output formatting.

#+BEGIN_SRC haskell
data NamedLambda = LambdaVariable String                    
                 | LambdaAbstraction String NamedLambda     
                 | LambdaApplication NamedLambda NamedLambda
#+END_SRC

**** Lambda to deBruijn                                           :ignore:
The translation from a natural \lambda-expression to de Bruijn notation
is done using a dictionary which keeps track of the bounded variables

#+BEGIN_SRC haskell
tobruijn :: Map.Map String Integer -- ^ names of the variables used
         -> Context                -- ^ names already binded on the scope
         -> NamedLambda            -- ^ initial expression
         -> Exp
-- Every lambda abstraction is inserted in the variable dictionary,
-- and every number in the dictionary increases to reflect we are entering
-- a deeper context.
tobruijn d context (LambdaAbstraction c e) = 
     Lambda $ tobruijn newdict context e
        where newdict = Map.insert c 1 (Map.map succ d)

-- Translation distributes over applications.
tobruijn d context (LambdaApplication f g) = 
     App (tobruijn d context f) (tobruijn d context g)

-- We look for every variable on the local dictionary and the current scope.
tobruijn d context (LambdaVariable c) =
  case Map.lookup c d of
    Just n  -> Var n
    Nothing -> fromMaybe (Var 0) (MultiBimap.lookupR c context)
#+END_SRC

**** deBruijn to Lambda                                           :ignore:
while the translation from a de Bruijn expression to a natural one is done
considering an infinite list of possible variable names and keeping a list
of currently-on-scope variables to name the indices.

#+BEGIN_SRC haskell
-- | An infinite list of all possible variable names 
-- in lexicographical order.
variableNames :: [String]
variableNames = concatMap (`replicateM` ['a'..'z']) [1..]

-- | A function translating a deBruijn expression into a 
-- natural lambda expression.
nameIndexes :: [String] -> [String] -> Exp -> NamedLambda
nameIndexes _    _   (Var 0) = LambdaVariable "undefined"
nameIndexes used _   (Var n) = 
  LambdaVariable (used !! pred (fromInteger n))
nameIndexes used new (Lambda e) = 
  LambdaAbstraction (head new) (nameIndexes (head new:used) (tail new) e)
nameIndexes used new (App f g) = 
  LambdaApplication (nameIndexes used new f) (nameIndexes used new g)
#+END_SRC

*** Evaluation
As we proved on Corollary [[cor-leftmosttheorem]], the leftmost reduction
strategy will find the leftmost reduction strategy if it
exists. Consequently, we will implement it using a function that
simply applies the leftmost possible reductions at each step. This
will allow us to show how the interpreter performs step-by-step
evaluations to the final user, as discussed in the [[*Verbose mode][verbose mode]] section.

#+BEGIN_SRC haskell
-- | Simplifies the expression recursively.
-- Applies only one parallel beta reduction at each step.
simplify :: Exp -> Exp
simplify (Lambda e)           = Lambda (simplify e)
simplify (App (Lambda f) x)   = betared (App (Lambda f) x)
simplify (App (Var e) x)      = App (Var e) (simplify x)
simplify (App a b)            = App (simplify a) (simplify b)
simplify (Var e)              = Var e

-- | Applies repeated simplification to the expression until it stabilizes and
-- returns all the intermediate results.
simplifySteps :: Exp -> [Exp]
simplifySteps e
  | e == s    = [e]
  | otherwise = e : simplifySteps s
  where s = simplify e
#+END_SRC

From the code we can see that the evaluation finishes whenever the
expression stabilizes. This can happen in two different cases

  * there are no more possible \beta-reductions, and the algorithm
    stops.
  * \beta-reductions do not change the expression. The computation
    would lead to an infinite loop, so it is immediately stopped.
    An common example of this is the \lambda-term $(\lambda x.x x)(\lambda x.x x)$.

*** Principal type inference
The interpreter implements the [[*Unification and type inference][unification and type inference]] algorithms
described in Lemma [[lemma-unification]] and Theorem [[thm-typeinfer]]. Their
recursive nature makes them very easy to implement directly on Haskell.

**** Type templates and substitutions                             :ignore:
We implement a simply-typed lambda calculus with [[*Curry-style types][Curry-style typing]]
and type templates. Our type system has

  * an unit type;
  * a bottom type;
  * product types;
  * union types;
  * and function types.

#+BEGIN_SRC haskell
-- | A type template is a free type variable or an arrow between two
-- types; that is, the function type.
data Type = Tvar Variable
          | Arrow Type Type
          | Times Type Type
          | Union Type Type
          | Unitty
          | Bottom
          deriving (Eq)
#+END_SRC

We will work with substitutions on type templates. They can be directly
defined as functions from types to types. A basic substitution that
inserts a given type on the place of a variable will be our building
block for more complex ones.

#+BEGIN_SRC haskell
type Substitution = Type -> Type

-- | A basic substution. It changes a variable for a type
subs :: Variable -> Type -> Substitution
subs x typ (Tvar y)
  | x == y    = typ
  | otherwise = Tvar y
subs x typ (Arrow a b) = Arrow (subs x typ a) (subs x typ b)
subs x typ (Times a b) = Times (subs x typ a) (subs x typ b)
subs x typ (Union a b) = Union (subs x typ a) (subs x typ b)
subs _ _ Unitty = Unitty
subs _ _ Bottom = Bottom
#+END_SRC

**** Unification                                                  :ignore:
Unification will be implemented making extensive use of the =Maybe=
monad. If the unification fails, it will return an error value, and
the error will be propagated to the whole computation. The algorithm
is exactly the same that was defined in Lemma [[lemma-unification]].

#+BEGIN_SRC haskell
-- | Unifies two types with their most general unifier. Returns the substitution
-- that transforms any of the types into the unifier.
unify :: Type -> Type -> Maybe Substitution
unify (Tvar x) (Tvar y)
  | x == y    = Just id
  | otherwise = Just (subs x (Tvar y))
unify (Tvar x) b
  | occurs x b = Nothing
  | otherwise  = Just (subs x b)
unify a (Tvar y)
  | occurs y a = Nothing
  | otherwise  = Just (subs y a)
unify (Arrow a b) (Arrow c d) = unifypair (a,b) (c,d)
unify (Times a b) (Times c d) = unifypair (a,b) (c,d)
unify (Union a b) (Union c d) = unifypair (a,b) (c,d)
unify Unitty Unitty = Just id
unify Bottom Bottom = Just id
unify _ _ = Nothing

-- | Unifies a pair of types
unifypair :: (Type,Type) -> (Type,Type) -> Maybe Substitution
unifypair (a,b) (c,d) = do
  p <- unify b d
  q <- unify (p a) (p c)
  return (q . p)
#+END_SRC

**** Type inference                                               :ignore:
The type inference algorithm is more involved. It takes a list
of fresh variables, a type context, a lambda expression and a
constraint on the type, expressed as a type template. It outputs
a substitution. As an example, the following code shows the type
inference algorithm for function types.

#+BEGIN_SRC haskell
-- | Type inference algorithm. Infers a type from a given context and expression
-- with a set of constraints represented by a unifier type. The result type must
-- be unifiable with this given type.
typeinfer :: [Variable] -- ^ List of fresh variables
          -> Context    -- ^ Type context
          -> Exp        -- ^ Lambda expression whose type has to be inferred
          -> Type       -- ^ Constraint
          -> Maybe Substitution

typeinfer (x:vars) ctx (App p q) b = do -- Writing inside the Maybe monad.
  sigma <- typeinfer (evens vars) ctx                  p (Arrow (Tvar x) b)
  tau   <- typeinfer (odds  vars) (applyctx sigma ctx) q (sigma (Tvar x))
  return (tau . sigma)
  where
    -- The list of fresh variables has to be split into two
    odds [] = []
    odds [_] = []
    odds (_:e:xs) = e : odds xs
    evens [] = []
    evens [e] = [e]
    evens (e:_:xs) = e : evens xs
#+END_SRC

The final form of the type inference algorithm will use a
normalization algorithm shortening the type names and will apply the
type inference to the empty type context. The complete code can be
found on the [[*Mikrokosmos complete code][Appendix]].

**** Gentzen deduction trees                                      :ignore:
A generalized version of the type inference algorithm is used
to generate derivation trees from terms, as it was described in
[[*Propositions as types][Propositions as types]].

In order to draw these diagrams in Unicode characters, a data type for
character blocks has been defined. A monoidal structure is defined
over them; blocks can be joined vertically and horizontally; and
every deduction step can be drawn independently.

#+BEGIN_SRC haskell
newtype Block = Block { getBlock :: [String] }
  deriving (Eq, Ord)

instance Monoid Block where
  mappend = joinBlocks -- monoid operation, joins blocks vertically
  mempty  = Block [[]] -- neutral element

-- Type signatures
joinBlocks :: Block -> Block -> Block
stackBlocks :: String -> Block -> Block -> Block
textBlock :: String -> Block
deductionBlock :: Block -> String -> [Block] -> Block
box :: Block -> Block
#+END_SRC

** User interaction
*** Monadic parser combinators
A common approach to building parsers in functional programming is to
model parsers as functions. Higher-order functions on parsers act as
/combinators/, which are used to implement complex parsers in a
modular way from a set of primitive ones. In this setting, parsers
exhibit a monad algebraic structure, which can be used to simplify
the combination of parsers. A technical report on *monadic parser combinators*
can be found on cite:hutton96.

The use of monads for parsing is discussed firstly in cite:Wadler85,
and later in cite:Wadler90 and cite:hutton98. The parser type is
defined as a function taking a =String= and returning a list of pairs,
representing a successful parse each. The first component of the pair
is the parsed value and the second component is the remaining
input. The Haskell code for this definition is

#+BEGIN_SRC haskell
newtype Parser a = Parser (String -> [(a,String)])

parse :: Parser a -> String -> [(a,String)]
parse (Parser p) = p

instance Monad Parser where
  return x = Parser (\s -> [(x,s)])
  p >>= q  = Parser (\s -> 
               concat [parse (q x) s' | (x,s') <- parse p s ])
#+END_SRC

where the monadic structure is defined by =bind= and =return=. Given a
value, the =return= function creates a parser that consumes no input
and simply returns the given value. The =>>== function acts as a sequencing
operator for parsers. It takes two parsers and applies the second one
over the remaining inputs of the first one, using the parsed values on
the first parsing as arguments.

An example of primitive *parser* is the =item= parser, which consumes a
character from a non-empty string. It is written in Haskell code as

#+BEGIN_SRC haskell
item :: Parser Char
item = Parser (\s -> case s of 
                       "" -> []
                       (c:s') -> [(c,s')])
#+END_SRC

and an example of *parser combinator* is the =many= function, which
creates a parser that allows one or more applications of the given
parser

#+BEGIN_SRC haskell
many :: Paser a -> Parser [a]
many p = do
  a  <- p
  as <- many p
  return (a:as)
#+END_SRC

in this example =many item= would be a parser consuming all characters
from the input string.

*** Parsec
*Parsec* is a monadic parser combinator Haskell library described in
cite:leijen2001. We have chosen to use it due to its simplicity and
extensive documentation. As we expect to use it to parse user live
input, which will tend to be short, performance is not a critical
concern. A high-performace library supporting incremental parsing,
such as *Attoparsec* cite:attoparsec, would be suitable otherwise.
*** Verbose mode
As we explained previously on the Evaluation section, the simplification
can be analyzed step-by-step. The interpreter allows us to see the
complete evaluation when the =verbose= mode is activated. To activate
it, we can execute =:verbose on= in the interpreter.

The difference can be seen on the following example.

#+BEGIN_EXAMPLE
mikro> plus 1 2
λa.λb.(a (a (a b))) ⇒ 3

mikro> :verbose on
verbose: on
mikro> plus 1 2
((plus 1) 2)
((λλλλ((4 2) ((3 2) 1)) λλ(2 1)) λλ(2 (2 1)))
(λλλ((λλ(2 1) 2) ((3 2) 1)) λλ(2 (2 1)))
λλ((λλ(2 1) 2) ((λλ(2 (2 1)) 2) 1))
λλ(λ(3 1) (λ(3 (3 1)) 1))
λλ(2 (λ(3 (3 1)) 1))
λλ(2 (2 (2 1)))

λa.λb.(a (a (a b))) ⇒ 3
#+END_EXAMPLE

The interpreter output can be colored to show specifically where it
is performing reductions. It is activated by default, but can be deactivated
by executing =:color off=. The following code implements /verbose mode/ in both
cases.

#+BEGIN_SRC haskell
-- | Shows an expression, coloring the next reduction if necessary
showReduction :: Exp -> String
showReduction (Lambda e)         = "λ" ++ showReduction e
showReduction (App (Lambda f) x) = betaColor (App (Lambda f) x)
showReduction (Var e)            = show e
showReduction (App rs x)         = 
  "(" ++ showReduction rs ++ " " ++ showReduction x ++ ")"
showReduction e                  = show e
#+END_SRC

*** SKI mode
Every \lambda-term can be written in terms of SKI combinators.
SKI combinator expressions can be defined as a binary tree having
S, K, and I as possible leafs.

#+BEGIN_SRC haskell
data Ski = S | K | I | Comb Ski Ski
#+END_SRC

The SKI-abstraction and bracket abstraction algorithms are implemented
on Mikrokosmos, and they can be used by activating the /ski mode/ with
=:ski on=. When this mode is activated, every result is written in terms
of SKI combinators.

#+begin_example
mikro> 2
λa.λb.(a (a b)) ⇒ S(S(KS)K)I ⇒ 2
mikro> and
λa.λb.((a b) a) ⇒ SSK ⇒ and
#+end_example

The code implementing these algorithms follows directly from the
theoretical version in cite:Hindley08.

#+BEGIN_SRC haskell
-- | Bracket abstraction of a SKI term, as defined in Hindley-Seldin
-- (2.18).
bracketabs :: String -> Ski -> Ski
bracketabs x (Cte y) = if x == y then I else Comb K (Cte y)
bracketabs x (Comb u (Cte y))
  | freein x u && x == y = u
  | freein x u           = Comb K (Comb u (Cte y))
  | otherwise            = Comb (Comb S (bracketabs x u)) (bracketabs x (Cte y))
bracketabs x (Comb u v)
  | freein x (Comb u v)  = Comb K (Comb u v)
  | otherwise            = Comb (Comb S (bracketabs x u)) (bracketabs x v)
bracketabs _ a           = Comb K a


-- | SKI abstraction of a named lambda term. From a lambda expression
-- creates a SKI equivalent expression. The following algorithm is a
-- version of the algorithm (9.10) on the Hindley-Seldin book.
skiabs :: NamedLambda -> Ski
skiabs (LambdaVariable x)      = Cte x
skiabs (LambdaApplication m n) = Comb (skiabs m) (skiabs n)
skiabs (LambdaAbstraction x m) = bracketabs x (skiabs m)
#+END_SRC
 
** Usage
*** Installation
The complete Mikrokosmos suite is divided in multiple parts:

 1) the *Mikrokosmos interpreter*, written in Haskell;
 2) the *Jupyter kernel*, written in Python;
 3) the *CodeMirror Lexer*, written in Javascript;
 4) the *Mikrokosmos libraries*, written in the Mikrokosmos language;
 5) the *Mikrokosmos-js* compilation, which can be used in web browsers.

These parts will be detailed on the following sections. A system that
already satisfies all dependencies (Stack, Pip and Jupyter), can install
Mikrokosmos using the following script, which is detailed on this section

#+BEGIN_SRC sh
# Mikrokosmos interpreter
stack install mikrokosmos
# Jupyter kernel for Mikrokosmos
sudo pip install imikrokosmos
# Libraries
git clone https://github.com/M42/mikrokosmos-lib.git ~/.mikrokosmos
#+END_SRC

**** Mikrokosmos interpreter :ignore:
The *Mikrokosmos interpreter* is listed in the central Haskell
package archive, /Hackage/ [fn:hackage]. The packaging of Mikrokosmos
has been done using the *cabal* tool; and the configuration of the
package can be read on the file =mikrokosmos.cabal= on the Mikrokosmos
code. As a result, Mikrokosmos can be installed using the *cabal* and
*stack* Haskell package managers. That is,

#+BEGIN_SRC sh
# With cabal
cabal install mikrokosmos
# With stack
stack install mikrokosmos
#+END_SRC

**** Mikrokosmos Jupyter kernel :ignore:
The *Mikrokosmos Jupyter kernel* is listed in the central Python
package archive. Jupyter is a dependency of this kernel, which only
can be used in conjunction with it. It can be installed with the
=pip= package manager as

#+BEGIN_SRC sh
sudo pip install imikrokosmos
#+END_SRC

and the installation can be checked by listing the available Jupyter
kernels with

#+BEGIN_SRC sh
jupyter kernelspec list
#+END_SRC

**** Mikrokosmos libraries :ignore:
The *Mikrokosmos libraries* can be downloaded directly from its GitHub
repository. [fn:mikrokosmoslibgit] They have to be placed under
=~/.mikrokosmos= if we want them to be locally available or under
=/usr/lib/mikrokosmos= if we want them to be globally available.

#+BEGIN_SRC sh
git clone https://github.com/M42/mikrokosmos-lib.git ~/.mikrokosmos
#+END_SRC

**** Complete script :ignore:
The following script installs the complete Mikrokosmos suite on a
fresh system. It has been tested under =Ubuntu 16.04.3 LTS (Xenial
Xerus)=.

#+BEGIN_SRC sh
# 1. Installs Stack, the Haskell package manager
wget -qO- https://get.haskellstack.org | sh
STACK=$(which stack)

# 2. Installs the ncurses library, used by the console interface
sudo apt install libncurses5-dev libncursesw5-dev

# 3. Installs the Mikrokosmos interpreter using Stack
$STACK setup
$STACK install mikrokosmos

# 4. Installs the Mikrokosmos standard libraries
sudo apt install git
git clone https://github.com/M42/mikrokosmos-lib.git ~/.mikrokosmos

# 5. Installs the IMikrokosmos kernel for Jupyter
sudo apt install python3-pip
sudo -H pip install --upgrade pip
sudo -H pip install jupyter
sudo -H pip install imikrokosmos
#+END_SRC

[fn:hackage]: Hackage can be accesed in: http://hackage.haskell.org/
and the Mikrokosmos package can be found in https://hackage.haskell.org/package/mikrokosmos
[fn:mikrokosmoslibgit]: The repository can be accessed in: https://github.com/M42/mikrokosmos-lib.git

*** Mikrokosmos interpreter
Once installed, the Mikrokosmos \lambda interpreter can be opened from
the terminal with the =mikrokosmos= command. It will enter a /read-eval-print loop/
where \lambda-expressions and interpreter commands can be evaluated.

#+BEGIN_EXAMPLE
$> mikrokosmos
Welcome to the Mikrokosmos Lambda Interpreter!
Version 0.5.0. GNU General Public License Version 3.
mikro> _
#+END_EXAMPLE

The interpreter evaluates every line as a lambda expression. Examples
on the use of the interpreter can be read on the following
sections. Apart from the evaluation of expressions, the interpreter
accepts the following commands

  * =:quit= and =:restart=, stop the interpreter;
  * =:verbose= activates /verbose mode/;
  * =:ski= activates /SKI mode/;
  * =:types= changes between untyped and simply typed \lambda-calculus;
  * =:color= deactivates colored output;
  * =:load= loads a library.

The Figure [[mikrosession]] is an example session on the mikrokosmos interpreter.

#+caption: Mikrokosmos interpreter session.
#+name: mikrosession
[[./images/mikrosession.png]]

*** Jupyter kernel
The *Jupyter Project* cite:jupyter is an open source project providing
support for interactive scientific computing. Specifically, the
Jupyter Notebook provides a web application for creating interactive
documents with live code and visualizations. 

We have developed a Mikrokosmos kernel for the Jupyter Notebook,
allowing the user to write and execute arbitrary Mikrokosmos code
on this web application. An example session can be seen on Figure
[[jupytersession]].

#+caption: Jupyter notebook Mikrokosmos session.
#+name: jupytersession
[[./images/jupytersession.png]]

The implementation is based on the =pexpect= library for Python.  It
allows direct interaction with any REPL and collects its results.
Specifically, the following Python lines represent the central idea of
this implementation

#+BEGIN_SRC python
# Initialization
mikro = pexpect.spawn('mikrokosmos')
mikro.expect('mikro>')

# Interpreter interaction
# Multiple-line support
output = ""
for line in code.split('\n'):
    # Send code to mikrokosmos
    self.mikro.sendline(line)
    self.mikro.expect('mikro> ')

    # Receive and filter output from mikrokosmos
    partialoutput = self.mikro.before
    partialoutput = partialoutput.decode('utf8')
    output = output + partialoutput
#+END_SRC

A =pip= installable package has been created following the
Python Packaging Authority guidelines. [fn:pypaguide] This allows
the kernel to be installed directly using the =pip= python package manager.

#+BEGIN_SRC bash
sudo -H pip install imikrokosmos
#+END_SRC

[fn:pypaguide]: The PyPA packaging user guide can be found in its official
page: https://packaging.python.org/

*** CodeMirror lexer
*CodeMirror* [fn:codemirror] is a text editor for the browser
implemented in Javascript. It is used internally by the Jupyter
Notebook.

A CodeMirror lexer for Mikrokosmos has been written. It uses
Javascript regular expressions and signals the ocurrence of any kind
of operator to CodeMirror. It enables syntax highlighting for Mikrokosmos
code on Jupyter Notebooks. It comes bundled with the kernel specification
and no additional installation is required.

#+BEGIN_SRC javascript
	CodeMirror.defineSimpleMode("mikrokosmos", {
	    start: [
	    // Comments
	    {regex: /\#.*/,
            token: "comment"},
	    // Interpreter
            {regex: /\:load|\:verbose|\:ski|\:restart|\:types|\:color/,
            token: "atom"},
	    // Binding
	    {regex: /(.*?)(\s*)(=)(\s*)(.*?)$/,
	    token: ["def",null,"operator",null,"variable"]},
	    // Operators
	    {regex: /[=!]+/,
            token: "operator"},
	    ],
	    meta: {
		dontIndentStates: ["comment"],
		lineComment: "#"
	    }
	}
#+END_SRC

[fn:codemirror]: Documentation for CodeMirror can be found in its
official page: https://codemirror.net/

*** JupyterHub
*JupyterHub* manages multiple instances of independent single-user Jupyter
notebooks. We used it to serve Mikrokosmos notebooks and tutorials to
students studying \lambda-calculus.

In order to install Mikrokosmos on a server and use it as =root= user,
we need

  * to clone the libraries into =/usr/lib/mikrokosmos=. They should be
    available system-wide.

  * to install the Mikrokosmos interpreter into =/usr/local/bin=. In
    this case, we chose not to install Mikrokosmos from source, but simply
    copy the binaries and check the availability of the =ncurses= library.

  * to install the Mikrokosmos Jupyter kernel as usual.

Our server used a SSL certificate; and OAuth autentication via GitHub.
Mikrokosmos tutorials were installed for every student.

*** Calling Mikrokosmos from Javascript
The GHCjs[fn:ghcjs] compiler allows transpiling from Haskell to Javascript.
Its foreign function interface allows a Haskell function to be passed as
a continuation to a Javascript function.

A particular version of the =Main.hs= module of Mikrokosmos was written in
order to provide a =mikrokosmos= function, callable from Javascript. This
version includes the standard libraries automatically and reads blocks of
texts as independent Mikrokosmos commands. The relevant use of the foreign
function interface is showed in the following code

#+BEGIN_SRC haskell
foreign import javascript unsafe "mikrokosmos = $1"
    set_mikrokosmos :: Callback a -> IO ()
#+END_SRC

which provides =mikrokosmos= as a Javascript function once the code is
transpiled. In particular, the following is an example of how to call
Mikrokosmos from Javascript

#+BEGIN_SRC javascript
button.onclick = function () {
   editor.save();
   outputcode.getDoc().setValue(mikrokosmos(inputarea.value).mkroutput);
   textAreaAdjust(outputarea);
}
#+END_SRC

A small script has been written in Javascript to help with the task of
embedding Mikrokosmos into a web page. It and can be included directly
from

=https://m42.github.io/mikrokosmos-js/mikrobox.js=

using GitHub as a CDN. It will convert any HTML script tag written as
follows

#+BEGIN_SRC html
<div class="mikrojs-console">
<script type="text/mikrokosmos">
(λx.x)
... your code
</script>
</div>
#+END_SRC

into a CodeMirror pad where Mikrokosmos can be executed. The Mikrokosmos
tutorials are an example of this feature and can be seen on Figure [[mikrokosmosjstutorial]].

#+caption: Mikrokosmos embedded into a web page.
#+name: mikrokosmosjstutorial
[[./images/mikrokosmosjs.png]]

[fn:ghcjs]: The GHCjs documentation is available on its web page https://github.com/ghcjs/ghcjs

** Programming in the untyped \lambda-calculus
# Untyped \lambda-calculus in programming languages

This section explains how to use the untyped \lambda-calculus to
encode data structures and useful data, such as booleans, linked lists,
natural numbers or binary trees. All this is done on pure \lambda-calculus
avoiding the addition of new syntax or axioms.

This presentation follows the Mikrokosmos tutorial on \lambda-calculus, which
aims to teach how it is possible to program using untyped \lambda-calculus
without discussing technical topics such as those we have discussed on
the chapter on [[*Untyped \lambda-calculus][untyped \lambda-calculus]]. It also follows
the exposition on cite:selinger13 of the usual Church encodings.

All the code on this section is valid Mikrokosmos code.

*** Basic syntax
In the interpreter, \lambda-abstractions are written with the symbol =\=,
representing a \lambda. This is a convention used on some functional languages
such as Haskell or Agda. Any alphanumeric string can be a variable and
can be defined to represent a particular \lambda-term using the === operator.

As a first example, we define the identity function (=id=), function 
composition (=compose=) and a constant function on two arguments which
always returns the first one untouched (=const=).

#+BEGIN_SRC haskell
id = \x.x
compose = \f.\g.\x.f (g x)
const = \x.\y.x
#+END_SRC

Evaluation of terms will be denoted with presented as comments to the
code,

#+BEGIN_SRC haskell
compose id id
-- [1]: λa.a ⇒ id
#+END_SRC

It is important to notice that multiple argument functions are defined as
higher one-argument functions which return another functions as arguments.
These intermediate functions are also valid \lambda-terms. For example

#+BEGIN_SRC haskell
discard = const id
#+END_SRC

is a function that discards one argument and returns the identity, =id=.
This way of defining multiple argument functions is called the *currying*
of a function in honor to the american logician Haskell Curry in cite:haskell58.
It is a particular instance of a deeper fact: exponentials are defined
by the following adjunction
\[
\hom(A \times B, C) \cong \hom(A, \hom(B,C)).
\]

*** A technique on inductive data encoding
Over this presentation, we will implicitly use a technique on the
majority of our data encodings which allows us to write an encoding
for any algebraically inductive generated data. This technique is used
without comment on cite:selinger13 and represents the basis of what is
called the *Church encoding* of data in \lambda-calculus.

We start considering the usual inductive representation of
a data type with constructors, as we do when representing a
syntax with a BNF, for example,
\[
\mathtt{Nat} ::= \mathtt{Zero} \mid \mathtt{Succ}\ \mathtt{Nat}.
\]
Or, in general
\[
\mathtt{D} ::= C_1 \mid C_2 \mid C_3 \mid \dots
\]

It is not possible to directly encode constructors on
\lambda-calculus. Even if we were able, they would have, in theory, no
computational content; the data structure would not
be reduced under any \lambda-term, and we would need at least the 
ability to pattern match on the constructors to define functions
on them. Our \lambda-calculus would need to be extended with
additional syntax for every new data structure.

This technique, instead, defines a data term as a function on
multiple arguments representing the missing constructors. In our example, 
the number $2$, which would be written as $\mathtt{Succ}(\mathtt{Succ}(\mathtt{Zero}))$,
would be encoded as
\[
2 = \lambda s.\ \lambda z.\ s (s (z)).
\]

In general, any instance of the data structure $\mathtt{D}$ would be encoded as a
\lambda-expression depending on all its constuctors
\[
\lambda c_{1}.\ \lambda c_{2}.\ \lambda c_{3}.\ \dots\ \lambda c_{n}. (\textit{term}).
\]

This acts as the definition of an initial algebra over the
constructors and lets us compute by instantiating this algebra on
particular cases. Particular examples are described on the following
sections.
# Link to categories

*** Booleans
Booleans can be defined as the data generated by a pair of constuctors
\[\mathtt{Bool} ::= \mathtt{True} \mid \mathtt{False}.
\]

Consequently, the Church encoding of booleans takes these constructors as
arguments and defines

#+BEGIN_SRC haskell
true  = \t.\f.t
false = \t.\f.f
#+END_SRC

**** If-else interpretation                                       :ignore:
Note that =true= and =const= are exactly the same term up to
\alpha-conversion. The same thing happens with =false= and =alwaysid=.
The absence of types prevents us to make any effort to discriminate
between these two uses of the same \lambda-term. Another side-effect
of this definition is that our =true= and =false= terms can be interpreted
as binary functions choosing between two arguments, i.e.,

  * $\mathtt{true}(a,b) = a$
  * $\mathtt{false}(a,b) = b$

We can test this interpretation on the interpreter to get

#+BEGIN_SRC haskell
true id const
false id const
--- [1]: id
--- [2]: const
#+END_SRC

This inspires the definition of an =ifelse= combinator as the identity

#+BEGIN_SRC haskell
ifelse = \b.b
(ifelse true) id const
(ifelse false) id const
--- [1]: id
--- [2]: const
#+END_SRC

**** Logic gates                                                  :ignore:
The usual logic gates can be defined profiting from this interpretation
of booleans

#+BEGIN_SRC haskell
and = \p.\q.p q p
or = \p.\q.p p q
not = \b.b false true
xor = \a.\b.a (not b) b
implies = \p.\q.or (not p) q

xor true true
and true true
--- [1]: false
--- [2]: true
#+END_SRC

*** Natural numbers
**** Peano natural numbers                                        :ignore:
Our definition of natural numbers is inspired by the Peano natural numbers.
We use two constructors

 * zero is a natural number, written as Z;
 * the successor of a natural number is a natural number, written as S;

and the BNF we defined when discussing how to [[*A technique on inductive data encoding][encode inductive data]].

#+BEGIN_SRC haskell
0    = \s.\z.z
succ = \n.\s.\z.s (n s z)
#+END_SRC

This definition of =0= is trivial: given a successor function and a
zero, return zero. The successor function seems more complex, but
it uses the same underlying idea: given a number, a successor and a
zero, apply the successor to the interpretation of that number using
the same successor and zero.

We can then name some natural numbers as

#+BEGIN_SRC haskell
1 = succ 0
2 = succ 1
3 = succ 2
4 = succ 3
5 = succ 4
6 = succ 5
...
#+END_SRC

even if we can not define an infinite number of terms as we might wish.

**** Interpretation as higher-order functions                     :ignore:
The interpretation the natural number $n$ as a higher order function
is a function taking an argument =f= and applying them $n$ times over
the second argument.

#+BEGIN_SRC haskell
5 not true
4 not true
double = \n.\s.\z.n (compose s s) z
double 3
--- [1]: false
--- [2]: true
--- [3]: 6
#+END_SRC

**** Addition and multiplication                                  :ignore:
Addition $n+m$ applies the successor $m$ times to $n$; and multiplication
$nm$ applies the $n\text{-fold}$ application of the successor $m$ times to $0$.

#+BEGIN_SRC haskell
plus = \m.\n.\s.\z.m s (n s z)
mult = \m.\n.\s.\z.m (n s) z
plus 2 1
mult 2 4
--- [1]: 3
--- [2]: 8
#+END_SRC

*** The predecessor function and predicates on numbers
**** Predecessor                                                  :ignore:
The predecessor function is much more complex than the previous ones.
As we can see, it is not trivial how could we compute the predecessor
using the limited form of induction that Church numerals allow.

Stephen Kleene, one of the students of Alonzo Church only discovered
how to write the predecessor function after thinking about it for a
long time (and he only discovered it while a long visit at the
dentist's, which is the reason why this definition is often called the
*/wisdom tooth trick/*, see cite:crossley75). We will use a slightly
different version of the definition that does not depend on a pair
datatype.

We will start defining a /reverse composition/ operator, called
=rcomp=; and we will study what happens when it is composed to itself;
that is

#+BEGIN_SRC haskell
rcomp = \f.\g.\h.h (g f)
\f.3 (inc f)
\f.4 (inc f)
\f.5 (inc f)
--- [1]: λa.λb.λc.c (a (a (b a)))
--- [2]: λa.λb.λc.c (a (a (a (b a))))
--- [3]: λa.λb.λc.c (a (a (a (a (b a)))))
#+END_SRC

will allow us now to use the =b= argument to discard the first instance
of the =a= argument and return the same number wihtout the last constructor.
Thus, our definition of =pred= is

#+BEGIN_SRC haskell
pred = \n.\s.\z.(n (inc s) (\x.z) (\x.x))
#+END_SRC

**** Predicates                                                   :ignore:
From the definition of =pred=, some predicates on numbers can be
defined. The first predicate will be a function distinguishing a
successor from a zero. It will be user later to build more complex
ones. It is built by appliying a =const false= function =n= times to a
true constant. Only if it is applied =0= times, it will return a true
value.

#+BEGIN_SRC haskell
iszero = \n.(n (const false) true)
iszero 0
iszero 2
--- [1]: true
--- [2]: false
#+END_SRC

From this predicate, we can derive predicates on equality and ordering.

#+BEGIN_SRC haskell
leq = \m.\n.(iszero (minus m n))
eq  = \m.\n.(and (leq m n) (leq n m))
#+END_SRC

*** Lists and trees
We would need two constructors to represent a list: a =nil= signaling
the end of the list and a =cons=, joining an element to the head of
the list. An example of list would be
\[\mathtt{cons}\ 1\ (\mathtt{cons}\ 2\ (\mathtt{cons}\ 3\ \mathtt{nil})).\]

Our definition takes those two constructors into account
#+BEGIN_SRC haskell
nil  = \c.\n.n
cons = \h.\t.\c.\n.(c h (t c n))
#+END_SRC
and the interpretation of a list as a higher-order function is its
=fold= function, a function taking a binary operation and an initial
element and appliying the operation repeteadly to every element on
the list.

\[\mathtt{cons}\ 1\ (\mathtt{cons}\ 2\ (\mathtt{cons}\ 3\ \mathtt{nil}))
\overset{fold\ plus\ 0}\longrightarrow 
\mathtt{plus}\ 1\ (\mathtt{plus}\ 2\ (\mathtt{plus}\ 3\ 0)) = 6\]

The =fold= operation and some operations on lists can be defined
explicitly as

#+BEGIN_SRC haskell
fold = \c.\n.\l.(l c n)
sum  = fold plus 0
prod = fold mult 1
all  = fold and true
any  = fold or false
length = foldr (\h.\t.succ t) 0

sum (cons 1 (cons 2 (cons 3 nil)))
all (cons true (cons true (cons true nil)))
--- [1]: 6
--- [2]: true
#+END_SRC

**** Map and filter                                               :ignore:
The two most commonly used particular cases of fold and frequent examples
of the functional programming paradigm are =map= and =filter=.

  - The *map* function applies a function =f= to every element on a
    list.
  - The *filter* function removes the elements of the list that do not
    satisfy a given predicate. It /filters/ the list, leaving only
    elements that satisfy the predicate.

They can be defined as follows.

#+BEGIN_SRC haskell
map    = \f.(fold (\h.\t.cons (f h) t) nil)
filter = \p.(foldr (\h.\t.((p h) (cons h t) t)) nil)
#+END_SRC

On =map=, given a =cons h t=, we return a =cons (f h) t=; and given a
=nil=, we return a =nil=. On =filter=, we use a boolean to decide at
each step whether to return a list with a head or return the tail
ignoring the head.

#+BEGIN_SRC haskell
mylist = cons 1 (cons 2 (cons 3 nil))
sum (map succ mylist)
length (filter (leq 2) mylist)
--- [1]: 9
--- [2]: 2
#+END_SRC
**** Binary trees                                                 :ignore:
Lists have been defined using two constructors and *binary trees* will
be defined using the same technique. The only difference with lists is
that the =cons= constructor is replaced by a =node= constructor, which
takes two binary trees as arguments. That is, a binary tree is

 * an empty tree; or
 * a node, containing a label, a left subtree, and a right subtree.

Defining functions using a fold-like combinator is again very simple
due to the chosen representation. We need a variant
of the usual function acting on three arguments, the label, the right
node and the left node.

#+BEGIN_SRC haskell
-- Binary tree definition
node = \x.\l.\r.\f.\n.(f x (l f n) (r f n))
-- Example on natural numbers
mytree    = node 4 (node 2 nil nil) (node 3 nil nil)
triplesum = \a.\b.\c.plus (plus a b) c
mytree triplesum 0
--- [1]: 9
#+END_SRC

**** TODO The universal properties of fold, map and filter
*** Fixed points
A fixpoint combinator is a term representing a higher-order function
that, given any function =f=, solves the equation
\[
\mathtt{x = f\ x}
\]
for =x=, meaning that, if =fix f= is the fixpoint of =f=, the following
sequence of equations holds
\[
\mathtt{fix}\ f =
f (\mathtt{fix}\ f) =
f ( f (\mathtt{fix}\ f)) =
f ( f ( f (\mathtt{fix}\ f))) =
\dots
\]

Such a combinator actually exists; it can be defined and used as
#+BEGIN_SRC haskell
fix != (\f.(\x.f (x x)) (\x.f (x x)))
fix (const id)
--- [1]: id
#+END_SRC

Examples of its applications are a /factorial/ function or a
/fibonacci/ function, as in
#+BEGIN_SRC haskell
fact != fix (\f.\n.iszero n 1 (mult n (f (pred n))))
fib  != fix (\f.\n.iszero n 1 (plus (f (pred n)) (f (pred (pred n)))))
fact 3
fib 3
--- [1]: 6
--- [2]: 5
#+END_SRC
Note the use of =iszero= to stop the recursion.

The =fix= function cannot be evaluated without arguments into a closed
form, so we have to delay the evaluation of the expression when we
bind it using =!==. Our evaluation strategy, however, will always find
a way to reduce the term if it is possible, as we saw in Corollary
[[cor-leftmosttheorem]]; even if it has intermediate irreducible terms.

#+BEGIN_SRC haskell
fix              -- diverges
true  id fix     -- evaluates to id
false id fix     -- diverges
#+END_SRC

Other examples of the interpreter dealing with non terminating functions
include infinite lists as in the following examples, where we take the
first term of an infinite list without having to evaluate it
completely or compare an infinite number arising as the fix point of
the successor function with a finite number.

#+BEGIN_SRC haskell
-- Head of an infinite list of zeroes
head = fold const false
head (fix (cons 0))
-- Compare infinity with other numbers
infinity != fix succ
leq infinity 6
---- [1]: 0
---- [2]: false
#+END_SRC

These definitions unfold as

 * $\mathtt{fix\ (cons\ 0) = cons\ 0\ (cons\ 0\ (cons\ 0\ \dots))}$, an infinite list of zeroes;
 * $\mathtt{fix\ succ\ \ \ \ \ \ = succ\ (succ\ (succ\ \dots))}$, an infinite natural number.

** Programming in the simply typed \lambda-calculus
This section explains how to use the simply typed \lambda-calculus to
encode compound data structures and proofs on intuitionistic logic.
We will use the interpreter as a typed language and, at the same time,
as a proof assistant for the intuitionistic propositional logic.

This presentation of simply typed structures follows the Mikrokosmos
tutorial and the previous sections on [[*Simply typed \lambda-calculus][simply typed \lambda-calculus]].
All the code on this section is valid Mikrokosmos code.

*** Function types and typeable terms
Types can be activated with the commmand =:types on=. If types are activated,
the interpreter will [[*Principal type inference][infer]] the principal type every term before its evaluation.
The type will then be displayed after the result of the computation.

#+attr_latex: :options [Typed terms on Mikrokosmos]
#+begin_exampleth
The following are examples of already defined terms on lambda calculus and
their corresponding types. It is important to notice how our previously
defined booleans have two different types; while our natural numbers will
have all the same type except from zero, whose type is a generalization on
the type of the natural numbers.

#+begin_src haskell
id    --- [1]: λa.a ⇒ id, I, ifelse :: A → A
true  --- [2]: λa.λb.a ⇒ K, true :: A → B → A
false --- [3]: λa.λb.b ⇒ nil, 0, false :: A → B → B
0     --- [4]: λa.λb.b ⇒ nil, 0, false :: A → B → B
1     --- [5]: λa.λb.(a b) ⇒ 1 :: (A → B) → A → B
2     --- [6]: λa.λb.(a (a b)) ⇒ 2 :: (A → A) → A → A
S     --- [7]: λa.λb.λc.((a c) (b c)) ⇒ S :: (A → B → C) → (A → B) → A → C
K     --- [8]: λa.λb.a ⇒ K, true :: A → B → A
#+end_src
#+end_exampleth

If a term is found to be non-typeable, Mikrokosmos will output an error
message signaling the fact. In this way, the evaluation of \lambda-terms
which could potentially not terminate is prevented. Only typed \lambda-terms
will be evaluated while the option =:types= is on; this ensures the termination
of every computation on typed terms.

#+attr_latex: :options [Non-typeable terms on Mikrokosmos]
#+begin_exampleth
Fixed point operators are a common example of non typeable terms. Its evaluation
on untyped \lambda-calculus would not terminate; and the type inference algorithm
fails on them.

#+BEGIN_SRC haskell
fix
--- Error: non typeable expression
fix (\f.\n.iszero n 1 (plus (f (pred n)) (f (pred (pred n))))) 3
--- Error: non typeable expression
#+END_SRC

Note that the evaluation of compound \lambda-expressions where the fixpoint
operators appear applied to other terms can terminate, but the terms are
still non typeable.
#+end_exampleth

*** Product, union, unit and void types
Until this point, we have only used the function type. We are working on the
implicational fragment of the STLC we described on the first [[*Typing rules for the simply typed \lambda-calculus][typing rules]].
We are now going to extend the type system in the same sense we [[*Extending the simply typed \lambda-calculus][extended]] the
STLC. The following types are added to the type system

| Type | Name          | Description                       |
|------+---------------+-----------------------------------|
| =→=  | Function type | Functions from a type to another. |
| =×=  | Product type  | Cartesian product of types.       |
| =+=  | Union type    | Disjoint union of types.          |
| =⊤=  | Unit type     | A type with exactly one element.  |
| =⊥=  | Void type     | A type with no elements.          |

And the following typed constructors are added to the language,

| Constructor | Type                              | Description               |
|-------------+-----------------------------------+---------------------------|
| =(-,-)=     | =A → B → A × B=                   | Pair of elements          |
| =fst=       | =(A × B) → A=                     | First projection          |
| =snd=       | =(A × B) → B=                     | Second projection         |
| =inl=       | =A → A + B=                       | First inclusion           |
| =inr=       | =B → A + B=                       | Second inclusion          |
| =caseof=    | =(A + B) → (A → C) → (B → C) → C= | Case analysis of an union |
| =unit=      | =⊤=                               | Unital element            |
| =abort=     | =⊥ → A=                           | Empty function            |
| =absurd=    | =⊥ → ⊥=                           | Particular empty function |

which correspond to the constructors we described on previous
sections. The only new addition is the =absurd= function, which is
only a particular case of =abort= useful when we want to make explicit
that we are deriving an instance of the empty type. This addition will
only make the logical interpretation on the following sections
clearer.

#+attr_latex: :options [Extended STLC on Mikrokosmos]
#+begin_exampleth
The following are examples of typed terms and functions on Mikrokosmos
using the extended typed constructors. The following terms are presented

  * a function swapping pairs, as an example of pair types.
  * two-case analysis of a number, deciding whether to multiply it by two
    or to compute its predecessor.
  * difference between =abort= and =absurd=.
  * example term containing the unit type.

#+BEGIN_SRC haskell
:load types

swap = \m.(snd m,fst m)
swap
---> λa.((SND a),(FST a)) ⇒ swap :: (A × B) → B × A

caseof (inl 1) pred (mult 2)
caseof (inr 1) pred (mult 2)
---> λa.λb.b ⇒ nil, 0, false :: A → B → B
---> λa.λb.(a (a b)) ⇒ 2 :: (A → A) → A → A

\x.((abort x),(absurd x))
---> λa.((ABORT a),(ABSURD a)) :: ⊥ → A × ⊥
#+END_SRC

Now it is possible to define a new encoding of the booleans with an
uniform type. The type =⊤ + ⊤= has two inhabitants, =inl ⊤= and =inr
⊤=; and they can be used by case analysis.

#+BEGIN_SRC haskell
btrue = inl unit
bfalse = inr unit
bnot = \a.caseof a (\a.bfalse) (\a.btrue)
bnot btrue
---> (INR UNIT) ⇒ bfalse :: A + ⊤
bnot bfalse
---> (INL UNIT) ⇒ btrue :: ⊤ + A
#+END_SRC
#+end_exampleth

With these extended types, Mikrokosmos can be used as a proof checker on
first-order intuitionistic logic by virtue of the Curry-Howard
correspondence.

*** A proof on intuitionistic logic
Under the logical interpretation of Mikrokosmos, we can transcribe proofs in
intuitionistic logic to \lambda-terms and check them on the interpreter.

#+begin_theorem
In intuitionistic logic, the double negation of the LEM holds for every
proposition, that is,
\[
\forall A\colon \neg \neg (A \lor \neg A)
\]
#+end_theorem
#+begin_proof
Suppose $\neg (A \lor \neg A)$. We are going to prove first that, under this
specific assumption, $\neg A$ holds. If $A$ were true, $A \lor \neg A$ would be true and we
would arrive to a contradition, so $\neg A$. But then, if we have $\neg A$ we also have
$A \lor \neg A$ and we arrive to a contradiction with the assumption. We should conclude
that $\neg \neg (A \lor \neg A)$.
#+end_proof

Note that this is, in fact, an intuitionistic proof. Although it seems
to use the intuitionistically forbidden technique of proving by
contradiction, it is actually only proving a negation.  There is a
difference between assuming $A$ to prove $\neg A$ and assuming $\neg
A$ to prove $A$: the first one is simply a proof of a negation, the
second one uses implicitly the law of excluded middle.

This can be translated to the Mikrokosmos implementation of simply typed \lambda-calculus
as the term

#+BEGIN_SRC haskell
notnotlem = \f.absurd (f (inr (\a.f (inl a))))
notnotlem
---> λa.(ABSURD (a (INR λb.(a (INL b))))) :: ((A + (A → ⊥)) → ⊥) → ⊥
#+END_SRC

whose type is precisely $\mathtt{((A + (A \to \bot)) \to \bot) \to \bot}$.
* Category theory (abstract)                                         :ignore:
* Category theory
# TODO: Presheaf categories

** Categories
*** Definition of category
We will think of a category as the algebraic structure that captures
the notion of composition. A category will be built from some sort
of objects linked by composable arrows; to which some associativity and
identity laws will apply.

**** Categories, objects and morphisms                            :ignore:
#+attr_latex: :options [Category]
#+begin_definition
A *category* ${\cal C}$, as defined in cite:maclane78, is given by

 * ${\cal C}_0$, a collection[fn:collection] whose elements are called
   *objects*[fn:objectnotation], and
 * ${\cal C}_1$, a collection whose elements are called *morphisms*.

Every morphism $f \in {\cal C}_1$ has two objects assigned: a
*domain*, written as $\mathrm{dom}(f) \in {\cal C}_0$, and a
*codomain*, written as $\mathrm{cod}(f) \in {\cal C}_0$; a common
notation for such morphism is
\[
f \colon \mathrm{dom}(f) \to \mathrm{cod}(f).
\]

Given two morphisms $f \colon A \to B$ and $g \colon B \to C$ there
exists a *composition morphism*, written as $g \circ f \colon A \to C$.
Morphism composition is a binary associative operation with
identity elements $\id_{A}\colon A \to A$, that is
\[
h \circ (g \circ f) = (h \circ g) \circ f
\quad\text{ and }\quad
f \circ \id_A = f = \id_B \circ f.
\]
#+end_definition

[fn:objectnotation]: We will sometimes write this class of objects of
a category ${\cal C}$ as $\mathrm{obj}({\cal C})$, but it is common to simply use ${\cal C}$ to denote it.

[fn:collection]: We use the term /collection/ to denote
some unspecified formal notion of compilation of "things" that could
be given by sets or proper classes. We will want to define categories
whose objects are all the possible sets and we will need the objects
to form a proper class in order to avoid inconsistent results such as
the Russell's paradox.

**** Definition of hom-sets and small categories                  :ignore:
#+attr_latex: :options [Hom-sets]
#+begin_definition
The *hom-set* of two objects $A,B$ on a category is the collection of morphisms 
between them. It is written as $\hom(A,B)$. The set of *endomorphisms*
of an object $A$ is the hom-set $\mathrm{end}(A) = \hom(A,A)$.
#+end_definition

Sometimes, when considering a hom-set, it will be useful to explicitly
specify the category on which we are working as $\hom_{{\cal C}}(A,B)$.

#+attr_latex: :options [Small and locally small categories]
#+begin_definition
A category is said to be *small* if the collections ${\cal C}_0,{\cal C}_1$ of objects and morphisms
are both sets (instead of proper classes). It is said to be *locally small* if every
hom-set is actually a set.
#+end_definition

*** Morphisms
**** Isomorphisms                                                 :ignore:
#+attr_latex: :options [Isomorphisms]
#+begin_definition
A morphism $f : A \to B$ is an *isomorphism* if an inverse morphism $f^{-1} : B \to A$
such that

  * $f^{-1} \circ f = \id_{A}$,
  * $f \circ f^{-1} = \id_{B}$;

exists.
#+end_definition

We call *automorphisms* to the endomorphisms which are also isomorphisms.

#+attr_latex: :options [Unicity of inverses]
#+begin_proposition
<<prop-unicityinverse>>
If the inverse of a morphism exists, it is unique. In fact, if a
morphism has a left-side inverse and a right-side inverse, they are
both-side inverses and they are equal.
#+end_proposition
#+begin_proof
Given $f : A \to B$ with inverses $g_1,g_2 : B \to A$; we have that
\[
g_1 = g_1 \circ \id_A = g_1 \circ (f \circ g_2) = 
(g_1 \circ f) \circ g_2 =
\id \circ g_2 = g_2.
\]
We have used associativity of composition, neutrality of the identity 
and the fact that $g_1$ is a left-side inverse and $g_2$ is a 
right-side inverse.
#+end_proof

#+begin_definition
Two objects are *isomorphic* if an isomorphism between them exists.
We write $A \cong B$ when $A$ and $B$ are isomorphic.
#+end_definition

#+attr_latex: :options [Isomorphy is an equivalence relation]
#+begin_proposition
The relation of being isomorphic is an equivalence relation. In
particular,

 * the identity, $\id = \id^{-1}$;
 * the inverse of an isomorphism, $(f^{-1})^{-1} = f$;
 * and the composition of isomorphisms, $(f \circ g)^{-1} = g^{-1} \circ f^{-1}$;

are all isomorphisms.
#+end_proposition

**** Monomorphisms and epimorphisms                               :ignore:
#+attr_latex: :options [Monomorphisms and epimorphisms]
#+begin_definition
A *monomorphism* is a left-cancellable morphism, that is, $f : A \to B$ is
a monomorphism if, for every $g,h : B \to A$, 
\[ f \circ g = f \circ h  \implies g = h.
\]
An *epimorphism* is a right-cancellable morphism, that is, $f : A \to B$ is
an epimorphism if, for every $g,h : B \to A$,
\[ g \circ f = h \circ f \implies g = h.
\]
A morphism which is a monomorphism and an epimorphism at the same time is
called a *bimorphism*.
#+end_definition

#+begin_remark
A morphism can be a bimorphism without being an isomorphism.
#+end_remark

**** Retractions and sections                                     :ignore:
#+attr_latex: :options [Retractions and sections]
#+begin_definition
A *retraction* is a left inverse, that is, a morphism which has a right inverse;
conversely, a *section* is a right inverse, a morphism which has a left inverse.
#+end_definition

By virtue of Proposition [[prop-unicityinverse]], a morphism which is both a
retraction and a section is an isomorphism.

*** Terminal objects, products and coproducts
#+attr_latex: :options [Initial object]
#+begin_definition
An object $I$ is an *initial object* if every object is the domain of exactly
one morphism to it. That is, for every object $A$ exists an unique morphism $I \to A$.
#+end_definition

#+attr_latex: :options [Terminal object]
#+begin_definition
An object $T$ is a *terminal object* if every object is the codomain of exactly
one morphism from it. That is, for every object $A$ exists an unique $A \to T$.
#+end_definition

#+attr_latex: :options [Zero object]
#+begin_definition
A *zero object* is an object which is both initial and terminal at the
same time.
#+end_definition

#+attr_latex: :options [Initial and final objects are essentially unique]
#+begin_proposition
<<prop-initialfinalunique>>
Initial and final objects in a category are essentially unique; that
is, any two initial objects are isomorphic and any two final objects
are isomorphic.
#+end_proposition
#+begin_proof
If $A,B$ were initial objects, by definition, there would be only one
morphism $f : A \to B$ and only one morphism $g : B \to A$. Moreover, there
would be only an endomorphism in $\mathrm{End}(A)$ and $\mathrm{End}(B)$ which should be
the identity. That implies,

  * $f \circ g = \id$,
  * $g \circ f = \id$.

As a consequence, $A \cong B$. A similar proof can be written for the terminal
object.
#+end_proof

# The definition of product was given by MacLane on 1949. (Awodey on CTF2.0)

#+attr_latex: :options [Product object]
#+begin_definition
An object $C$ is the *product* of two objects $A,B$ on a category if there
are two morphisms
\[\begin{tikzcd}
A & C \rar{\pi_B}\lar[swap]{\pi_A} & B
\end{tikzcd}\]
such that, for any other object $D$ with two morphisms $f_1 : D \to A$ and
$f_2 : D \to B$, an unique morphism $h : D \to C$, such that $f_1 = \pi_A \circ h$
and $f_2 = \pi_B \circ h$. Diagramatically,
\[\begin{tikzcd}[column sep=tiny]
& D \dar[dashed]{\exists! h} \ar[bend left]{ddr}{f_2}\ar[bend right,swap]{ddl}{f_1} & \\
& C \drar{\pi_B}\dlar[swap]{\pi_A} & \\
A && B &.
\end{tikzcd}\]
#+end_definition

Note that the product of two objects does not have to exist on a category;
but when it exists, it is essentially unique. In fact, we will be able later
to construct a category in which the product object is the final object of
the category and Proposition [[prop-initialfinalunique]] can be applied. We will
write /the/ product object of $A,B$ as $A \times B$.


#+attr_latex: :options [Coproduct object]
#+begin_definition
An object $C$ is the *coproduct* of two objects $A,B$ on a category if there
are two morphisms
\[\begin{tikzcd}
A \rar{i_A} & C & B \lar[swap]{i_B}
\end{tikzcd}\]
such that, for any other object $D$ with two morphisms $f_1 : D \to A$ and
$f_2 : D \to B$, an unique morphism $h : D \to C$, such that $f_1 = i_A \circ h$
and $f_2 = i_B \circ h$. Diagramatically,
\[\begin{tikzcd}[column sep=tiny]
& D & \\
& C \uar[dashed]{\exists! h}  & \\
A \ar[bend left]{uur}{f_1}\urar{i_A} && B \ular[swap]{i_B} \ar[bend right,swap]{uul}{f_2} &.
\end{tikzcd}\]
#+end_definition

The same discussion we had earlier for the product can be rewritten here for
the coproduct only reversing the direction of the arrows. We will write /the/
coproduct of $A,B$ as $A \amalg B$. As we will see later, the notion of a coproduct
is dual to the notion of product; and the same proofs can be applied on
both cases, only by reversing the arrows.

*** Examples of categories
**** Discrete categories                                          :ignore:
#+attr_latex: :options [Discrete categories]
#+begin_exampleth
A category is *discrete* if it has no other morphisms than the identities.
A discrete category is uniquely defined by its class of objects and
every class of objects defines a category.
#+end_exampleth

**** Monoids, groups                                              :ignore:
#+attr_latex: :options [Monoids, groups]
#+begin_exampleth
A single-object category is a *monoid*. [fn:monoid] A monoid in which
every morphism is an isomorphism is a *group*. A *groupoid* is a
category (of any number of objects) where all the morphisms are
isomorphisms.
#+end_exampleth

[fn:monoid]: This definition is equivalent to the usual definition of monoid
if we take the morphisms as elements of the monoid and composition of morphisms
as the monoid operation.

**** Posets                                                       :ignore:
#+attr_latex: :options [Partially ordered sets]
#+begin_exampleth
Every partial ordering defines a category in which the elements are the
objects and an only morphism between two objects $\rho_{a,b} : a \to b$ exists 

In particular, every ordinal can be seen as a partially ordered set
and defines a category.
#+end_exampleth

**** Category of Sets                                             :ignore:
#+attr_latex: :options [The category of sets]
#+begin_exampleth
The category $\Sets$ is defined as the category with all the
possible sets as objects and functions between them as morphisms. It
is trivial to check associativity of composition and the existence of
the identity function for any set.
#+end_exampleth

**** Category of Groups                                           :ignore:
#+attr_latex: :options [The category of groups]
#+begin_exampleth
The category $\mathtt{Grp}$ is defined as the category with groups as objects
and group homomorphisms between them as morphisms.
#+end_exampleth

**** Category of R-modules                                        :ignore:
#+attr_latex: :options [The category of R-modules]
#+begin_exampleth
The category $R\text{-Mod}$ is defined as the category with $R\text{-modules}$ as
objects and module homomorphisms between them as morphisms. We know
that the composition of module homomorphisms and the identity are
also module homomorphisms.

In particular, abelian groups form a category as $\mathbb{Z}\text{-modules}$.
#+end_exampleth

**** Category of Topological spaces                               :ignore:
#+attr_latex: :options [The category of topological spaces]
#+begin_exampleth
The category $\mathtt{Top}$ is defined as the category with topologicaal spaces as
objects and continuous functions between them as morphisms.
#+end_exampleth

** Functors and natural transformations
#+begin_quote
"Category" has been defined in order to define "functor" and "functor"
has been defined in order to define "natural transformation".

      -- *Saunders MacLane*, /Categories for the working mathematician/.
#+end_quote

Functors and natural transformations were defined for the first time
by Eilenberg and MacLane in cite:maclane42 while studying Čech
cohomology. While initially they were devised mainly as a language for
studying homology, they have proven its foundational value with the
passage of time.

*** Functors
**** Definition of functor                                        :ignore:
#+attr_latex: :options [Functor]
#+begin_definition
A *functor* will be interpreted as a morphism of categories.
Given two categories ${\cal C}$ and ${\cal D}$, a functor between them $F : {\cal C} \to {\cal D}$ is given by

  * an *object function*, $F : \mathrm{obj}({\cal C}) \to \mathrm{obj}({\cal D})$;
  * and an *arrow function*, $F : (A \to B) \to (FA \to FB)$ for any two
    objects $A,B$ of the category;

such that

  * $F(\id_A) = \id_{FA}$, identities are preserved;
  * $F(f \circ g) = Ff \circ Fg$, the functor respects composition.
#+end_definition

**** Composition of functors                                      :ignore:
Functors can be composed as we did with morphisms. In fact, a category of
categories can be defined; having functors as morphisms.

#+attr_latex: :options [Composition of functors]
#+begin_definition
Given two functors $F \colon {\cal C} \to {\cal B}$ and $G \colon {\cal B} \to {\cal A}$, their composite functor
$G \circ F : {\cal C} \to {\cal A}$ is given by the composition of object and arrow functions
of the functors. This composition is trivially associative.
#+end_definition

#+attr_latex: :options [Identity functor]
#+begin_definition
The identity functor on a category $I_{{\cal C}}\colon {\cal C} \to {\cal C}$ is given by identity object
and arrow functions. It is trivially neutral with respect to composition.
#+end_definition

**** Full and faithfull functors                                  :ignore:
#+attr_latex: :options [Full functor]
#+begin_definition
A functor $F$ is *full* if the arrow map is surjective. That is, if every
$g : FA \to FB$ is of the form $Ff$ for some morphism $f \colon A \to B$.
#+end_definition
#+attr_latex: :options [Faithful functor]
#+begin_definition
A functor $F$ is *faithful* if the arrow map is injective. That is,
if, for every two arrows $f_1,f_2 : A \to B$, $Ff_1 = Ff_2$ implies
$f_1 = f_2$.
#+end_definition

It is easy to notice that the composition of faithful (respectively,
full) functors is again a faithful functor (respectively, full).

**** Isomorphisms of categories                                   :ignore:
#+attr_latex: :options [Isomorphism of categories]
#+begin_definition
An *isomorphism of categories* is a functor $T$ whose object and arrow functions
are bijections. Equivalently, it is a functor $T$ such that there exists an /inverse/
functor $S$ such that $T \circ S$ and $S \circ T$ are identity functors.
#+end_definition

# Subcategories
# Category of categories
*** Natural transformations
#+attr_latex: :options [Natural transformation]
#+begin_definition
A *natural transformation* between two functors with the same domain
and codomain, $\alpha\colon F \todot G$, is a family of morphisms parameterized by 
the objects of the domain category, $\alpha_C\colon FC \to GC$ such that the
following diagram commutes

\[\begin{tikzcd}
C \dar{f} & & SC \rar{\tau_C}\dar{Sf} & TC \dar{Tf} \\
C' & & SC' \rar{\tau_{C'}} & TC'
\end{tikzcd}\]

for every arrow $f : C \to C'$.
#+end_definition

It is also said that the family of morphisms is /natural/ in its
parameter. This naturality property is what allows us to translate a
commutative diagram from a functor to another.

\[\begin{tikzcd}
A\arrow{dd}{h}\drar{f} &   & & F A\arrow{dd}{F h}\drar{F f} \arrow{rrr}{\tau A} &     & & G A\arrow{dd}{}\drar{G f} &     \\
  & B \dlar{g} & &     & F B \dlar{F g} \arrow{rrr}{\tau B} & &     & G B \dlar{G g} \\
C &   & & F C \arrow{rrr}{\tau C} &     & & G C &     \\
\end{tikzcd}\]

#+attr_latex: :options [Natural isomorphism]
#+begin_definition
A *natural isomorphism* is a natural transformation in which every component,
every morphism of the parameterized family, is invertible.
#+end_definition

The inverses of a natural transformation form another natural
transformation, whose naturality follows from the naturality of the
original transformation.

**** TODO Equivalence of categories
*** Composition of natural transformations
**** Vertical composition                                         :ignore:
#+attr_latex: :options [Vertical composition of natural transformations]
#+begin_definition
The *vertical composition* of two natural transformations $\tau : S\to T$
and $\sigma : R \to S$, denoted by $\tau \cdot \sigma$ is the family of morphisms defined by the objectwise
composition of the components of the two natural transformations, i.e.

\[\begin{tikzcd}
Rc \rar{Rf}\dar{\sigma_c}\arrow[swap,bend right=90]{dd}{(\tau \circ \sigma)_c} &
Rc' \dar{\sigma_{c'}} \arrow[bend left=90]{dd}{(\tau \circ \sigma)_{c'}} \\
Sc \rar{Sf}\dar{\tau_c}  &
Sc' \dar{\tau_{c'}} \\
Tc \rar{Tf}  &  Tc' 
\end{tikzcd}\]
#+end_definition

#+attr_latex: :options [Vertical composition is a natural transformation]
#+begin_proposition
The vertical composition of two natural transformations is in fact
a natural transformation.
#+end_proposition
#+begin_proof
Naturality of the composition follows from the naturality of its two
factors. In other words, the commutativity of the external square on
the above diagram follows from the commutativity of the two internal
squares.
#+end_proof

**** Horizontal composition                                       :ignore:
#+attr_latex: :options [Horizontal composition of natural transformations]
#+begin_definition
The *horizontal composition* of two natural transformations $\tau \colon S \to T$ and
$\tau' \colon S' \to T'$, with domains and codomains as in the following diagram
\[\begin{tikzcd}
C 
\arrow[bend left=50]{r}[name=U,below]{}{S}
\arrow[bend right=50]{r}[name=D]{}[swap]{T}
& 
B \arrow[Rightarrow,from=U,to=D]{}{\tau}
\arrow[bend left=50]{r}[name=UU,below]{}{S'}
\arrow[bend right=50]{r}[name=DD]{}[swap]{T'}
&
C \arrow[Rightarrow,from=UU,to=DD]{}{\sigma}
\end{tikzcd}\]

is denoted by $\tau' \circ \tau \colon S'S \to T'T$ and is defined as the family of morphisms
given by $\tau' \circ \tau = T' \tau \circ \tau' = \tau' \circ S'\tau$, that is, by the diagonal of the 
following commutative square
\[\begin{tikzcd}
S'Sc\rar{\tau'_{Sc}} \drar{\scriptsize{(\tau' \circ \tau)_c}} \dar[swap]{S'\tau_c} & T'Sc \dar{T' \tau_c} \\
S'Tc\rar{\tau'_{Tc}} & T'Tc
\end{tikzcd}\]
#+end_definition

#+attr_latex: :options [Horizontal composition is a natural transformation]
#+begin_proposition
The horizontal composition of two natural transformations is in fact
a natural transformation.
#+end_proposition
#+begin_proof
It is natural as the following diagram is the composition of two
naturality squares
\[\begin{tikzcd}
S'Sc \rar{S'\tau} \dar{S'Sf} &
S'Tc \rar{\tau'}  \dar{S'Tf} &
T'Tc \dar{T'Tf} \\
S'Sb \rar{S'\tau} &
S'Tb \rar{\tau'} &
T'Tb
\end{tikzcd}\]

defined respectively by the naturality of $S'\tau$ and $\tau'$.
#+end_proof

** Constructions on categories
*** Opposite categories and contravariant functors
#+attr_latex: :options [Opposite category]
#+begin_definition
The *opposite category* ${\cal C}^{op}$ of a category ${\cal C}$ is a category with the
same objects as ${\cal C}$ but with all its arrows reversed. That is, for each
morphism $f : A \to B$, there exists a morphism $f^{op} : B \to A$ in ${\cal C}^{op}$.
Composition is defined as
\[
f^{op} \circ g^{op} = (g\circ f)^{op},
\]
exactly when the composite $g \circ f$ is defined in ${\cal C}$.
#+end_definition

Reversing all the arrows is a process that directly translates every
property of the category into a /dual/ property. A morphism $f$ is a
monomorphism if and only if $f^{op}$ is an epimorphism; a terminal object
in ${\cal C}$ is an initial object in ${\cal C}^{op}$ and a right inverse becomes a left
inverse on the opposite category. This process is also an /involution/,
where $(f^{op})^{op}$ can be seen as $f$ and $({\cal C}^{op})^{op}$ is trivially isomorphic to ${\cal C}$.

#+attr_latex: :options [Contravariant functor]
#+begin_definition
A *contravariant* functor from ${\cal C}$ to ${\cal D}$ is a functor from the opposite category,
that is, $F \colon {\cal C}^{op}\to {\cal D}$. Non-contravariant functors are often called *covariant*
functors, to emphasize the difference.
#+end_definition

# Hom functors as an example of covariant and contravariant functors
*** TODO The category of all categories
*** Product categories
#+attr_latex: :options [Product category]
#+begin_definition
The *product category* of two categories ${\cal C}$ and ${\cal D}$, denoted by ${\cal C} \times {\cal D}$ is a
category having

  * pairs $\left\langle c,d \right\rangle$ as objects, where $c \in {\cal C}$ and $d \in {\cal D}$;
  * and pairs $\pair{f,g} : \pair{c,d} \to \pair{c',d'}$ as morphisms, where $f \colon c \to c'$ and
    $g \colon d \to d'$ are morphisms in their respective categories.

The identity morphism of any object $\pair{c,d}$ is $\pair{\id_c, \id_d}$, and composition is
defined componentwise as
\[
(f',g') \circ (f,g) = (f' \circ f,g' \circ g).
\]
#+end_definition

We also define *projection functors* $P\colon {\cal C} \times {\cal D} \to {\cal C}$ and $Q : {\cal C} \times {\cal D} \to {\cal D}$
on arrows as $P\pair{f,g} = f$ and $Q\pair{f,g} = g$. Note that this definition of
product, using these projections, would be the product of two categories on a
category of categories with functors as morphisms.

#+attr_latex: :options [Product of functors]
#+begin_definition
The *product functor* of two functors $F\colon {\cal C} \to {\cal C}'$ and $G \colon {\cal D} \to {\cal D}'$ is a
functor $F \times G \colon {\cal C} \times {\cal D} \to {\cal C}' \times {\cal D}'$ which can be defined

  * on objects as $(F \times G)\pair{c,d} = \pair{Fc,Gd}$;
  * and on arrows as $(F \times G)\pair{f,g} = \pair{Ff,Gg}$.
#+end_definition

It can be seen as the unique functor making the following diagram
commute

\[\begin{tikzcd}
{\cal C} \dar{F} &
{\cal C} \times {\cal D}  \rar{Q}\lar[swap]{P} \dar[dashed]{F \times G}&
{\cal D} \dar{G} \\
{\cal C}' &
{\cal C}' \times {\cal D}' \rar[swap]{Q'}\lar{P'}&
{\cal D}'
\end{tikzcd}\]

In this sense, the $\times$ operation is itself a functor acting on objects
and morphisms of the $\mathtt{Cat}$ category of all categories.

*** TODO Bifunctors
# Definition of Bifunctors
# Bifunctors are defined by its two parts, obtained by fixing an element
# Proposition 1 on page 37 Maclane

*** TODO Comma categories
** Universality
*** Universal arrows
#+attr_latex: :options [Universal arrow]
#+begin_definition
A *universal arrow* from $c$ to $S$ is an arrow $u \colon c \to Sr$ such that
for every $c \to Sd$ exists a unique $r \to d$ making this diagram commute

\[\begin{tikzcd}
& Sd & d \\
c \rar[swap]{u}\urar{g} & Sr \uar[swap,dashed]{Sf} & r \uar[dashed]{\exists! f} &.
\end{tikzcd}\]
#+end_definition

#+attr_latex: :options [Universality in terms of hom-sets]
#+begin_proposition
The arrow $u \colon c \to Sr$ is universal iff $f \mapsto Sf \circ u$ is a bijection
$\hom(r,d) \cong \hom(c,Sd)$ natural in $d$. Any natural bijection of this
kind is determined by a unique universal arrow.
#+end_proposition
#+begin_proof
Bijection follows from the definition of universal arrow, and
naturality follows from $S(gf)\circ u = Sg \circ Sf \circ u$.

Given a bijection $\varphi$, we define $u = \varphi(\id_r)$.
By naturality we have the bijection $\varphi(f) = Sf \circ u$, every arrow
is written in this way.
#+end_proof

# TODO Universal from a functor to another

*** Representability
#+attr_latex: :options [Representation of a functor]
#+begin_definition
A *representation* of $K \colon D \to \Sets$ is a natural isomorphism
\[
\psi\colon \hom_{D}(r,-) \cong K.
\]

A functor is /representable/ if it has a representation. An object $r$ is
called a /representing object/. $D$ must have small hom-sets.
#+end_definition

#+attr_latex: :options [Representations in terms of universal arrows]
#+begin_proposition
If $u \colon \ast \to Kr$ is a universal arrow for a functor $K\colon D \to \Sets$, then
$f \mapsto K(f)(u\ast)$ is a representation. Every representation is obtained
in this way.
#+end_proposition
#+begin_proof
We know that $\hom(\ast, X) \xrightarrow{.} X$ is a natural isomorphism in $X$; in particular
$\hom(\ast, K-) \xrightarrow{.} K-$. Every representation is built then as

\[ \hom_{D}(r,-) \cong \hom(\ast,K-) \cong K, \]

for every natural isomorphism $D(r,-) \cong \Sets(\ast,K-)$. But every natural
isomorphism of this kind is a [[*Universal arrows as natural bijections][universal arrow]].
#+end_proof

*** Yoneda Lemma
#+attr_latex: :options [Yoneda Lemma]
#+begin_lemma
For any $K\colon D \to \Sets$ and $r \in D$, there is a bijection
\[
y \colon \mathrm{Nat}(\hom_{D}(r,-), K) \cong Kr
\]

sending the natural transformation $\alpha \colon \hom_{D}(r,-) \xrightarrow{.} K$
to the image of the identity, $\alpha_r1_r$.
#+end_lemma
#+begin_proof

#+end_proof

#+attr_latex: :options [Characterization of natural transformations between representable functors]
#+begin_corollary
Given $r,s \in D$, any natural transformation $\hom(r,-) \xrightarrow{.} \hom(s,-)$ has
the form $h_{\ast}$ for a unique $h\colon s \to r$.
#+end_corollary
#+begin_proof
Using Yoneda Lemma, we know that
\[
\mathrm{Nat}(\hom_D(r,-), \hom_D(s,-)) \cong \hom_D(s,r),
\]

sending the natural transformation to a morphism $\alpha(id_r) = h \colon s \to r$. The
rest of the natural transformation is determined as $h_{\ast}$ by naturality.
#+end_proof

#+attr_latex: :options [Addendum to the Yoneda Lemma]
#+begin_proposition
The bijection on the [[*Yoneda Lemma][Yoneda Lemma]] is a natural isomorphism between
two $\Sets^D \times D \to \Sets$ functors.
#+end_proposition
#+begin_proof
# TODO
#+end_proof

#+begin_definition
In the conditions of [[*Yoneda Lemma][Yoneda Lemma]], the *Yoneda functor*,
$Y \colon D^{op} \to \Sets^{D}$, is defined with the arrow function

\[
\left(f \colon s \to r\right) \mapsto 
\Big(D(f,-) \colon D(r,-) \to D(s,-)\Big).
\]
#+end_definition

#+begin_proposition
The Yoneda functor is full and faithful.
#+end_proposition
#+begin_proof
# TODO
#+end_proof

# The Yoneda functor is a currying of the hom functor.

*** TODO Products and limits
**** TODO Pullbacks
**** TODO Example: p-adic numbers
*** TODO Coproducts and colimits
**** TODO Coproducts as universal arrows
**** TODO Pushouts
**** Colimits
#+attr_latex: :options [Colimits]
#+begin_definition
A *colimit* of a functor from a given /index category/ $F \colon J \to {\cal C}$ is an object
$r \in {\cal C}$ such that there exists a universal arrow $u \colon F \todot \Delta r$ from $F$ to $\Delta$.
It is usually written as $r = \varinjlim F$.
#+end_definition

** Adjoints
*** Adjunctions
#+attr_latex: :options [Adjunction]
#+begin_definition
An *adjunction* from categories $X$ to $A$ is a pair of functors
$F\colon X \to A$, $G\colon A \to X$ with a natural bijection
\[
\varphi \colon \hom(Fx,a) \cong \hom(x,Ga),
\]

natural in both $x\in X$ and $a \in A$. We write it as $F \dashv G$.
#+end_definition

#+attr_latex: :options [Unit and counit of an adjunction]
#+begin_definition
An adjunction determines a *unit* and a *counit*;

 1) the *unit* is natural transformation made with universal arrows $\eta\colon I \todot GF$, where
    the right adjoint of each $f \colon Fx \to a$ is
    \[
    \varphi f = Gf \circ \eta_x \colon x \to Ga.
    \]
 2) the *counit* is natural transformation made with universal arrows $\varepsilon \colon FG \todot I$, where
    the left adjoint of each $g \colon x \to Ga$ is
    \[
    \varphi^{-1}g = \varepsilon \circ Fg \colon Fx \to a.
    \]

that follow the /triangle identities/ $\eta G \circ G \varepsilon = \id$ and $F\eta \circ \varepsilon F = \id$.
#+end_definition

#+attr_latex: :options [Characterization of adjunctions]
#+begin_proposition
Each adjunction is completely determined by any of

 1) functors $F,G$ and $\eta\colon 1 \todot GF$ where $\eta_x\colon x \to GFx$ is universal to $G$.
 2) functor $G$ and universals $\eta_x \colon x \to GF_0 x$, creating a functor $F$.
 3) functors $F,G$ and $\varepsilon\colon FG \todot 1$ where $\varepsilon_a\colon FGa \to a$ is universal from $F$.
 4) functor $F$ and universals $\varepsilon_a\colon FG_0a \to a$, creating a functor $G$.
 5) functors $F,G$, with units and counits satisfiying the triangle
    identities $\eta G \circ G \varepsilon = \id$ and $F\eta \circ \varepsilon F = \id$.
#+end_proposition
#+begin_proof
# TODO
#+end_proof

** Monads and algebras
*** Monads
**** Definition of Monads                                         :ignore:
#+attr_latex: :options [Monad]
#+begin_definition
A *monad* is a functor $T\colon X \to X$ with natural transformations

 * $\eta\colon I \todot T$, called /unit/
 * $\mu \colon T^2 \todot T$, called /multiplication/

such that
\[\begin{tikzcd}
T^3 \rar{T\mu}\dar{\mu T} & T^2\dar{\mu} \\
T^2 \rar{\mu} & T
\end{tikzcd}
\qquad
\begin{tikzcd}
IT \rar{\eta T}\drar[swap]{\cong} & T^2\dar{\mu} & \lar[swap]{T\eta}\dlar{\cong} TI \\
& T & &.
\end{tikzcd}\]
#+end_definition

**** Each adjunction gives rise to a monad                        :ignore:
#+attr_latex: :options [Each adjunction gives rise to a monad]
#+begin_proposition
Given $F \dashv G$, $GF$ is a monad.
#+end_proposition
#+begin_proof
We take the unit of the adjunction as the monad unit. We define the
product as $\mu = G\varepsilon F$. Associativity follows from these diagrams
\[\begin{tikzcd}
FGFG\rar{FG\varepsilon} \dar[swap]{\varepsilon FG} & FG \dar{\varepsilon} \\
FG\rar{\varepsilon} & I
\end{tikzcd}
\qquad
\begin{tikzcd}
GFGFGF\rar{GFG\varepsilon F} \dar[swap]{G\varepsilon FGF} & GFGF \dar{G\varepsilon F} \\
GFGF\rar{G\varepsilon F} & GF &,
\end{tikzcd}\]

where the first is commutative by the [[*Interchange law][interchange law]] and the second
is obtained by applying functors $G$ and $F$. Unit laws follow from
the [[*Unit and counit][triangular identities]] after applying $F$ and $G$.
#+end_proof

*** Comonads
#+attr_latex: :options [Comonad]
#+begin_definition
A *comonad* is a functor $L\colon X \to X$ with natural transformations

 * $\varepsilon\colon L\to I$, called /counit/
 * $\delta\colon L \to L^2$, called /comultiplication/

such that
\[\begin{tikzcd}
L\rar{\delta} \dar[swap]{\delta} & L^{2} \dar{L\delta} \\
L^{2}\rar{\delta L} & L^{3}
\end{tikzcd}
\qquad
\begin{tikzcd}
& L \dar{\delta} \dlar[swap]{\cong} \drar{\cong} & \\
IL & 
L^2 \lar{\varepsilon L}\rar[swap]{L \varepsilon} & 
LI
&.
\end{tikzcd}\]
#+end_definition

*** Algebras for a monad
#+attr_latex: :options [T-algebra]
#+begin_definition
For a monad $T$, a $T\textbf{-algebra}$ is an object $x$ with an arrow $h \colon Tx \to x$ called 
/structure map/ making these diagrams commute
\[\begin{tikzcd}
T^{2}x \rar{Th}\dar[swap]{\mu} & Tx \dar{h} \\
Tx\rar{h} & x &.
\end{tikzcd}
\]
#+end_definition

#+attr_latex: :options [Morphism of T-algebras]
#+begin_definition
A *morphism of T-algebras* is an arrow $f\colon x \to x'$ making the following square
commute
\[\begin{tikzcd}
Tx \dar[swap]{Tf}\rar{h} & Tx \dar{f} \\
Tx' \rar[swap]{h'} & Tx' &.
\end{tikzcd}\]
#+end_definition

#+attr_latex: :options [Category of T-algebras]
#+begin_proposition
The set of all $T\text{-algebras}$ and their morphisms form a category $X^{T}$.
#+end_proposition
#+begin_proof
Given $f\colon x \to x'$ and $g\colon x'\to x''$, $T\text{-algebra}$ morphisms, their composition
is also a $T\text{-algebra}$ morphism, due to the fact that this diagram

\[\begin{tikzcd}
Tx \rar{h}\dar[swap]{Tf} & 
x \dar{f}\\
Tx' \dar[swap]{Tg} \rar{h'} &
x' \dar{g}\\
Tx'' \rar{h''}&
x''
\end{tikzcd}\]

commutes.
#+end_proof
** Kan extensions
*** Dinatural transformations
#+ATTR_LATEX: :options [Dinatural transformation]
#+BEGIN_definition
A *dinatural transformation* between functors $F,G \colon {\cal C}^{op} \times {\cal C} \to {\cal B}$
is a family of morphisms $\alpha_x \colon F(x,x) \to G(x,x)$ for each $x \in {\cal C}$,
called /components/, such that for every $f \colon x \to y$,
\[\begin{tikzcd} &
F(x,x) \rar{\alpha_{x}} &
G(x,x) \drar{G(\id,f)} & \\
F(y,x) \urar{F(f,\id)}\drar[swap]{F(\id,f)} & & &
G(x,y) \\
&
F(y,y) \rar{\alpha_{y}} &
G(y,y) \urar[swap]{G(f,\id)} &
\end{tikzcd}\]

commutes
#+END_definition

# TODO: Every natural transformation gives a dinatural transformation.

**** TODO Natural transformation of covariant to contravariant functor :ignore:
**** Extranatural transformations (wedges)                        :ignore:
#+ATTR_LATEX: :options [Extranatural transformation]
#+BEGIN_definition
An *extranatural transformation* (also called *wedge*) is a dinatural
transformation to a constant functor.
#+END_definition

That is, an extranatural transformation from the functor $F$ to the
object $a$ is a family of morphisms $\alpha_x \colon F(x,x) \to b$ such that

\[\begin{tikzcd}
F(y,x)\rar{F(\id,f)} \dar[swap]{F(f,\id)} & F(y,y) \dar{\alpha_{y}} \\
F(x,x)\rar[swap]{\alpha_{x}} & b
\end{tikzcd}\]

commutes for every $f \colon x \to y$. We write wedges as $\alpha \colon F \toddot b$.

**** TODO Examples of dinatural transformations                   :ignore:
*** Ends
# cowedges ?

#+ATTR_LATEX: :options [End]
#+BEGIN_definition
An *end* of $F \colon {\cal C}^{op} \times {\cal C} \to {\cal B}$ is a universal dinatural transformation
from a constant $e$ to $F$. That is, a wedge $\sigma \colon e \toddot F$ such that for every
other $\tau \colon k \toddot F$, there exists a unique $u \colon k \to e$ such that $\sigma_x \circ u = \tau_x$.
#+END_definition

We usually call /end/ to the object $e$, and we write
\[
e = \int_x F(c,c).
\]

**** TODO Examples of ends
\[
\mathrm{Nat}(U,V) = \int_x \hom(Ux,Vx)
\]

*** Coends
*** Kan extensions

* Categorical logic (abstract)                                       :ignore:
#+LATEX: \ctparttext{\color{black}\begin{center}
This section is based on cite:maclane94.

Topos theory arises independently with Grothendieck and sheaf theory,
Lawvere and the axiomatization of set theory and Paul Cohen with the
forcing techniques with allowed to construct new models of ZFC.
#+LATEX: \end{center}}

* Categorical logic
** Lawvere theories
# FROM: Introduction to categorical logic, Awodey & Bauer

# I should join the two first sections and exaplain the first definition
# only as an introduction.

*** Motivation for algebraic theories
We will develop an unified approach to the study of algebraic
structures based on constants, operatio4ns and equations; such as
groups, modules or rings.

**** First definition of algebraic theory                         :ignore:
Our *algebraic theories* are usually given by

  * a /signature/, a family of sets $\left\{ \Sigma_k \right\}_{k \in \mathbb{N}}$ whose elements are called
    /k-ary operations/. The /terms/ of a signature are defined inductively,
    being variables or k-ary operations applied to k-tuples of terms;

  * and a set of /axioms/, which are equations between terms.

**** Examples: theory of groups, theory of fields                 :ignore:
For example, the theory of groups is given by 

 * a /binary operation/ called $\cdot$,
 * a /unary operation/ written as $^{-1}$, and
 * a /nullary operation/ or a /constant/ called $e$.

Satisfiying the following equations

 * $(x \cdot y) \cdot z = x \cdot (y \cdot z)$,

 * $x \cdot e = x$,

 * $e \cdot x = x$,

 * $x \cdot x^{-1} = e$, and

 * $x^{-1} \cdot x = e$.

Quantifiers are not needed here, as we are interpreting
each $x,y,z$ as free variables.

Theories in which the operations are not defined for every possible
term, cannot be expressed in this way. Fields, in which the inverse of
$0$ is not defined, are not expressable in this form.

**** Interpretations of algebraic theories                        :ignore:
A theory can be *interpreted* on a category ${\cal C}$ as

  * an object of the category, $A \in {\cal C}$;
  * with morphisms $If \colon A^k \to A$ for every k-ary operation $f$.

Any interpretation of the theory induces an interpretation for every
term on a context. That is, a term $t$ can be given in the
variable context $x_1,\dots,x_n$ if all variables that
appear in $t$ appear in $x_1,\dots,x_n$. We write that as
\[
x_1,x_2,\dots,x_n \mid t;
\]
and the *interpretation of the term* $x_1,\dots,x_n \mid t$ *on that context* is a morphism
$I(x_1,\dots,x_n \mid t) \colon A^n \to A}$ defined inductively knowing that

  * the interpretation of the i-th variable is the i-th
    projection
    \[I(x_1,\dots,x_n \mid x_i) = \pi_i : A^n \to A,\]

  * the interpretation of an operation over a term is the
    interpretation of the morphism componsed with the componentwise
    interpretation of subterms
    \[
    I(f \pair{t_1,\dots,t_k}) =
    If \circ \pair{It_1,\dots,It_k}
    \colon A^n \to A.
    \]

The interpretation of a particular variable depends therefore on the
context. We say that an interpretation *satisfies* an equation $\Gamma \mid u=v$
in a particular given context if the interpretation of both terms of
the equation is the same on that context, $I(\Gamma \mid u) = I(\Gamma \mid v)$.

We usually would like to find interpretations where all the axioms of the
theory were satisfied. These are called *models of the algebraic theory*.

**** Notion of representation-free theories                       :ignore:
# This notion is not representation free
# Example
# Category of an algebraic theory
The problem with this notion of algebraic theory is that it is not
representation-free; it is not independent of the choice of constants,
operations or axioms. There may be multiple formulations of the same
theory, with different but equivalent axioms. For instance,
cite:mccune91 discusses many single-equation axiomatizations of groups, 
such as
\[
x\ /\
\big(((x/x)/y)/z)\ /\ ((x/x)/x)/z\big)
= y
\]

with the binary operation $/$, related to the usual multiplication as $x / y = x \cdot y^{-1}$.

Our solution to this problem will be to capture all the algebraic
information of a theory -- all operations, constants and axioms --
into a category. Differently presented but equivalent theories will
give rise to the same category. This category will have /contexts/
$[x_1,\dots,x_n]$ as objects. A morphism from $[x_1,\dots,x_n]$ to
$[x_1,\dots,x_m]$ will be a tuple of terms
\[ 
\pair{t_1,\dots,t_k}
\colon [x_1,\dots,x_n] \to [x_1,\dots,x_m]
\]
such that every $t_k$ is given in the context $[x_1,\dots,x_n]$. 
Composition is defined componentwise as substitution of the terms of the 
first morphism into the variables of the second one, that is,
\[
\pair{s_1,\dots,s_n} = \pair{u_1,\dots,u_n} \circ \pair{t_1,\dots,t_m},
\]
where
\[ 
s_i = u_i[t_1,\dots,t_m / x_1,\dots,x_m].
\]

Two morphisms in this category $\pair{t_1,\dots,t_n}$ and $\pair{s_1,\dots,s_n}$ are equal if the axioms of
the theory imply the componentwise equality of its terms, that is, $t_i = s_i$.

This interpretation will lead us to our definition of *algebraic*
*theory* as a category with finite products.

Every model $M$ in the previous sense could be seen as a functor from
this category to a given category ${\cal C}$ preserving finite products. Once the
image of $M[x_1] = A$ is chosen, the functor is determined on objects by
\[
M[x_1,\dots,x_n] = A^{k}
\]
and once it is defined for the basic operations, it is inductively determined
on morphisms as

  * $M\pair{x_i} = \pi_i \colon A^k \to A$, for any morphism $\pair{x_i}$;
  * $M\pair{t_1,\dots,t_m} = \pair{Mt_1,\dots,Mt_m} \colon A^m \to A$, the 
    componentwise interpretation of subterms;
  * $M\pair{f\pair{t_1,\dots,t_m}} = Mf \circ \pair{Mt_1,\dots,Mt_{m}} \colon (M\mathbb{A})^m\to M\mathbb{A}$.

The fact that $M$ is a well-defined functor follows from the
assumption that it is a model.

*** Algebraic theories as categories
**** Algebraic theory                                             :ignore:
#+ATTR_LATEX: :options [Lawvere algebraic theory]
#+BEGIN_definition
An *algebraic theory* is a category $\mathbb{A}$ with finite products and objects
forming a sequence $A^0,A^1,A^2,\dots$ such that $A^m \times A^n = A^{m+n}$ for
any $m,n$.
#+END_definition

From this definition, it follows that $A^0$ must be the terminal object.

**** Models as functors                                           :ignore:
#+ATTR_LATEX: :options [Model]
#+BEGIN_definition
A *model* of an algebraic theory $\mathbb{A}$ in a category ${\cal C}$ is a functor
$M \colon \mathbb{A} \to {\cal C}$ preserving all finite products.
#+END_definition

#+ATTR_LATEX: :options [Category of models of a theory]
#+BEGIN_definition
The *category of models* $\mathtt{Mod}_{{\cal C}}(\mathbb{A})$ is the full subcategory of functor
category ${\cal C}^{\mathbb{A}}$ given by the functors preserving all finite products.
Morphisms between models of a theory in a category are natural transformations.
#+END_definition

#+ATTR_LATEX: :options [Algebraic category]
#+BEGIN_definition
An *algebraic category* is category equivalent to a category of the form $\mathtt{Mod}_{{\cal C}}(\mathbb{A})$,
where $\mathbb{A}$ is an algebraic theory.
#+END_definition

**** Examples                                                     :ignore:
#+ATTR_LATEX: :options [Fields have no algebraic theory]
#+BEGIN_exampleth
The category $\mathtt{Fields}$ is not an algebraic category.
Any algebraic category $\mathtt{Mod}_{{\cal C}}(\mathbb{A})$ has a terminal object given by the
constant functor $\Delta_1 : \mathbb{A} \to {\cal C}$ to $1$, the terminal object of ${\cal C}$. Note that
${\cal C}$ must have a terminal object for a model to it to exist, as
models must preserve all finite products. We know that $\Delta_1$ is a
terminal object because, in general, it is the terminal object of the
category of functors ${\cal C}^{\mathbb{A}}$.
However, $\mathtt{Fields}$ has no terminal object.
#+END_exampleth

*** Completeness for algebraic theories
# TODO: What completeness is

#+ATTR_LATEX: :options [Completeness for algebraic theories]
#+BEGIN_theorem
Given $\mathbb{A}$ an algebraic theory, there exists a category ${\cal A}$ with
a model $U \in \mathtt{Mod}_{{\cal A}}(\mathbb{A})$ such that, for every terms $u,v$,
\[
U \text{ satisfies } u = v 
\iff
\mathbb{A} \text{ proves } u = v.
\] 
This is called the *universal model* for $\mathbb{A}$. This theorem
asserts that categorical semantics of algebraic theories are complete.
#+END_theorem
#+BEGIN_proof
Simply taking $\mathbb{A}$ with the identity functor, we have an universal
model for $\mathbb{A}$.
#+END_proof

The universal model needs not to be set-theoretic, but we can
always find a universal model in a presheaf category via the
Yoneda embedding.

#+ATTR_LATEX: :options [Yoneda embedding as a universal model]
#+BEGIN_proposition
The Yoneda embedding $y \colon \mathbb{A} \to \widehat{\mathbb{A}}$ is a universal model for $\mathbb{A}$.
#+END_proposition
#+BEGIN_proof
It preserves finite products because it preserves all limits, hence
it is a model. As it is a faithful functor, we know that any equation
proved in the model is an eqation proved by the theory.
#+END_proof

** Cartesian closed categories
*** Exponential
#+ATTR_LATEX: :options [Exponential]
#+BEGIN_definition
An *exponential* of $A$ and $B$ in a category with binary products is an
object $B^A$ with a morphism $e : B^A \times A \to B$ called /evaluation morphism/,
such that, for any $f : C \times A \to B$ exists a unique $\widetilde{f} : C \to B^A$ such that
the following diagram commutes
\[\begin{tikzcd}
B^A & B^A \times A \drar{e} & \\
C \uar[dashed]{\widetilde f}  & C \times A \rar[swap]{f}\uar{\widetilde f \times id} & B 
\end{tikzcd}\]
#+END_definition

An object $A$ for which the exponentiation $-^A$ is always defined is called
*exponentiable*.

#+ATTR_LATEX: :options [Exponentials as adjoints]
#+BEGIN_proposition
An object $A$ in a category with binary products ${\cal C}$ is exponentiable if and only if
the functor $(- \times A) : {\cal C} \to {\cal C}$ has a right adjoint.
#+END_proposition
#+BEGIN_proof
# TODO
#+END_proof

*** Cartesian category
#+ATTR_LATEX: :options [Cartesian category]
#+BEGIN_definition
A *cartesian category* is a category with all finite products.
#+END_definition
#+ATTR_LATEX: :options [Cartesian closed category]
#+BEGIN_definition
A *cartesian closed category* is a category with all finite products
and exponentials.
#+END_definition

The definition of cartesian closed category can be written in terms of
existence of adjoints.

#+ATTR_LATEX: :options [Cartesian closed categories and adjoints]
#+BEGIN_proposition
Any category ${\cal C}$ is cartesian closed if and only if there exist
right adjoints for the following functors

  * $! \colon {\cal C} \to 1$, the unique functor to the terminal category;
  * $\Delta \colon {\cal C} \to {\cal C} \times {\cal C}$, the diagonal functor;
  * $(- \times A) \colon {\cal C} \to {\cal C}$, the product functor, for each $A \in {\cal C}$.
#+END_proposition
#+BEGIN_proof
# TODO
#+END_proof

#+ATTR_LATEX: :options [Category of small cartesian closed categories]
#+BEGIN_definition
We call $\mathtt{Ccc}$ to the category of small cartesian closed
categories with functors preserving finite products and
exponentials as morphisms. These functors are called
*cartesian closed functors*.
#+END_definition

*** Frames and locales
#+ATTR_LATEX: :options [Completeness for posets]
#+BEGIN_proposition
Complete posets are cocomplete. Cocomplete posets are complete.
#+END_proposition
#+BEGIN_proof
# TODO
#+END_proof

#+ATTR_LATEX: :options [Frames]
#+BEGIN_definition
*Frames* are complete cartesian closed posets.
#+END_definition

# TODO: Equivalently A frame is a poset with distributive law.

#+ATTR_LATEX: :options [Frame morphism]
#+BEGIN_definition
A *frame morphism* is a function between frames preserving finite
infima and arbitrary suprema.
#+END_definition

# TODO: Locales

# TODO: Locales as non-pointed topological spaces, from An informal
# introduction to topos theory

** Heyting algebras
In this section, we develop the notion of a *Heyting algebra* and show
its differences with a Boolean algebra.

There is a correlation between classical propositional calculus and
the Boolean algebra of the subsets of a given set. If we interpret
a proposition $p$ as a subset of a given universal set $P \subset U$ and
fix an element $u \in U$, propositions can be translated to $u \in P$, 
logical connectives can be translated as

\[\begin{tabular}{cc|cc}
logic &  & &  subsets \\
\hline
$P \land Q$ & and & intersection & $P \cap Q$  \\
$P \lor Q$ & or & union  & $P \cup Q$ \\
$\neg P$ & not & complement & $\overline{P}$ \\
$P \to Q$ & implication & complement union & $\overline{P} \cup Q$
\end{tabular}\]

using crucially that $\neg P \land Q \equiv P \to Q$.

In the same way that Boolean algebras correspond to classical
propositional logic, Heyting algebras correspond to intuitionistic
propositional calculus. Its model on a set-like theory is not the subsets
of a given set, but instead, only the /open/ sets of a given
topological space

\[\begin{tabular}{cc|cc}
logic &  & & open sets \\
\hline
$P \land Q$ & and & intersection & $P \cap Q$  \\
$P \lor Q$ & or & union  & $P \cup Q$ \\
$\neg P$ & not & interior of the complement & $\mathrm{int}\left(\overline{P}\right)$ \\
$P \to Q$ & implication & interior of complement and consequent & $\mathrm{int}(\overline{P} \cup Q)$
\end{tabular}\]

where $\mathrm{int}$ is the topological interior of a set.
# TODO: This definition of the implication does not match with the used
# on Moerdijk. It is equivalent but different.

*** Lattices and Boolean algebras
**** Lattices, definition                                         :ignore:
#+attr_latex: :options [Lattice]
#+begin_definition
A *lattice* is a partially ordered set with all binary products and
coproducts. It is a *bounded lattice* if it has all finite products
and coproducts.
#+end_definition

We will usually work with bounded lattices and simply call them
/lattices/. A bounded lattice can be defined then by the following
inference rules

\begin{prooftree}
\AXC{$0 \leq x \leq 1$}
\AXC{$z \leq x$}
\AXC{$z \leq y$}
\doubleLine
\BIC{$z \leq x \land y$}
\AXC{$x \leq z$}
\AXC{$y \leq z$}
\doubleLine
\BIC{$x \lor y \leq z$}
\noLine
\TIC{$$}
\end{prooftree}


meaning that it has a terminal and a final object, in order to have
all finte products and coproducts, and all binary products and
coproducts.

#+ATTR_LATEX: :options [Lattice homomorphism]
#+BEGIN_definition
A lattice homomorphism is a function between lattices preserving
finite products and coproducts. That is, a function $f$ such that

  * $f(0) = 0$, $f(1) = 1$,
  * $f(x \wedge y) = f(x) \land f(y)$,
  * $f(x \vee y) = f(x) \vee f(y)$;

and the category of lattices and lattice homomorphisms is denoted
by $\mathtt{Lat}$.
#+END_definition

A bounded lattice can also be defined as a set with $0,1$ and two binary
operations $\land,\lor$ satisfying

  * $1 \land x = x$, and $0 \lor x = x$;
  * $x \land x = x$, and $x \lor x = x$;
  * $x \land (y \lor x) = x = (x \land y) \lor x$;
  * $x \land y = y \land x$ and $x\lor y = y \lor x$.

This perspective allows us also to define a lattice object in any
category as an object $L$ with morphisms

\[
\land \colon L \times L \to L,
\quad
\lor \colon L \times L \to L,
\quad
0,1 \colon \mathrm{I} \to L,
\]

where $\mathrm{I}$ is the terminal object of the category; and commutative
diagrams encoding the previous equations.

**** Distributive lattices                                        :ignore:
#+attr_latex: :options [Distributive lattice]
#+begin_definition
A *distributive lattice* is a lattice where
\[
x \land (y \lor z) = (x \land y) \lor (x \land z),
\]

holds for all $x,y,z$.
#+end_definition
# TODO: This implies the dual distributive law

**** Complements                                                  :ignore:
#+attr_latex: :options [Complement]
#+begin_definition
A *complement* of $a$ in a bounded lattice is an element $\overline{a}$ such that
\[
a \land \overline{a} = 0, \qquad
a \lor \overline{a} = 1.
\]
#+end_definition

#+attr_latex: :options [The complement in distributive lattices is unique]
#+begin_proposition
If a complement of an element exists in a distributive lattice, it is unique.
#+end_proposition
# TODO: Commutativity is needed on this proof, but it is not stated elsewhere
#+begin_proof
Given $a$ with two complements $x,y$, we have that
\[
x = x \land (a \lor y) = (x \land a) \lor (x \land y) =
(y \land a) \lor (x \land y) = y \lor (x \land a) = y.
\]
#+end_proof

**** Boolean algebras                                             :ignore:
#+attr_latex: :options [Boolean algebra]
#+begin_definition
A *Boolean algebra* is a distributive bounded lattice in which every element
has a complement.
#+end_definition

Boolean algebras satisfy certain known properties such as the DeMorgan laws
and the double negation elimination rule.

# TODO: Every Boolean algebra is the algebra of subsets of some set.
# This is a result by M.H.Stone, cited in pag 50 Moerdijk.

*** Heyting algebras
**** Definition of Heyting algebras                               :ignore:
#+attr_latex: :options [Heyting algebra]
#+begin_definition
A *Heyting algebra*, also called *Brouwerian lattice*, is a bounded lattice
which is cartesian closed as a category; i.e., for every pair of elements
$x,y$, the exponential $y^x$ exists.
#+end_definition

The exponential in Heyting algebras is usually written as $x \Rightarrow y$ and is 
characterized by its adjunction with the product
\[
z \leq (x \Rightarrow y) \text{ if and only if } z \land x \leq y,
\]
which can be expressed logically as
\begin{prooftree}
\AXC{$z \land x \leq y$}
\UIC{$z \leq x \Rightarrow y$}
\end{prooftree}

#+ATTR_LATEX: :options [Heyting algebra homomorphism]
#+BEGIN_definition
A *Heyting algebra homomorphism* is a lattice homomorphism between
Heyting algebras which does preserve implication. The category of
Heyting algebras is written as $\mathtt{Heyt}$.
#+END_definition

# TODO: Any complete distributive lattice is a Heyting algebra

**** Boolean algebras are Heyting algebras                        :ignore:
#+attr_latex: :options [Boolean algebras are Heyting algebras]
#+begin_proposition
Every Boolean algebra is a Heyting algebra with exponentials given by
\[
(x \Rightarrow y) = \overline{x} \land y.
\]
#+end_proposition
#+begin_proof
We will prove that
\[
z \leq (\overline{x} \lor y) \text{ if and only if } z \land x \leq y.
\]
If $z \leq (\overline{x} \lor y)$,
\[
z \land x \leq
(\overline{x} \lor y) \land x \leq
(\overline{x} \land x) \lor (y \land x) \leq
y \land x \leq
y;
\]
and if $z \land x \leq y$,
\[
z = 
z \land 1 =
z \land (\overline{x} \lor x) =
(z \land \overline{x}) \lor (z \land x) \leq 
(z \land \overline{x}) \lor y \leq
z \lor y.
\]
#+end_proof

**** Negation on Heyting algebras                                 :ignore:
#+attr_latex: :options [Negation]
#+begin_definition
The *negation* of $x$ in a Heyting algebra is defined as
\[
\neg x = (x \Rightarrow 0).
\]
#+end_definition

In general, we only have that $\neg\neg x \leq x$. An element for which
$\neg\neg x = x$ is called a *regular* element.

# TODO: Regular elements form a Boolean algebra.

# TODO: Frames and Heyting algebras. What are the regular elements?

**** Properties of a Heyting algebra                              :ignore:
#+begin_proposition
In any Heyting algebra,

  1) $x \leq \neg \neg x$,
  2) $x \leq y$ implies $\neg y \leq \neg x$,
  3) $\neg x = \neg\neg\neg x$,
  4) $\neg\neg (x \land y) = \neg \neg x \land \neg \neg y$,
  5) $(x \impl x) = 1$,
  6) $x \land (x \impl y) = x \land y$,
  7) $y \land (x \impl y) = y$,
  8) $x \impl (y \land z) = (x \impl y) \land (x \impl z)$.

Any bounded lattice $L$ with an operation satisfying the last four properties
is a Heyting algebra with this operation as implication.
#+end_proposition
#+begin_proof
We can prove the inequalities using the definition of implication.

  1) By definition, $x \land (x \Rightarrow \bot) \leq \bot$.
  2) Again, by definition, $\neg y \land x \leq \neg y \land y \leq \bot$.
  3) Is a consequence of the first two inequalities.
  4) We know that $x \land y \leq x,y$, and therefore $\neg\neg (x\land y) \leq \neg \neg x \land \neg \neg y$.
     We can prove $\neg\neg x \land \neg\neg y \leq \neg\neg (x \land y)$ using the definition of
     negation to get $\neg\neg x \land \neg\neg y \land \neg (x \land y) \leq \bot$, and then by reversing
     the definition of implication $\neg\neg y \land \neg (x \land y) \leq \neg\neg\neg x = x$. Applying
     the same reasoning to $y$, we finally get $x \land y \land \neg (x \land y) \leq \bot$.
  5) Follows from $x \land 1 \leq x$.
  6) Using the evaluation morphism, we know that $x \land (x \impl y) \leq y \leq x \land y$.
  7) Using the definition of implication $y = y \land y \leq y \land (x \Rightarrow y)$.
  8) The exponential $x \Rightarrow -$ is a right adjoint and it preserves products.
#+end_proof

**** Characterization of Boolean algebras                         :ignore:
#+attr_latex: :options [Complements are negations in Heyting algebras]
#+begin_proposition
If an element has a complement on a Heyting algebra, it must be $\neg x$.
#+end_proposition
#+begin_proof
Let $a$ a complement of $x$. By definition, $x \land a = \bot$ and therefore $a \leq \neg x$.
The reverse inequality can be proven using the lattice properties as
\[
\neg x = \neg x \land (x \lor a) = \neg x \land a.
\]
#+end_proof

#+attr_latex: :options [Characterization of Boolean algebras]
#+begin_proposition
A Heyting algebra is Boolean if and only if $\neg\neg x = x$ for every $x$; 
and if and only if $x \lor \neg x = 1$ for every $x$.
#+end_proposition
#+begin_proof
In a Boolean algebra the complement is unique and $\neg\neg x = x$. Now, 
if $\neg\neg y = y$ for every $y$,
\[
x \lor \neg x = \neg\neg (x \lor \neg x) = \neg (\neg x \land \neg\neg x) = \top;
\]
and then, as $x \lor \neg x = \bot$, $\neg x$ must be the complement of $x$. We have
used the fact that $\neg (x \lor y) = (\neg x) \land (\neg y)$ in any Heyting algebra.
#+end_proof

*** Intuitionistic propositional calculus
#+ATTR_LATEX: :options [Existence of free Heyting algebras]
#+BEGIN_proposition
# TODO
#+END_proposition

#+ATTR_LATEX: :options [Intuitionistic propositional calculus]
#+BEGIN_definition
The *Intuitionistic Propositional Calculus* (IPC) is the free Heyting
algebra over an infinite countable set $\{p_0,p_1,p_2,\dots\}$ of elements 
which are usually called /atomic propositions/.
#+END_definition

**** Classical propositional calculus
#+ATTR_LATEX: :options [Classical propositional calculus]
#+BEGIN_definition
The *Classical Propositional Calculus* (CPC) is the free Boolean
algebra over an infinite countable set $\{p_0,p_1,p_2,\dots\}$.
#+END_definition

# TODO: Law of excluded middle and reductio ad absurdum.

*** Quantifiers as adjoints
#+begin_definition
Given a relation between sets $S \subseteq X \times Y$, the functors $\forall_p, \exists_p \colon {\cal P}(X \times Y) \to {\cal P}(Y)$
are defined as

  * $\forall_p S = \left\{ y \mid \forall x: \pair{x,y} \in S \right\}$, and
  * $\exists_p S = \left\{ y \mid \exists x: \pair{x,y} \in S \right\}$.
#+end_definition

#+begin_theorem
The functors $\exists_p,\forall_p$ are the left and right adjoints to the inverse image of the
projection functor, $p^{\ast} \colon {\cal P}(Y) \to {\cal P}(X \times Y)$.
#+end_theorem
#+begin_proof
# TODO: Proof
#+end_proof

#+begin_theorem
Given any function on sets $f \colon Z \to Y$, the inverse image functor $f^{\ast} \colon {\cal P}Y \to {\cal P}Z$
has left and right adjoints, called $\exists_f$ and $\forall_f$.
#+end_theorem

** Simply-typed \lambda-theories
*** Simply-typed \lambda-theories
#+ATTR_LATEX: :options [Simply typed lambda theory]
#+BEGIN_definition
A simply-typed \lambda-theory is given by

  * a set of /basic types/.
  * a set of /basic constants/ with their types.
  * a set of /equations/ on those terms, of the form $\Gamma \mid u = t : A$,
    where $\Gamma$ is the variable context.
#+END_definition

# TODO Example: Groups
# TODO Any algebraic theory is a lambda theory
# TODO Theory of a reflexive type

*** Interpretation of \lambda-theories
#+ATTR_LATEX: :options [Interpretation of a lambda theory]
#+BEGIN_definition
An *interpretation* of a \lambda-calculus $\mathbb{T}$ in a cartesian
closed category ${\cal C}$ is given by

  1. an object $\intr{A} \in \mathbb{C}$ for every basic type $A$ in $\mathbb{T}$.
  2. a morphism $\intr{c} \colon 1 \to \intr{A}$ for every constant $c : A$.

The interpretation can be extended to all types using the terminal
object, binary products and exponentials

 * $\intr{1} = 1$;
 * $\intr{A \times B} = \intr{A} \times \intr{B}$;
 * $\intr{A \impl B} = \intr{B}^{\intr{A}}$.

Every context $\Gamma = x_1:A_1, \dots,x_n:A_n$ can be interpreted as the
object $$
#+END_definition

*** Syntactic categories
#+ATTR_LATEX: :options [Syntactic category]
#+BEGIN_definition
The *syntactic category* of a \lambda-theory, ${\cal S}(\mathbb{T})$, is given by

 * the types of the theory as objects; and
 * the terms in context $x : A \mid t :B$

#+END_definition

#+ATTR_LATEX: :options [Syntactic categories are cartesian closed]
#+BEGIN_proposition
The syntactic category of a \lambda-theory is cartesian closed.
#+END_proposition
#+BEGIN_proof
# TODO
#+END_proof


#+ATTR_LATEX: :options [Model of a \lambda-theory]
#+BEGIN_definition
A *model* of a \lambda-theory $\mathbb{T}$ is a functor $M \colon {\cal S}(\mathbb{T}) \to {\cal C}$ preserving
finite products and exponentials.
#+END_definition

# TODO: Canonical interpretation

*** Translations, category of lambda-theories
#+ATTR_LATEX: :options [Translation]
#+BEGIN_definition
A *translation* between \lambda-theories $\tau \colon \mathbb{T} \to \mathbb{U}$ is a model of $\mathbb{T}$
in the syntactic category ${\cal S}(U)$.
#+END_definition

#+ATTR_LATEX: :options [Translation]
#+BEGIN_definition
A *translation* between \lambda-theories $\tau \colon \mathbb{T} \to \mathbb{U}$ is given by

  1. a type $\tau A$ in $\mathbb{U}$ for each type $A$ in $\mathbb{T}$; in such a way that
     \[
     \tau 1 = 1,\qquad
     \tau(A \times B) = \tau A \times \tau B,\qquad
     \tau(A \to B) = \tau A \to \tau B.
     \]

  2. a term $\tau c : \tau A$ in $\mathbb{T}$ for each term $c : A$ in $\mathbb{T}$; in such a way that
     \[\begin{aligned}
     \tau(\texttt{fst}\ t) = \texttt{fst}\ (\tau t), &&
     \tau(\texttt{snd}\ t) = \texttt{snd}\ (\tau t), &&
     \tau\pair{u,v} = \pair{\tau u, \tau v}, \\
     \tau (tu) = (\tau t)(\tau u), &&
     \tau (\lambda x:A. t) = \lambda x:\tau A. \tau t,
     \end{aligned}\]

A translation is also required to preserve all equations. That is, if
$\Gamma \mid t = u : A$ can be proved in $\mathbb{T}$, $\tau \Gamma \mid \tau t = \tau u : \tau A$ should be provable
in $\mathbb{U}$.
#+END_definition


#+ATTR_LATEX: :options [Category of lambda-theories]
#+BEGIN_definition
The category $\mathtt{\lambda Thr}$ has \lambda-theories as objects and translations
between them as morphisms.
#+END_definition

# TODO: Notion of isomorphism of types
# TODO: Equivalence of theories

*** Internal language of a category
#+ATTR_LATEX: :options [Internal language]
#+BEGIN_definition
The *internal language* of a small cartesian closed category ${\cal C}$
is a \lambda-theory given by

 1. a /basic type/ $\intl{A }$ for every object $A \in {\cal C}$;

 2. a /constant/ $\intl{f} \colon \intl{A} \to \intl{B}$ for every morphism $f : A \to B$;

 3. the /identity/ axiom 
    \[
    x : \intl{A} \mid \intl{\id}\ x = x : \intl{A}
    \] 
    for every $A$;

 4. the /composition/ axiom
    \[
    x : \intl{A} \mid \intl{g \circ f}\ x = \intl{g}(\intl{f} x) : \intl{C}
    \]
    for every pair of composable morphisms $f : A \to B$ 
    and $g : B \to C$;

 5. and /constants/
    \[\begin{aligned}
    \texttt{T} &: \texttt{1} \to \intl{1} \\
    \texttt{P}_{A,B} &: \intl{A} \times \intl{B} \to \intl{A \times B} \\
    \texttt{E}_{A,B} &: (\intl{A} \to \intl{B}) \to \intl{B^{A}}
    \end{aligned}\]
    for any $A,B \in {\cal C}$.

Satisfying
# TODO
#+END_definition

**** TODO Category of CCCs and category of lambdaTheories
**** TODO Equivalence of CC categories
*** TODO Bicartesian closed categories
** TODO First-order logic
** TODO Models of \lambda-calculus
# Parece que esta sección es innecesaria después de todo lo
# anterior.

*** TODO Types and sets
# From the Hott book
*** TODO Models
# From Selinger
*** TODO Continuous partial orderings
# From Selinger
** Topoi
*** Subobject classifier
#+attr_latex: :options [Subobject classifier]
#+begin_definition
A *subobject classifier* is an object $\Omega$ with a monomorphism $\mathrm{true} \colon 1 \to \Omega$
such that, for every monomorphism $S \to X$, there exists a unique $\phi$ such that
\[\begin{tikzcd}
S\rar{} \dar[swap]{} & 1 \dar{\mathrm{true}} \\
X\rar[dashed]{\phi} & \Omega
\end{tikzcd}\]
forms a pullback square.
#+end_definition

*** Definition of a topos
#+attr_latex: :options [Topos]
#+begin_definition
An *elementary topos* (plural /topoi/) is a cartesian closed category
with all finite limits and a subobject classifier.
#+end_definition
* Type theory
# to conduct my thoughts in an orderly fashion, by commencing with those
# objects that are simplest and easiest to know, in order to ascend
# little by little, as by degrees, to the knowledge of the most
# composite things, and by supposing an order even among those things
# that do not naturally precede one another.

** Intuitionistic logic
*** Constructive mathematics
*** Agda example
In intuitionistic logic, the double negation of the LEM holds for every
proposition, that is,

\[
\forall A\colon \neg \neg (A \lor \neg A)
\]

**** Machine proof
#+latex: \ExecuteMetaData[latex/Ctlc.tex]{id}

** TODO Martin-Löf Type Theory, dependent types
*** Type in type
# Russell's paradox code
# Girard paradox in agda: https://github.com/nzl-nott/PhD-of-nzl/blob/master/Exercise/Ex4/girard.agda
# Hurkens' paradox: https://github.com/agda/agda/blob/master/test/Succeed/Hurkens.agda
# Hurkens: https://www.cs.cmu.edu/~kw/scans/hurkens95tlca.pdf
# Burali-Forti paradox

** TODO Type theory as a foundation of mathematics
** TODO Constructive mathematics
*** TODO Axiom of choice implies excluded middle
# In Agda or Coq!?
** TODO Homotopy Type Theory
* Conclusions
* Appendices
bibliographystyle:alpha
bibliography:Bibliography.bib

** TODO A note on structural induction
http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.184.4173
** TODO Mikrokosmos complete code
** TODO Mikrokosmos user's guide
** TODO Quotes
# These quotes can be placed on the partial abstracts or in the general one.
