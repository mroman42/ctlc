#+TITLE: Category Theory and Lambda Calculus
#+AUTHOR: Mario Román
#+OPTIONS: broken-links:mark toc:t tasks:nil num:3
#+SETUPFILE: ctlc.setup
#+LATEX_HEADER_EXTRA: %\input{titlepage}

# Evitar notas al pie de página
# Quitar abreviaturas
# Usar 1 y 0 para objetos inicial y final

** Abstract
:PROPERTIES:
:UNNUMBERED: t
:END:

This is the abstract.
# There is no claim of originality (?)

** Acknowlegments                                                 :noexport:
# David Charte and Ignacio Cordón, testing the first versions of
# Mikrokosmos.

# Alejandro García, Adrián Ranea and David Charte, template usage.

This document has been written with Emacs26 and org-mode 9, using the
=org= file format and LaTeX as intermediate format. The document
follows the =classicthesis= [[http://www.latextemplates.com/templates/theses/2/thesis_2.pdf][template]] by *André Miede*. The =minted=
package has been used for code listings and the =tikzcd= package has
been used for commutative diagrams.

* Lambda calculus (abstract)                                         :ignore:
#+LATEX: \ctparttext{\color{black}\begin{center}
The \lambda-calculus is a collection of systems formalizing the notion
of functions. They can be seen as programming languages and formal
logics at the same time. We focus on the properties of the untyped
\lambda-calculus and simply typed \lambda-calculus and its relation to
logic.
#+LATEX: \end{center}}

* Lambda calculus
** Untyped \lambda-calculus
When are two functions equal? Classically in mathematics, /functions are graphs/.
A function from a
domain to a codomain, $f \colon X \to Y$, is seen as a subset of the product
space: $f \subset X \times Y$.
Any two functions are identical if they map equal inputs to equal outputs;
and a function it is completely determined by what its outputs are.
This vision is called */extensional/*.

From a computational point of view, this perspective could seem incomplete
in some cases. Computationally, we usually care not only about the result but,
crucially, about /how/ it can be computed. 
Classically in computer science, /functions are formulae/; and two functions 
mapping equal inputs to equal outputs need not to be equal. For instance, two sorting algorithms
can have a different efficiency or different memory requisites, even if 
they output the same sorted list. This vision, where two functions are
equal if and only if they are given by essentially the same formula is
called */intensional/*.

The *\lambda-calculus* is a collection of formal systems, all of them
based on the lambda notation introduced by Alonzo Church in the 1930s
while trying to develop a foundational notion of functions (/as formulae/)
on mathematics. It is a logical theory of functions, where application and
abstraction are primitive notions; and, at the same time, it
is also one of the simplest programming languages, in which many other
full-fledged languages are based, as we will explain in detail later.

The *untyped* or *pure \lambda-calculus* is, syntactically, the
simplest of those formal systems. In it, a function does not need a
domain nor a codomain; every function is a formula that can be
directly applied to any expression. It even allows functions to be
applied to themselves, a notion that would be troublesome
in our usual set-theoretical foundations. In particular, if $f$ is a member of its own
domain, the infinite descending sequence
\[
f \ni \{f,f(f)\} \ni f \ni \{f,f(f)\} \ni \dots,
\]
would exist, thus contradicting the *regularity axiom* of Zermelo-Fraenkel
set theory (see, for example, cite:kunen11).
However, untyped \lambda-calculus presents some problems such as non-terminating
functions.

This presentation of the untyped lambda calculus will follow
cite:Hindley08 and cite:selinger13.

*** The untyped \lambda-calculus
As a formal language, the untyped \lambda-calculus is given by a set of
equations between expressions called /\lambda-terms/, and equivalences
between them can be computed using some manipulation rules.
These \lambda-terms can stand for functions or arguments indistinctly:
they all use the same \lambda-notation in order to define function
abstractions and applications.

The *\lambda-notation* allows a function to be written and inlined as any other element
of the language, identifying it with the formula it represents and 
admitting a more compact notation. For example, the polynomial function
$p(x) = x^2 + x$
would be written in \lambda-calculus as
$\lambda x.\ x^2 + x$; and $p(2)$ would be written as $(\lambda x.\ x^2+x)(2)$. In general,
$\lambda x.M$ is a function taking $x$ as an argument and returning $M$,
which is a term where $x$ may appear in.

The use of \lambda-notation also eases the writing of
*higher-order functions*, functions whose arguments or outputs are
functions themselves. For instance,
\[
\lambda f.(\ \lambda y.\ f(f(y))\ )
\]
would be a function taking $f$ as an argument and returning $\lambda y.\ f(f(y))$,
which is itself a function; most commonly written as $f \circ f$. In particular,
\[
\Big( \big( \lambda f.(\ \lambda y.\ f(f(y))\ ) \big)
\big( \lambda x.\ x^2 + x \big) \Big) (1)
\]
evaluates to $6$.

# Darle la vuelta a la frase
#+attr_latex: :options [Lambda terms]
#+begin_definition
<<def-lambdaterms>>
*\lambda-terms* are constructed inductively using the following rules

  * every */variable/*, taken from an infinite countable set of
    variables and usually written as lowercase single letters
    $(x, y, z, \dots)$, is a \lambda-term;

  * given two \lambda-terms $M,N$; its */application/*, $MN$, is a \lambda-term;

  * given a \lambda-term $M$ and a variable $x$, its */abstraction/*, $\lambda x.M$,
    is a \lambda-term;

  * every possible \lambda-term can be constructed using these rules, no other
    \lambda-term exists.

Equivalently, they can also be defined by the following Backus-Naur form,
\[
\mathtt{Term} ::= x \mid (\mathtt{Term}\ \mathtt{Term}) \mid (\lambda x.\mathtt{Term})\quad,
\]
where $x$ can be any variable.
#+end_definition

By convention, we omit outermost parentheses and assume
left-associativity, for example, $MNP$ will always mean $(MN)P$. Note
that the application of \lambda-terms is different from its
composition, which is distributive and will be formally defined later. We
consider \lambda-abstraction as having the lowest precedence. For
example, $\lambda x. M N$ should be read as $\lambda x.(MN)$ instead
of $(\lambda x.M) N$.

# NOTE: This is not necessary.
# Multiple \lambda-abstractions can be also contracted to a single 
# multivariate abstraction; thus $\lambda x.\lambda y.M$ can 
# become $\lambda x,y.M$.

*** Free and bound variables, substitution
In \lambda-calculus, the scope of a variable restricts to the \lambda-abstraction
where it appeared, if any. Thus, the same variable can be used multiple
times on the same term independently. For example, in 
$(\lambda x.x)(\lambda x.x)$, the variable $x$ appears twice
with two different meanings.

Any ocurrence of a variable $x$ inside the /scope/ of a lambda is said
to be */bound/*; and any variable without bound ocurrences is said to be
*/free/*. Formally, we can define the set of free variables on a given
term as follows.

#+attr_latex: :options [Free variables]
#+begin_definition
<<def-freevariables>>
The *set of free variables* of a term $M$ is defined inductively as
\[\begin{aligned}
\freevars(x) &= \{x\}, \\
\freevars(MN) &= \freevars(M) \cup \freevars(N), \\
\freevars(\lambda x.M) &= \freevars(M) \setminus \{x\}.
\end{aligned}\]
#+end_definition

Evaluation in \lambda-calculus relies in the notion of */substitution/*.
Any free ocurrence of a variable can be substituted by a term, as we do
when we are evaluating terms. For instance, in the previous example, we
evaluated $(\lambda x.\ x^2+x)(2)$ by substituting $2$ in the place of $x$ inside $x^{2} + x$;
as in
\[\begin{tikzcd}
(\lambda x.\ x^2+x)(2) \rar{x \mapsto 2}
&
2^{2} + 2.
\end{tikzcd}\]
This, however, should be done avoiding the unintended binding which happens
when a variable is substituted inside the scope of a binder with the
same name, as in the following example: if we were to evaluate the expression
$(\lambda x.\ y x)(\lambda z.\ xz)$,
where $x$ appears two times (once bound and once free), we should substitute $y$ by $(\lambda z.xz)$
on $(\lambda x.yx)$ and $x$ (the free variable) would get tied to $x$ (the bounded variable)
\[\begin{tikzcd}
(\lambda y.\lambda x.yx)(\lambda z.\ xz) \ar{rr}{y \mapsto (\lambda z.xz)} && (\lambda x.(\lambda z.xz)x).
\end{tikzcd}\]

To avoid this, the bounded $x$ must be given a new name before the
substitution, which must be carried as
\[\begin{tikzcd}
(\lambda y. \lambda u.y u)(\lambda z.\ xz) \ar{rr}{y \mapsto (\lambda z.xz)} & & (\lambda u.(\lambda z.xz)u),
\end{tikzcd}\]
keeping the free character of $x$.

#+attr_latex: :options [Substitution on lambda terms]
#+begin_definition
The *substitution* of a variable $x$ by a term $N$ on $M$ is
written as $M[N/x]$ and is defined inductively as
\[\begin{aligned}
x[N/x] &\equiv N,\\
y[N/x] &\equiv y, & \text{ if } y \neq x,\\
(MP)[N/x] &\equiv (M[N/x])(P[N/x]),\\
(\lambda x.P)[N/x] &\equiv \lambda x.P,\\
(\lambda y.P)[N/x] &\equiv \lambda y.P[N/x] & \text{ if } y \notin \freevars(N), \\
(\lambda y.P)[N/x] &\equiv \lambda z.P[z/y][N/x] & \text{ if } y \in \freevars(N),
\end{aligned}\]

where, in the last clause, $z$ is a fresh unused variable.
#+end_definition

We could define a criterion for choosing exactly what this new
variable should be, or simply accept that our definition will not be
exactly well-defined, but only
/well-defined up to a change on the name of the variables/.
This equivalence relation will be defined 
formally on the next section. In practice, it is common to follow the 
/Barendregt's variable convention/, which simply assumes that bound 
variables have been renamed to be distinct.

*** \alpha-equivalence
In \lambda-terms, variables are only placeholders and its name, as we have
seen before, is not relevant. Two \lambda-terms whose only difference is
the naming of the variables are called \alpha-equivalent. For example,
\[
(\lambda x.\lambda y. x\ y) \quad\text{ is $\alpha$-equivalent to }\quad (\lambda f.\lambda x. f\ x).
\]

*\alpha-equivalence* formally captures the fact that the name of a bound
variable can be changed without changing the meaning of the term. This
idea appears recurrently on mathematics; for example, the renaming of variables of
integration or the variable on a limit are a examples of \alpha-equivalence.
\[
\int_0^1 x^2\ dx = \int_0^1 y^2\ dy;
\qquad
\lim_{x \to \infty} \frac{1}{x} = \lim_{y \to \infty} \frac{1}{y}.
\]

#+attr_latex: :options [\alpha-equivalence]
#+begin_definition
*\alpha-equivalence* is the smallest relation $=_{\alpha}$ on
\lambda-terms that is an equivalence relation, that is to say that

  * it is /reflexive/, $M =_{\alpha} M$;
  * it is /symmetric/, if $M =_{\alpha} N$, then $N =_{\alpha} M$;
  * and it is /transitive/, if $M=_{\alpha}N$ and $N=_{\alpha}P$, then $M=_{\alpha}P$;

and it is compatible with the structure of lambda terms,

  * if $M =_{\alpha} M'$ and $N =_{\alpha} N'$, then $MN =_{\alpha}M'N'$;
  * if $M=_{\alpha}M'$, then $\lambda x.M =_{\alpha} \lambda x.M'$;
  * if $y$ does not appear on $M$, $\lambda x.M =_{\alpha} \lambda y.M[y/x]$.
#+end_definition

*** \beta-reduction
The core idea of evaluation in \lambda-calculus is captured by the notion
of *\beta-reduction*.
Until now, evaluation has been only informally described; it is time
to define it as a relation, $\tto_{\beta}$, going from the initial term to
any of its partial evaluations. We
will firstly consider a /one-step reduction/ relationship, called
$\to_{\beta}$, which will be extended by transitivity to $\tto_{\beta}$.

Ideally, we would like to define evaluation as a series of reductions
into a canonical form which could not be further reduced.
Unfortunately, as we will see later, it is not possible to find, in
general, that canonical form.

#+attr_latex: :options [\beta-reduction]
#+begin_definition
<<def-betared>>
The *single-step \beta-reduction* is the smallest relation on \lambda-terms
capturing the notion of evaluation and preserving the structure of \lambda-abstractions
and applications. That is, the smallest relation containing

  * $(\lambda x.M)N \to_{\beta}M[N/x]$ for any terms $M,N$ and any variable $x$,
  * $MN \to_{\beta} M'N$ and $NM \to_{\beta} NM'$ for any $M,M'$ such that $M \to_{\beta} M'$, and
  * $\lambda x.M \to_{\beta} \lambda x.M'$, for any $M,M'$ such that $M \to_{\beta} M'$.

The reflexive transitive closure of $\to_{\beta}$ is written as $\tto_{\beta}$. The symmetric
closure of $\tto_{\beta}$ is called *\beta-equivalence* and written as $=_{\beta}$ or simply $=$.
#+end_definition

*** \eta-reduction
Although we lost the extensional view of functions when we decided to
adopt the /functions as formulae/ perspective, the idea of function
extensionality in \lambda-calculus can be partially recovered by the notion
of \eta-reduction.
This form of /function extensionality for \lambda-terms/ can be captured
by the notion that any term which simply applies a function to the
argument it takes can be reduced to the actual function. That is,
any $\lambda x.M x$ can be reduced to $M$.

#+attr_latex: :options [\eta-reduction]
#+begin_definition
The *\eta-reduction* is the smallest relation on \lambda-terms satisfiying the
same congruence rules as \beta-reduction and the following axiom
\[
\lambda x.Mx \to_{\eta} M,\text{ for any } x \notin \mathrm{FV}(M).
\]
We define single-step \beta\eta-reduction as the union of \beta-reduction
and \eta-reduction. This will be written as $\to_{\beta\eta}$, and its reflexive transitive
closure will be $\tto_{\beta\eta}$.
#+end_definition

Note that, in the particular case where $M$ is itself a \lambda-abstraction,
\eta-reduction is simply a particular case of \beta-reduction.

# TODO: Comments in https://cstheory.stackexchange.com/a/8261/28986
# suggest a theorem in Urzyczyn, Sorensen which might be relevant

*** Confluence
As we mentioned above, it is not possible in general to evaluate a \lambda-term
into a canonical, non-reducible term. However, we will be able to prove
that, in the cases where it exists, it is unique. This property
is a consequence of a sightly more general one, */confluence/*, which
can be defined in any abstract rewriting system.

#+attr_latex: :options [Confluence]
#+begin_definition
A relation $\to$ on a set ${\cal S}$ is *confluent* if, given its reflexive transitive closure
$\tto$, for any $M,N,P \in {\cal S}$,  $M \tto N$ and $M \tto P$ imply the existence of some
$Z \in {\cal S}$ such that $N \tto Z$ and $P \tto Z$.
#+end_definition

Given any binary relation $\to$ of which $\tto$ is its reflexive transitive
closure, we can consider three seemingly related properties

  * the *confluence* (also called /Church-Rosser property/) we have just defined,
  * the *quasidiamond property*, which assumes $M \to N$ and $M \to P$,
  * the *diamond property*, which is defined substituting $\tto$ by $\to$ on
    the definition on confluence.

Diagrammatically, the three properties can be represented as
\[\begin{tikzcd}[column sep=small]
& 
M \drar[two heads]\dlar[two heads] &&& 
M \drar\dlar &&& 
M \drar\dlar &\\
N \drar[dashed,two heads] && 
P \dlar[dashed,two heads] & 
N \drar[dashed,two heads] &&
P \dlar[dashed,two heads] &
N \drar[dashed] && 
P \dlar[dashed] \\& 
Z &&&
Z &&&
Z &\\
\end{tikzcd}\]
and we can show that the diamond relation implies confluence; while
the quasidiamond does not. Both claims are easy to prove, and they
show us that, in order to prove confluence for a given relation, we
can use the diamond property instead of the quasidiamond property.

The statement of $\tto_{\beta}$ and $\tto_{\beta\eta}$ being confluent is what we
call the */Church-Rosser Theorem/*. The definition of a relation satisfying
the diamond property and whose reflexive transitive closure is $\tto_{\beta\eta}$ will
be the core of our proof.

*** The Church-Rosser theorem
The proof presented here is due to Tait and Per Martin-Löf; an earlier
but more convoluted proof was discovered by Alonzo Church and Barkley 
Rosser in 1935 (see cite:barendregt84 and cite:pollack95).
It is based on the idea of parallel one-step reduction.

#+attr_latex: :options [Parallel one-step reduction]
#+begin_definition
We define the *parallel one-step reduction* relation on \lambda-terms, $\rhd$,
as the smallest relation satisfying that the following properties

  * reflexivity, $x \rhd x$;
  * parallel application, $PN \rhd P'N'$;
  * congruence to \lambda-abstraction, $\lambda x.N \rhd \lambda x.N'$;
  * parallel substitution, $(\lambda x.P)N \rhd P'[N'/x]$;
  * and extensionality, $\lambda x.P x \rhd P'$, if $x \not\in \mathrm{FV}(P)$,

hold for any variable $x$ and any terms $N,N',P,P'$ such that $P \rhd P'$ and $N \rhd N'$.
#+end_definition

Using the first three rules, it is trivial to show that this relation
is in fact reflexive.

#+begin_lemma
<<lemma-transclosureparallel>>
The reflexive transitive closure of $\rhd$ is $\tto_{\beta\eta}$.
In particular, given any \lambda-terms $M,M'$,

  1) if $M \to_{\beta\eta} M'$, then $M \rhd M'$.
  2) if $M \rhd M'$, then $M \tto_{\beta\eta} M'$;
#+end_lemma
#+begin_proof
  1) We can prove this by exhaustion and structural induction on
     \lambda-terms, the possible ways in which we arrive at $M \to M'$
     are

     * $(\lambda x.M)N \to M[N/x]$; where we know that, by parallel substitution
       and reflexivity $(\lambda x.M)N \rhd M[N/x]$;

     * $MN \to M'N$ and $NM \to NM'$; where we know that, by
       induction $M \rhd M'$, and by parallel application and reflexivity, $MN \rhd M'N$
       and $NM \rhd NM'$;

     * congruence to \lambda-abstraction, which is a shared property between
       the two relations where we can apply structural induction again;

     * $\lambda x. Px \to P$, where $x \not\in \mathrm{FV}(P)$ and we can apply extensionality for $\rhd$
       and reflexivity.

  2) We can prove this by induction on any derivation of $M \rhd M'$. The
     possible ways in which we arrive at this are
     
     * the trivial one, reflexivity;

     * parallel application $NP \rhd N'P'$, where, by induction, we have $P \tto P'$ 
       and $N \tto N'$. Using two steps, $NP \tto N'P \tto N'P'$ we prove $NP \tto N'P'$;

     * congruence to \lambda-abstraction $\lambda x.N \rhd \lambda x.N'$, where, by induction,
       we know that $N \tto N'$, so $\lambda x.N \tto \lambda x.N'$;

     * parallel substitution, $(\lambda x.P)N \rhd P'[N'/x]$, where, by induction,
       we know that $P \tto P'$ and $N\tto N'$. Using multiple steps,
       $(\lambda x.P)N \tto (\lambda x.P')N \tto (\lambda x.P')N' \to P'[N'/x]$;

     * extensionality, $\lambda x.P x \rhd P'$, where by induction $P \tto P'$, and trivially,
       $\lambda x.Px \tto \lambda x.P'x$.

Because of this, the reflexive transitive closure of $\rhd$ should be a subset and a
superset of $\tto$ at the same time.
#+end_proof

#+attr_latex: :options [Substitution Lemma]
#+begin_lemma
<<lemma-subsl>>
Assuming $M \rhd M'$ and $U \rhd U'$, $M[U/y] \rhd M'[U'/y]$.
#+end_lemma
#+begin_proof
We apply structural induction on derivations of $M \rhd M'$, depending
on what the last rule we used to derive it was.

  * Reflexivity, $M = x$. If $x=y$, we simply use $U \rhd U'$; if $x \neq y$,
    we use reflexivity on $x$ to get $x \rhd x$.

  * Parallel application. By induction hypothesis, $P[U/y] \rhd P'[U'/y]$ and
    $N[U/y]\rhd N'[U'/y]$, hence $(PN)[U/y] \rhd (P'N')[U'/y]$.

  * Congruence. By induction, $N[U/y] \rhd N'[U'/y]$ and $\lambda x.N[U/y] \rhd \lambda x.N'[U'/y]$.

  * Parallel substitution. By induction, $P[U/y] \rhd P'[U'/y]$ and $N[U/y] \rhd N[U'/y]$,
    hence $((\lambda x.P)N)[U/y] \rhd P'[U'/y][N'[U'/y]/x] = P'[N'/x][U'/y]$.

  * Extensionality, given $x \notin \mathrm{FV}(P)$. By induction, $P \rhd P'$, hence
    $\lambda x.P[U/y]x \rhd P'[U'/y]$.

Note that we are implicitely assuming the Barendregt's variable convention; all
variables have been renamed to avoid clashes.
#+end_proof

#+attr_latex: :options [Maximal parallel one-step reduct]
#+begin_definition 
The *maximal parallel one-step reduct* $M^{\ast}$ of a \lambda-term $M$ is defined
inductively as

  * $x^{\ast} = x$, if $x$ is a variable;
  * $(PN)^{\ast} = P^{\ast}N^{\ast}$;
  * $((\lambda x.P)N)^{\ast} = P^{\ast}[N^{\ast}/x]$;
  * $(\lambda x.N)^{\ast} = \lambda x.N^{\ast}$;
  * $(\lambda x.Px)^{\ast} = P^{\ast}$, given $x \notin \mathrm{FV}(P)$.
#+end_definition

#+attr_latex: :options [Diamond property of parallel reduction]
#+begin_lemma
<<lemma-paralleldiamond>>
Given any $M'$ such that $M \rhd M'$, $M' \rhd M^{\ast}$. Parallel one-step reduction 
has the diamond property.
#+end_lemma
#+begin_proof
We apply again structural induction on the derivation of $M \rhd M'$.

  * Reflexivity gives us $M' = x = M^{\ast}$.

  * Parallel application. By induction, we have $P \rhd P^\ast$ and $N \rhd N^{\ast}$; depending
    on the form of $P$, we have

    - $P$ is not a \lambda-abstraction and $P'N' \rhd P^{\ast}N^{\ast} = (PN)^{\ast}$.

    - $P = \lambda x.Q$ and $P \rhd P'$ could be derived using congruence to \lambda-abstraction
      or extensionality. On the first case we know by induction hypothesis that $Q'\rhd Q^{\ast}$
      and $(\lambda x.Q')N' \rhd Q^{\ast}[N^{\ast}/x]$. On the second case, we can take $P = \lambda x.Rx$, where,
      $R \rhd R'$. By induction, $(R'x) \rhd (Rx)^{\ast}$ and now we apply the substitution lemma
      to have $R'N' = (R'x)[N'/x] \rhd (Rx)^{\ast}[N^{\ast}/x]$.

  * Congruence. Given $N \rhd N'$; by induction $N' \rhd N^{\ast}$, and depending on the form of
    $N$ we have two cases

    - $N$ is not of the form $Px$ where $x \not\in \mathrm{FV}(P)$; we can apply congruence to 
      \lambda-abstraction.

    - $N = Px$ where $x \notin \mathrm{FV}(P)$; and $N \rhd N'$ could be derived by parallel application
      or parallel substitution. On the first case, given $P \rhd P'$, we know that $P' \rhd P^{\ast}$
      by induction hypothesis and $\lambda x.P'x \rhd P^{\ast}$ by extensionality. On the second case,
      $N = (\lambda y.Q)x$ and $N' = Q'[x/y]$, where $Q \rhd Q'$. Hence $P \rhd \lambda y.Q'$, and by
      induction hypothesis, $\lambda y.Q' \rhd P^{\ast}$.

  * Parallel substitution, with $N \rhd N'$ and $Q \rhd Q'$; we know that $M^{\ast} = Q^{\ast}[N^{\ast}/x]$
    and we can apply the substitution lemma (lemma [[lemma-subsl]]) to get $M' \rhd M^{\ast}$.

  * Extensionality. We know that $P \rhd P'$ and $x \notin \mathrm{FV}(P)$, so by induction hypothesis
    we know that $P' \rhd P^{\ast} = M^{\ast}$.$\qedhere$
#+end_proof

#+attr_latex: :options [Church-Rosser Theorem]
#+begin_theorem
<<theorem-churchrosser>>
The relation $\tto_{\beta\eta}$ is confluent.
#+end_theorem
#+begin_proof
Parallel reduction, $\rhd$, satisfies the diamond property (lemma [[lemma-paralleldiamond]]), 
which implies the Church-Rosser property. Its reflexive transitive closure is $\tto_{\beta\eta}$
(lemma [[lemma-transclosureparallel]]),
whose diamond property implies confluence for $\to_{\beta\eta}$.
#+end_proof

*** Normalization
Once the Church-Rosser theorem is proved, we can formally define the notion
of a normal form as a completely reduced \lambda-term.

#+attr_latex: :options [Normal forms]
#+begin_definition
A \lambda-term is said to be in *\beta-normal form* if \beta-reduction
cannot be applied to it or any of its subformulas. We define *\eta-normal forms*
and *\beta\eta-normal forms* analogously.
#+end_definition

Fully evaluating \lambda-terms usually means to apply reductions to
them until a normal form is reached. We know, by virtue of Theorem
[[theorem-churchrosser]], that, if a normal form for a particular term
exists, it is unique; but we do not know whether a normal form
actually exists. We say that a term *has* a normal form when it can be
reduced to a normal form.

#+begin_definition
A term is *weakly normalizing* if there exists a sequence of reductions
from it to a normal form. It is *strongly* normalizing if every sequence
of reductions is finite.
#+end_definition

A consequence of Theorem [[theorem-churchrosser]] is that a weakly normalizing
term has a unique normal form. Strong normalization implies weak normalization,
but the converse is not true; as an example, the term
\[
\Omega = (\lambda x.(x x))(\lambda x.(x x))
\]
is neither weakly nor strongly normalizing; and the term
$(\lambda x.\lambda y.y)\ \Omega\ (\lambda x.x)$
is weakly but not strongly normalizing. It can be reduced to a normal form as
\[
(\lambda x.\lambda y.y)\ \Omega\ (\lambda x.x) \longrightarrow_{\beta} (\lambda x.x).
\]

*** Standarization and evaluation strategies
# Barendregt, 1985, section 13.2

# Leftmost vs Rightmost evaluation
# Leftmost does always normalize if it is possible
# Rightmost only normalizes if it is necessary

# https://cs.stackexchange.com/questions/7702/applicative-order-and-normal-order-in-lambda-calculus
# This case illustrates a more general phenomenon: applicative order
# reduction only ever finds a normal form if the term is strongly
# normalizing, whereas normal order reduction always finds the normal
# form if there is one. This happens because applicative order always
# evaluates fully arguments first, and so misses the opportunity for
# an argument to turn out to be unused; whereas normal order evaluates
# arguments as late as possible, and so always wins if the argument
# turns out to be unused.

# Statement: http://www.nyu.edu/projects/barker/Lambda/barendregt.94.pdf
# Barendregt (1984) Theorem 13.2.2
We would like to find a \beta-reduction strategy such that, if a term
has a normal form, it can be found by following that strategy. Our
basic result will be the *standarization theorem*, which shows that,
if a \beta-reduction to a normal form exists, then a sequence of
\beta-reductions from left to right on the \lambda-expression will be
able to find it. From this result, we will be able to prove that the
reduction strategy that always reduces the leftmost \beta-abstraction
will always find a normal form if it exists.

This section follows cite:kashima00, cite:barendsen94 and cite:barendregt84.

#+attr_latex: :options [Leftmost one-step reduction]
#+begin_definition
We define the relation $M \to_{n} N$ when $N$ can be obtained by \beta-reducing
the $n\text{-th}$ leftmost \beta-reducible application of the expression.
We call $\to_{1}$ the *leftmost one-step reduction* and we write it as $\to_{l}$;
accordingly, $\tto_{l}$ is its reflexive transitive closure.
#+end_definition

#+attr_latex: :options [Standard sequence]
#+begin_definition
A sequence of \beta-reductions $M_0 \to_{n_1} M_1 \to_{n_2} M_2 \to_{n_3} \dots \to_{n_k} M_{k}$ 
is *standard* if $\{n_i\}$ is a non-decreasing sequence.
#+end_definition

We will prove that every term that can be reduced to a normal form can
be reduced to it using a standard sequence, from this result, the existence
of an optimal beta reduction strategy, in the sense that it will always reach
a normal form if one exists, will follow as a corollary.

#+attr_latex: :options [Standarization theorem]
#+begin_theorem
<<thm-standarization>>
If $M \tto_{\beta} N$, there exists a standard sequence from $M$ to $N$.
#+end_theorem
#+begin_proof
We start by defining the following two binary relations. The first one
is the minimal reflexive transitive relation on \lambda-terms
capturing a form of \beta-reduction called /head \beta-reduction/;
that is, it is the minimal relation $\tto_h$ such that

  * $A \tto_h A$,
  * $(\lambda x.A_0)A_1A_2 \dots A_m \tto_{h} A_0[A_1/x]A_2 \dots A_m$, for any term of the form $A_1A_2\dots A_n$, and
  * $A \tto_{h} C$ for any terms $A,B,C$ such that $A \tto_{h} B \tto_{h} C$.

The second one is called /standard reduction/. It is the minimal relation
between \lambda-terms such that

  * $M \tto_h x$ implies $M \tto_s x$, for any variable $x$,
  * $M \tto_h AB$, $A \tto_s C$ and $B \tto_s D$, imply $M \tto_s CD$,
  * $M \tto_h \lambda x.A$ and $A \tto_s B$ imply $M \to_s \lambda x.B$.

We can check the following trivial properties by structural induction

  1) $\tto_h$ implies $\tto_{l}$,
  2) $\tto_{s}$ implies the existence of a standard \beta-reduction,
  3) $\tto_{s}$ is reflexive, by induction on the structure of a term,
  4) if $M \tto_{h} N$, then $MP \tto_{h} NP$,
  5) if $M \tto_h N \tto_s P$, then $M \tto_{s} P$,
  6) if $M \tto_h N$, then $M[P/x] \tto_h N[P/x]$,
  7) if $M \tto_s N$ and $P \tto_s Q$, then $M[P/z] \tto_{s} N[Q/z]$.

And now we can prove that $K \tto_{s} (\lambda x.M)N$ implies $K \tto_s M[N/x]$.
From the fact that $K \tto_s (\lambda x.M)N$, we know that there must exist $P$ and $Q$ such
that $K \tto_h PQ$, $P \tto_s \lambda x.M$ and $Q \tto_s N$; and from $P \tto_s \lambda x.M$, we know
that there exists $W$ such that $P \tto_h \lambda x.W$ and $W \tto_s M$. From all this information,
we can conclude that
\[
K \tto_h PQ \tto_{h} (\lambda x.W)Q \tto W[Q/x] \tto_s M[N/x];
\]
which, by (3.), implies $K \tto_s M[N/x]$.

We finally prove that, if $K \tto_s M \to_{\beta} N$, then $K \tto_s N$. This proves the theorem,
as every \beta-reduction $M \tto_s M \tto_\beta N$ implies $M \tto_s N$. We analize the possible
ways in which $M \to_{\beta} N$ can be derived.

  1) If $K \tto_{s} (\lambda x.M)N \to_{\beta} M[N/x]$, it has been
     already showed that $K \tto_s M[N/x]$.
  2) If $K \tto_s MN \to_{\beta} M'N$ with $M \to_{\beta} M'$, we know that there exist $K \tto_h WQ$ 
     such that $W \tto_s M$ and $Q \tto_s N$; by induction $W \tto_s M'$, and then $WQ \tto_s M'N$.
     The case $K \tto_s MN \to_{\beta} MN'$ is entirely analogous.
  3) If $K \tto_s \lambda x.M \to_{\beta} \lambda x.M'$, with $M \to_{\beta} M'$, we know that there exists $W$ such
     that $K \tto_h \lambda x.W$ and $W \tto_s M$. By induction $W \tto_s M'$, and $K \tto_s \lambda x.M'$.$\qedhere$
#+end_proof

#+attr_latex: :options [Leftmost reduction theorem]
#+begin_corollary
<<cor-leftmosttheorem>>
We define the *leftmost reduction strategy* as the strategy that
reduces the leftmost \beta-reducible application at each step. If $M$ has a
normal form, the leftmost reduction strategy will lead to it.
#+end_corollary
#+begin_proof
Note that, if $M \to_n N$, where $N$ is in \beta-normal form; $n$ must be exactly
$1$. If $M$ has a normal form and $M \tto_{\beta} N$, by Theorem [[thm-standarization]],
there must exist a standard sequence from $M$ to $N$ whose last step is of the
form $\to_{l}$; as the sequence is non-decreasing, every step has to be of the form $\to_{l}$.
#+end_proof

*** SKI combinators
**** SKI definition                                               :ignore:
As we have seen in previous sections, untyped \lambda-calculus is already
a very syntactically simple system; but it can be further reduced to
a few \lambda-terms without losing its expressiveness. In particular, untyped
\lambda-calculus can be /essentially/ recovered from only two of its terms;
these are

 * $S = \lambda x.\lambda y.\lambda z. xz(yz)$, and
 * $K = \lambda x.\lambda y.x$.

A language can be defined with these combinators and function
application. Every \lambda-term can be translated to this language and recovered up
to $=_{\beta\eta}$ equivalence. For example, the identity \lambda-term, $I$, can be written as
\[
I = \lambda x.x = SKK.
\]
It is common to also add the $I = \lambda x.x$ as a basic term to this language,
even if it can be written in terms of $S$ and $K$, as a
way to ease the writing of long complex terms. Terms written with
these combinators are called */SKI-terms/*.

The language of *SKI-terms* can be defined by the following Backus-Naus form
\[
\mathtt{SKI} ::= x \mid (\mathtt{SKI}\ \mathtt{SKI}) \mid S \mid K \mid I\quad,
\]
where $x$ are free variables.

**** Transformation of SKI combinators                            :ignore:
#+attr_latex: :options [Lambda transform]
#+begin_definition
The *Lambda-transform* of a SKI-term is a \lambda-term defined
recursively as

  * $\lambdatrans(x) = x$, for any variable $x$;
  * $\lambdatrans(I) = (\lambda x.x)$;
  * $\lambdatrans(K) = (\lambda x.\lambda y.x)$;
  * $\lambdatrans(S) = (\lambda x.\lambda y.\lambda z.xz(yz))$;
  * $\lambdatrans(XY) = \lambdatrans(X)\lambdatrans(Y)$.
#+end_definition

#+attr_latex: :options [Bracket abstraction]
#+begin_definition
The *bracket abstraction* of the SKI-term $U$ on the variable $x$ is
written as $[x].U$ and defined recursively as

  * $[x].x = I$;
  * $[x].M = KM$, if $x \notin \freevars(M)$;
  * $[x].Ux = U$, if $x \notin \freevars(U)$;
  * $[x].UV = S([x].U)([x].V)$, otherwise.

where $\freevars$ is the set of free variables; as defined on Definition
[[def-freevariables]].
#+end_definition

#+attr_latex: :options [SKI abstraction]
#+begin_definition
The *SKI abstraction* of a \lambda-term $M$, written as $\skiabs(M)$ is
defined recursively as

  * $\skiabs(x) = x$, for any variable $x$;
  * $\skiabs(MN) = \skiabs(M)\skiabs(N)$;
  * $\skiabs(\lambda x.M) = [x].\skiabs(M)$;

where $[x].U$ is the bracket abstraction of the SKI-term $U$.
#+end_definition

#+attr_latex: :options [SKI combinators and lambda terms]
#+begin_theorem
The SKI-abstraction is a retraction of the Lambda-transform of the term,
that is, for any SKI-term $U$,
\[
\skiabs(\lambdatrans(U)) = U.
\]
#+end_theorem
#+begin_proof
By structural induction on $U$,

  * $\skiabs\lambdatrans(x) = x$, for any variable $x$;
  * $\skiabs\lambdatrans(I) = [x].x = I$;
  * $\skiabs\lambdatrans(K) = [x].[y].x = [x].Kx = K$;
  * $\skiabs\lambdatrans(S) = [x].[y].[z].xz(yz) = [x].[y].Sxy = S$; and
  * $\skiabs\lambdatrans(MN) = MN$.$\qedhere$
#+end_proof

In general this translation is not an isomorphism. As an example
\[
\lambdatrans(\skiabs(\lambda u. v u)) = \lambdatrans(v) = v.
\]
However, the \lambda-terms can be essentially recovered if we relax equality
between \lambda-terms to mean $=_{\beta\eta}$.
# This problem could be addressed by using a relaxed form of
# equality containing \eta-equivalence, see cite:Hindley08 for details.

#+ATTR_LATEX: :options [Recovering lambda terms from SKI combinators]
#+BEGIN_theorem
For any \lambda-term $M$,
\[
\lambdatrans(\skiabs(M)) =_{\beta\eta} M.
\]
#+END_theorem
#+BEGIN_proof
We can firstly prove by structural induction that $\lambdatrans([x].M) = \lambda x.\lambdatrans(M)$
for any $M$. In fact, we know that $\lambdatrans([x].x) = \lambda x.x$ for any 
variable $x$; we also know that
\[\begin{aligned}
\lambdatrans([x].MN) &= \lambdatrans(S([x].M)([x].N)) \\
          &= (\lambda x.\lambda y.\lambda z. xz(yz))(\lambda x.\lambdatrans(M))(\lambda x.\lambdatrans(N)) \\
          &= \lambda z.\lambdatrans(M)\lambdatrans(N);
\end{aligned}\]
also, if $x$ is free in $M$,
\[
\lambdatrans([x].M) = \lambdatrans(KM) = (\lambda x.\lambda y.x) \lambdatrans(M) =_{\beta} \lambda x.\lambdatrans(M);
\]
and finally, if $x$ is free in $U$,
\[
\lambdatrans([x].Ux) = \lambdatrans(U) =_{\eta} \lambda x.\lambdatrans(U)x\ .
\]
Now we can use this result to prove the main theorem. Again by
structural induction,

 * $\lambdatrans\skiabs(x) = x$;
 * $\lambdatrans\skiabs(MN) = \lambdatrans\skiabs(M)\lambdatrans\skiabs(N) = MN$;
 * $\lambdatrans\skiabs(\lambda x.M) = \lambdatrans([x].\skiabs(M)) =_{\beta\eta} \lambda x.\lambdatrans\skiabs(M) = \lambda x.M$.$\qedhere$
#+END_proof

*** Turing completeness
# Turing, Church and Gödel.
# Papers by Turing, Church and Gödel.
# The lambda calculus as a reasonable machine. Ugo Dal Lago.

# https://en.wikipedia.org/wiki/Entscheidungsproblem

Three different notions of computability were proposed in the 1930s

 * the *general recursive functions* were defined by Herbrand and Gödel.
   They form a class of functions over the natural numbers closed under
   composition, recursion and unbound search.

 * the *\lambda-definable functions* were proposed by Church. They are
   functions on the natural numbers that can be represented by
   \lambda-terms.

 * the *Turing computable functions*, proposed by Alan Turing as the
   functions that can be defined on a theoretical model of a machine,
   the /Turing machines/.

In cite:church36 and cite:turing37, Church and Turing proved the equivalence of
the three definitions. This lead to the metatheoretical */Church-Turing thesis/*,
which postulated the equivalence between these models of computation and the
intuitive notion of /effective calculability/ mathematicians were using.
In practice, this means that the \lambda-calculus, as a programming language, is as
expressive as Turing machines; it can define every computable function.
It is Turing-complete.

# We will informally prove this equivalence: 
# a \lambda-calculus interpreter will be written in chapter ?, proving
# that \lambda-calculus is representable in a Turing machine
# equivalent, namely, our computer;
# general recursive functions will be implemented in \lambda-calculus
# in chapter ? proving that a Turing machine can be represented in it.
# interpreter and implementing general recursive functions on it.

A complete implementation of untyped \lambda-calculus is discussed in the
chapter on [[*Mikrokosmos][Mikrokosmos]]; and a detailed description on how to use the
untyped \lambda-calculus as a programming language is given in the chapter
''[[*Programming in the untyped \lambda-calculus][Programming in the untyped \lambda-calculus]]''.

# Church - An unsolvable problem of elementary number theory
# Corollary 1 pág 362.
# The set of well-formed formulas which have no normal form is not
# recursively enumerable.

** Simply typed \lambda-calculus
*/Types/* were introduced in mathematics as a response to the
Russell's paradox, found in the first naive axiomatizations of set
theory. An attempt to use untyped \lambda-calculus as a foundational
logical system by Church suffered from the */Rosser-Kleene paradox/*, as
detailed in cite:kleene35 and cite:curry46; and types were a way to avoid it.
Once types are added, a deep connection between \lambda-calculus and
logic arises. This connection will be discussed in the [[*The Curry-Howard correspondence][next chapter]].

In programming languages, types indicate how the programmer intends to
use the data, prevent errors, and enforce certain invariants and
levels of abstraction in programs. The role of types in
\lambda-calculus when interpreted as a programming language closely
matches what we would expect of types in any common programming
language, and typed \lambda-calculus has been the basis of many modern
type systems for programming languages.

*Simply typed \lambda-calculus* is a refinment of the untyped
\lambda-calculus. In it, each term has a type, which limits how it can
be combined with other terms. Only a set of basic types and function
types between any to types are considered in this system. Whereas
functions in untyped \lambda-calculus could be applied over any term,
now a function of type $A \to B$ can only be applied over a term of
type $A$, to produce a new term of type $B$, where $A$ and $B$ could
be, themselves, function types.

We will give now a presentation of simply typed \lambda-calculus based
on cite:Hindley08. Our presentation will rely only on the /arrow type constructor/
$\to$. While other presentations of simply typed
\lambda-calculus extend this definition with type constructors
providing pairs or union types, as it is done in cite:selinger13, it
seems clearer to present a first minimal version of the
\lambda-calculus. Such extensions will be explained later, and its
exposition will profit from the logical interpretation that we will
explain in "[[*Propositions as types][propositions as types]]".

*** Simple types
We start assuming a set of *basic types*. Those basic types would
correspond, in a programming language interpretation, with the
fundamental types of the language. Examples would be the type of
strings or the type of integers. Minimal presentations of \lambda-calculus
tend to use only one basic type.

#+attr_latex: :options [Simple types]
#+begin_definition
The set of *simple types* is given by the following Backus-Naur form
\[\mathtt{Type} ::= 
\iota \mid 
\mathtt{Type} \to \mathtt{Type},\]
where $\iota$ would be any /basic type/.
#+end_definition

That is to say that, for every two types $A,B$, there exists a
*function type* $A \to B$ between them.

*** Typing rules for simply typed \lambda-calculus
We will now define the terms of simply typed \lambda-calculus using
the same constructors we used on the untyped version. Those are the
*/raw typed \lambda-terms/*.

#+attr_latex: :options [Raw typed lambda terms]
#+begin_definition
The set of *typed lambda terms* is given by the following Backus-Naus form
\[ \mathtt{Term} ::=
x \mid
\mathtt{Term}\ \mathtt{Term} \mid
\lambda x^{\mathtt{Type}}. \mathtt{Term}.
\]
#+end_definition

The main difference here with Definition [[def-lambdaterms]] is 
that every bound variable has a type, and therefore, every \lambda-abstraction
of the form $(\lambda x^A. M)$ can be applied only over terms type $A$; if $M$ is of
type $B$, this term will be of type $A \to B$. 

However, the set of raw typed \lambda-terms contains some meaningless terms
under this type interpretation, such as $(\lambda x^A. M)(\lambda x^A. M)$.[fn:meaninglesstype]
*Typing rules* will give them the desired expressive power; only a subset
of these raw lambda terms will be typeable, and we will choose to work
only with that subset. When a particular term $M$ has type $A$, we write
this relation as $M : A$, where the $:$ symbol should be read as ''is of type''.

[fn:meaninglesstype]: In particular, we can not apply a function of type $A \to B$ to
a term of type $A \to B$; it is expecting a term of type $A$.

**** Typing rules                                                 :ignore:
#+attr_latex: :options [Typing context]
#+begin_definition
A *typing context* is a sequence of type assumptions
$x_1:A_1,\dots,x_n:A_n$, where no variable $x_{i}$ appears more than once.
We will implicitely assume that the order in which these
assumptions appear does not matter.
#+end_definition

Every typing rule assumes a typing context, usually denoted by $\Gamma$.
Concatenation of typing contexts is written as $\Gamma,\Gamma'$; and
the fact that $\psi$ follows from $\Gamma$ is written as $\Gamma \vdash \psi$.
Typing rules are written as rules of inference; the premises are
listed above and the conclusion is written below the line.

 1) The $(var)$ rule simply makes explicit the type of a variable from
    the context. That is, a context that assumes that $x : A$ can
    be written as $\Gamma,x:A$; and we can trivially deduce from it that $x:A$.

    \begin{prooftree}
    \RightLabel{($var$)}
    \AXC{}
    \UIC{$\Gamma, x:A \vdash x:A$}
    \end{prooftree}

 2) The $(abs)$ rule declares that the type of a \lambda-abstraction is the type of
    functions from the variable type to the result type. If a term $M:B$ can
    be built from the assumption that $x:A$, then $\lambda x^{A}. M : A \to B$. It acts as
    a /constructor/ of function terms.

   \begin{prooftree}
   \RightLabel{$(abs)$}
   \AXC{$\Gamma, x:A \vdash M : B$}
   \UIC{$\Gamma \vdash \lambda x.M : A \to B$}
   \end{prooftree}

 3) The $(app)$ rule declares the type of a well-typed application.
    A term $f : A \to B$ applied to a term $a : A$ is a term
    $f\ a : B$. It acts as a /destructor/ of function terms.

    \begin{prooftree}
    \RightLabel{$(app)$}
    \AXC{$\Gamma \vdash f : A \to B$}
    \AXC{$\Gamma \vdash a : A$}
    \BIC{$\Gamma \vdash f a : B$}
    \end{prooftree}

#+begin_definition
A term $M$ is *typeable* in a giving context $\Gamma$ if a typing
judgement of the form $\Gamma \vdash M : T$ can be derived using only
the previous typing rules.
#+end_definition

From now on, we only consider typeable terms as the only terms of
simply typed \lambda-calculus. As a consequence, the set of \lambda-terms
of simply typed \lambda-calculus is only a subset of the terms of untyped
\lambda-calculus.

**** TODO Beta rules for STLC                                     :noexport:ignore:
# These rules are not necessary, we are only using function types.

Typeable terms follow the following set of \beta-reduction rules, which include
the untyped \beta-reduction rule defined in Definition [[def-betared]] and
add explicit reduction rules for the new types. Namely,

 * function application, $(\lambda x.M)N \to_{\beta} M[N/x]$;
 * first projection $\pi_{1} (M,N) \to_{\beta} M$; and
 * second projection $\pi_{2}_{} (M,N) \to_{\beta} M$.

These rules govern how can we compute with pair types. With this
rules $\tto_{\beta}$ and $=_{\beta}$ should be redefined accordingly.

**** Examples of typeable and non-typeable terms                  :ignore:
#+ATTR_LATEX: :options [Typeable and non-typeable terms]
#+BEGIN_exampleth
The term $\lambda f.\lambda x.f (f x)$ is typeable.
If we abbreviate $\Gamma = f:A \to A,\ x:A$, the detailed typing derivation
can be written as
\begin{prooftree}
\AX$\fCenter$
\RightLabel{$(var)$}
\UI$\Gamma\ \fCenter\vdash f : A \to A$
\AX$\fCenter$
\RightLabel{$(var)$}
\UI$\Gamma\ \fCenter\vdash x : A$
\AX$\fCenter$
\RightLabel{$(var)$}
\UI$\Gamma\ \fCenter\vdash f : A \to A$
\RightLabel{$(app)$}
\BI$\Gamma\ \fCenter\vdash f\ x : A$
\RightLabel{$(app)$}
\BI$f : A \to A, x : A\ \fCenter\vdash f (f x) : A$
\RightLabel{$(abs)$}
\UI$f : A \to A\ \fCenter\vdash \lambda x. f (f x) : A \to A$
\RightLabel{$(abs)$}
\UI$\fCenter\vdash \lambda f.\lambda x.f (f x) : (A \to A) \to A \to A$
\end{prooftree}
The term $(\lambda x.x\ x)$, however, is not typeable. If $x$ were of type $\psi$,
it also should be of type $\psi \to \sigma$ for some $\sigma$ in order for $x\ x$ to
be well-typed;
but $\psi \equiv \psi \to \sigma$ is not solvable, as it can be shown by structural
induction on the term $\psi$.
#+END_exampleth

It can be seen that the typing derivation of a term somehow encodes
the complete \lambda-term. If we were to derive the term bottom-up, there
would be only one possible choice at each step on which rule to use.
In the following sections we will discuss a type inference algorithm
that determines if a type is typeable and what its type should be,
and we will make precise these intuitions.

*** Curry-style types
Two different approaches to typing in \lambda-calculus are commonly used.

 * *Church-style* typing, also known as /explicit typing/, originated from
   the work of Alonzo Church in cite:church40, where he described a STLC
   with two basic types. The term's type is defined as an intrinsic property
   of the term; and the same term has to be always interpreted with the same
   type.

 * *Curry-style* typing, also known as /implicit typing/; which
   creates a formalism where every single term can be given an
   infinite number of types.  This technique is called
   */polymorphism/* when it is a formal part of the language; but
   here, it is only used to allow us to build intermediate terms
   without having to directly specify their type.

As an example, we can consider the identity term $I = \lambda x.x$. It would have to be 
defined for each possible type. That is, we should consider a family of different 
identity terms $I_A = \lambda x.x : A \to A$. Curry-style typing allows us to consider 
parametric types with type variables, and to type the identity as 
$I = \lambda x.x : \sigma \to \sigma$ where $\sigma$ would a free type variable.

#+attr_latex: :options [Parametric types]
#+begin_definition
Given a infinite numerable set of /type variables/, we define *parametric types*
or /type templates/ inductively as
\[\mathtt{PType} ::= 
\iota \mid
\mathtt{Tvar} \mid 
\mathtt{PType} \to \mathtt{PType}, \]
where $\iota$ is a basic type, $\mathtt{Tvar}$ is a type variable and $\mathtt{PType}$ is
a parametric type.
That is, all basic types and type variables are atomic parametric types; and we also
consider the arrow type between two parametric types.
#+end_definition

The difference between the two typing styles is then not a mere notational
convention, but a difference on the expressive power that we assign to each
term. The interesting property of type variables is that they can act as
placeholders for other type templates. This is formalized with the notion
of type substitution.

#+attr_latex: :options [Type substitution]
#+begin_definition
A *substitution* $\psi$ is any function from type variables to type templates. Any
substitution $\psi$ can be extended to a function between type templates called $\overline{\psi}$
and defined inductively by

   * $\overline{\psi} \iota = \iota$, for any basic type $\iota$;
   * $\overline{\psi} \sigma = \psi \sigma$, for any type variable $\sigma$;
   * $\overline{\psi} (A \to B) = \overline{\psi} A \to \overline{\psi} B$.

That is, the parametric type $\overline{\psi} A$ is the same as $A$ but with every type variable
replaced according to the substitution $\psi$.
#+end_definition

We consider a type to be /more general/ than other if the latter can be obtained by
applying a substitution to the former. In this case, the latter is called an /instance/
of the former. For example, $A \to B$ is more general than its instance
$(C \to D) \to B$, where $A$ has been substituted by $C \to D$. An
crucial property of simply typed \lambda-calculus is that every type has a most
general type, called its /principal type/; this will be proved in Theorem [[thm-typeinfer]].

#+attr_latex: :options [Principal type]
#+begin_definition
A closed \lambda-term $M$ has a *principal type* $\pi$ if $M : \pi$ and given any
$M : \tau$, we can obtain $\tau$ as an instance of $\pi$, that is, $\overline{\sigma} \pi = \tau$.
#+end_definition

*** Unification and type inference
**** Unification :ignore:
The unification of two type templates is the construction of two substitutions
making them equal as type templates; that is, the construction of a type that
is a particular instance of both at the same time. We will not only aim for
an unifier but for the most general one between them.

#+attr_latex: :options [Most general unifier]
#+begin_definition
A substitution $\psi$ is called an *unifier* of two sequences of type templates
$\left\{ A_i \right\}_{i = 1,\dots,n}$ and $\left\{ B_i \right\}_{i=1,\dots,n}$ if $\overline{\psi} A_i = \overline{\psi} B_i$ for any $i$. We say that it
is the *most general unifier* if given any other unifier $\phi$ exists a substitution
$\varphi$ such that $\phi = \overline{\varphi} \circ \psi$.
#+end_definition

#+begin_lemma
<<lemma-unification>>
If an unifier of $\left\{ A_i \right\}_{i = 1,\dots,n}$ and $\left\{ B_i \right\}_{i=1,\dots,n}$ exists, the most general unifier
can be found using the following recursive definition of $\mathtt{unify}(A_1,\dots,A_n;B_1,\dots,B_n)$.

  1) $\mathtt{unify}(x;x) = \id$ and $\mathtt{unify}(\iota,\iota) = \id$;
  2) $\mathtt{unify}(x;B) = (x \mapsto B)$, the substitution that only changes $x$ by $B$;
     if $x$ does not occur in $B$. The algorithm *fails* if $x$ occurs in $B$;
  3) $\mathtt{unify}(A;x)$ is defined symmetrically;
  4) $\mathtt{unify}(A \to A'; B \to B') = \mathtt{unify}(A,A';B,B')$;
  5) $\mathtt{unify}(A,A_1,\dots; B,B_1,\dots) = \overline{\psi} \circ \rho$ where $\rho = \mathtt{unify}(A_1,\dots;B_1,\dots)$ 
     and $\psi = \mathtt{unify}(\overline{\rho}A; \overline{\rho}B)$;
  6) $\mathtt{unify}$ fails in any other case.

Where $x$ is any type variable. The two sequences of types have no unifier if and only
if $\mathtt{unify}(A,B)$ fails.
#+end_lemma
#+begin_proof
It is easy to notice that, by structural induction, if
$\mathtt{unify}(A;B)$ exists, it is in fact an unifier.

If the unifier fails in clause 2, there is obviously no possible unifier: the number
of constructors on the first type template will be always smaller than the second one.
If the unifier fails in clause 6, the type templates are fundamentally different, they
have different head constructors and this is invariant to substitutions. This proves
that the failure of the algorithm implies the non existence of an unifier.

We now prove that, if $A$ and $B$ can be unified, $\mathtt{unify}(A,B)$ is the most general unifier.
For instance, in the clause 2, if we call $\psi = (x \mapsto B)$ and, if $\eta$ were another unifier,
then $\eta x = \overline{\eta}x = \overline{\eta} B = \overline{\eta}(\psi(x))$; hence $\overline{\eta} \circ \psi = \eta$ by definition of $\psi$. A similar argument can 
be applied to clauses 3 and 4. In the clause 5, we suppose the existence of some unifier $\psi'$. 
The recursive call gives us the most general unifier $\rho$ of $A_1,\dots,A_n$ and $B_1,\dots,B_{n}$; and 
since it is more general than $\psi'$, there exists an $\alpha$ such that $\overline{\alpha} \circ \rho = \psi'$. Now,
$\overline{\alpha}(\overline{\rho}A) = \psi'(A) = \psi'(B) = \overline{\alpha}(\overline{\rho} B)$, hence $\alpha$ is a unifier of $\overline{\rho}A$ and $\overline{\rho}B$; we can take the 
most general unifier to be $\psi$, so $\overline{\beta} \circ \psi = \overline{\alpha}$; and finally, $\overline{\beta} \circ (\overline{\psi} \circ \rho) = \overline{\alpha} \circ \rho = \psi'$.

We also need to prove that the unification algorithm terminates. Firstly, we note that
every substitution generated by the algorithm is either the identity or it removes at least
one type variable. We can perform induction on the size of the argument on all clauses except
for clause 5, where a substitution is applied and the number of type variables is reduced.
Therefore, we need to apply induction on the number of type variables and only then apply
induction on the size of the arguments.
#+end_proof

**** Type Inference :ignore:
Using unification, we can define type inference.

#+attr_latex: :options [Type inference]
#+begin_theorem
<<thm-typeinfer>>
The algorithm $\mathtt{typeinfer}(M,B)$, defined as follows, finds the most general substitution $\sigma$
such that $x_1 : \sigma A_1, \dots, x_n : \sigma A_n \vdash M : \overline{\sigma} B$ is a valid typing judgment if it exists;
and fails otherwise.

  1) $\mathtt{typeinfer}(x_i:A_i,\Gamma \vdash x_i : B) = \mathtt{unify}(A_i,B)$;
  2) $\mathtt{typeinfer}(\Gamma \vdash MN : B) = \overline{\varphi} \circ \psi$, where $\psi = \mathtt{typeinfer}(\Gamma \vdash M : x \to B)$ and
     $\varphi = \mathtt{typeinfer}(\overline{\psi}\Gamma \vdash N : \overline{\psi}x)$ for a fresh type variable $x$;
  3) $\mathtt{typeinfer}(\Gamma \vdash \lambda x.M : B) = \overline{\varphi} \circ \psi$ where $\psi = \mathtt{unify}(B; z \to z')$ and
     $\varphi = \mathtt{typeinfer}(\overline{\psi}\Gamma, x:\overline{\psi}z \vdash M : \overline{\psi}z')$ for fresh type variables $z,z'$.

Note that the existence of fresh type variables is always asserted by the set of
type variables being infinite. The output of this algorithm is defined up to
a permutation of type variables.
#+end_theorem
#+begin_proof
The algorithm terminates by induction on the size of $M$. It is easy to check
by structural induction that the inferred type judgments are in fact valid.
If the algorithm fails, by Lemma [[lemma-unification]], it is also clear that the
type inference is not possible.

On the first case, the type is obviously the most general substitution
by virtue of the previous Lemma [[lemma-unification]].  On the second
case, if $\alpha$ were another possible substitution, in particular, it should
be less general than $\psi$, so $\alpha = \beta \circ \psi$. As $\beta$ would be then a possible substitution
making $\overline{\psi}\Gamma \vdash N : \overline{\psi}x$ valid, it should be less general than $\varphi$, so 
$\alpha = \overline{\beta} \circ \psi = \overline{\gamma} \circ \overline{\varphi} \circ \beta$.
On the third case, if $\alpha$ were another possible substitution, it should unify
$B$ to a function type, so $\alpha = \overline{\beta} \circ \psi$. Then $\beta$ should make the type inference
$\overline{\psi}\Gamma, x:\overline{\psi}z \vdash M : \overline{\psi}z'$ possible, so $\beta = \overline{\gamma} \circ \varphi$.
We have proved that the inferred type is in general the most general one.
#+end_proof

#+attr_latex: :options [Principal type property]
#+begin_corollary
Every typeable pure \lambda-term has a principal type.
#+end_corollary
#+begin_proof
Given a typeable term $M$, we can compute $\mathtt{typeinfer}(x_1:A_1,\dots,x_n:A_n \vdash M : B)$,
where $x_1,\dots,x_n$ are the free variables on $M$ and $A_1,\dots,A_n,B$ are fresh type
variables. By virtue of Theorem [[thm-typeinfer]], the result is the most general type of $M$
if we assume the variables to have the given types.
#+end_proof

*** Subject reduction and normalization
A crucial property is that type inference and \beta-reductions do not
interfere with each other. A term can be \beta-reduced without changing
its type.

#+attr_latex: :options [Subject reduction]
#+begin_theorem
The type is preserved on \beta-reductions; that is, if $\Gamma \vdash M : A$ and
and $M \tto_{\beta} M'$, then $\Gamma \vdash M' : A$.
#+end_theorem
#+begin_proof
If $M$ has been derived by \beta-reduction, $M = (\lambda x.P)$
and $M' = P[Q/x]$. $\Gamma \vdash M:A$ implies $\Gamma,x:B \vdash P : A$ and
$\Gamma \vdash Q : B$. Again by structural induction on $P$ (where the only crucial
case uses that $x$ and $Q$ have the same type) we can prove
that substitutions do not alter the type and thus, $\Gamma,Q:B \vdash P[Q/x] : A$.
#+end_proof

We have seen previously that the term $\Omega = (\lambda x.xx)(\lambda x.xx)$ is
not weakly normalizing; but it is also non-typeable. In this section
we will prove that, in fact, every typeable term is strongly normalizing.
We start proving some lemmas about the notion of /reducibility/, which
will lead us to the Strong Normalization Theorem. This proof will
follow cite:girard89.

The notion of */reducibility/* is an abstract concept originally
defined by Tait in cite:tait67 which we will use to ease this
proof. It should not be confused with the notion of \beta-reduction.

#+ATTR_LATEX: :options [Reducibility]
#+BEGIN_definition
We inductively define the set of *reducible* terms of type $T$
for basic and arrow types.

 * If $t : T$ where $T$ a basic type, $t \in \redu_{T}$ if $t$ is strongly
   normalizable.

 * If $t : U \to V$, an arrow type, $t \in \redu_{U \to V}$ if $t\ u \in \redu_{V}$ for all
   $u \in \redu_{U}$.
#+END_definition

Properties of reducibility will be used directly in the Strong
Normalization Theorem. We prove three of them at the same time in
order to use mutual induction.

#+ATTR_LATEX: :options [Properties of reducibility]
#+BEGIN_proposition
<<prop-reducibilityprop>>
The following three properties hold;

  1. if $t \in \redu_{T}$, then $t$ is strongly normalizable;
  2. if $t \in \redu_{T}$ and $t \to_{\beta} t'$, $t' \in \redu_{T}$; and
  3. if $t$ is not a \lambda-abstraction and $t' \in \redu_{T}$ for every $t \to_{\beta} t'$,
     then $t \in \redu_{T}$.
#+END_proposition
#+BEGIN_proof
For basic types,

  1. holds trivially.

  2. holds by the definition of strong normalization.

  3. if any one-step \beta-reduction leads to a strongly normalizing term,
     the term itself must be strongly normalizing.

For arrow types,

  1. if $x : U$ is a variable, we can inductively apply (3) to get $x \in \redu_{U}$;
     then, $t\ x \in \redu_{V}$ is strongly normalizing and $t$ in particular must be 
     strongly normalizing;

  2. if $t \to_{\beta} t'$ then for every $u \in \redu_{U}$, $t\ u \in \redu_{V}$ and $t\ u \to_{\beta} t'\ u$.
     By induction, $t'\ u \in \redu_{V}$;

  3. if $u \in \redu_{U}$, it is strongly normalizable. As $t$ is not a \lambda-abstraction,
     he term $t\ u$ can only be reduced to $t'\ u$ or $t\ u'$. If $t \to_{\beta} t'$; by induction, $t'\ u \in \redu_{V}$.
     If $u \to_{\beta} u'$, we could proceed by induction over the length of the longest
     chain of \beta-reductions starting from $u$ and assume that $t\ u'$ is irreducible.
     In every case, we have proved that $t\ u$ only reduces to already reducible terms;
     thus, $t\ u \in \redu_{U}$.
#+END_proof

#+ATTR_LATEX: :options [Abstraction lemma]
#+BEGIN_lemma
<<lemma-reductionabstraction>>
If $v[u/x] \in \redu_{V}_{}$ for all $u \in \redu_{U}$, then $\lambda x.v \in \redu_{U \to V}_{}_{}$.
#+END_lemma
#+BEGIN_proof
We apply induction over the sum of the lengths of the longest
\beta-reduction sequences from $v[x/x]$ and $u$. The term $(\lambda x.v) u$ can be \beta-reduced to

  * $v[u/x] \in \redu_{U}$; in the base case of induction, this is the only choice;
  * $(\lambda x.v')u$ where $v \to_{\beta }v'$, and, by induction, $(\lambda x.v') u \in \redu_{V}$;
  * $(\lambda x.v)u'$ where $u \to_{\beta} u'$, and, again by induction, $(\lambda x.v) u' \in \redu_{V}$.

Thus, by Proposition [[prop-reducibilityprop]], $(\lambda x.v) \in \redu_{U \to V}$.
#+END_proof

A final lemma is needed before the proof of the Strong Normalization Theorem.
It is a generalization of the main theorem, useful because of the stronger
induction hypothesis it provides.

#+ATTR_LATEX: :options [Strong Normalization lemma]
#+BEGIN_lemma
<<lemma-strongnormalization>>
Given an arbitrary $t : T$ with free variables $x_{1} : U_{1}, \dots, x_{n} : U_{n}$, and reducible
terms $u_{1} \in \redu_{U_1}, \dots, u_{n} \in \redu_{U_{2}}$, we know that
\[
t[u_1 / x_1][u_2 / x_{2}]\dots[u_n / x_n] \in \redu_{T}.
\]
#+END_lemma
#+BEGIN_proof
We call $\tilde{t} = t[u_1 / x_1][u_2 / x_{2}]\dots[u_n / x_n]$ and apply structural induction over $t$,

  * if $t = x_i$, then we simply use that $u_i \in \redu_{U_i}$,

  * if $t = v\ w$, then we apply induction hypothesis to get $\tilde{v} \in \redu_{R \to T},\tilde{w} \in \redu_{R}$ 
    for some type $R$. Then, by definition, $\tilde{t} = \tilde{v}\ \tilde{w} \in \redu_T$,

  * if $t = \lambda y. v : R \to S$, then by induction $\tilde{v}[r/y] \in \redu_S$ for every $r : R$.
    We can then apply Lemma [[lemma-reductionabstraction]] to get that
    $\tilde{t} = \lambda y.\tilde{v} \in \redu_{R \to S}$.$\qedhere$
#+END_proof

#+attr_latex: :options [Strong Normalization Theorem]
#+begin_theorem
In the STLC, all terms are strongly normalizing.
#+end_theorem
#+BEGIN_proof
It is the particular case of Lemma [[lemma-strongnormalization]] where we
take $u_i = x_i$.
#+END_proof

# [[https://math.stackexchange.com/questions/1319149/what-breaks-the-turing-completeness-of-simply-typed-lambda-calculus][What breaks turing completeness of STLC]] (link)
Every term normalizes in the STLC and every computation ends.  We
know, however, that the Halting Problem is unsolvable, so the STLC
must be not Turing complete.

** The Curry-Howard correspondence
# Tutorial on Curry-Howard http://purelytheoretical.com/papers/ATCHC.pdf
# Local soundness and completeness http://www.cs.cmu.edu/~fp/courses/15816-s10/lectures/01-judgments.pdf
# https://www.elsevier.com/books/lectures-on-the-curry-howard-isomorphism/sorensen/978-0-444-52077-7

*** Extending the simply typed \lambda-calculus
We will add now special syntax for some terms and types, such as
pairs, unions and unit types. This syntax will make our \lambda-calculus
more expressive, but the unification and type inference algorithms
will continue to work. The previous proofs and algorithms can be extended to cover
all the new cases.
# And this is done on the mikrokosmos implementation

**** Simple types II                                              :ignore:
#+attr_latex: :options [Simple types II]
#+begin_definition
The new set of *simple types* is given by the following BNF
\[\mathtt{Type} ::= \iota \mid 
\mathtt{Type} \to \mathtt{Type} \mid
\mathtt{Type} \times \mathtt{Type} \mid
\mathtt{Type} + \mathtt{Type} \mid
1 \mid
0,\]
where $\iota$ would be any /basic type/.
#+end_definition

That is to say that, for any given types $A,B$, there exists a product
type $A \times B$, consisting of the pairs of elements where the first
one is of type $A$ and the second one of type $B$; there exists the
union type $A + B$, consisting of a disjoint union of tagged terms
from $A$ or $B$; an unit type $1$ with only an element, and an empty
or void type $0$ without inhabitants. The raw typed \lambda-terms are
extended to use these new types.

**** Raw typed lambda terms II                                    :ignore:
#+attr_latex: :options [Raw typed lambda terms II]
#+begin_definition
The new set of raw *typed lambda terms* is given by the BNF
\[\begin{aligned} 
\mathtt{Term} ::=\ &
x \mid
\mathtt{Term}\mathtt{Term} \mid
\lambda x. \mathtt{Term} \mid \\&
\left\langle \mathtt{Term},\mathtt{Term} \right\rangle \mid
\pi_1 \mathtt{Term} \mid
\pi_2 \mathtt{Term} \mid \\&
\textrm{inl}\ \mathtt{Term} \mid
\textrm{inr}\ \mathtt{Term} \mid
\textrm{case}\ \mathtt{Term}\ \textrm{of}\ \mathtt{Term}; \mathtt{Term} \mid \\&
\textrm{abort}\ \mathtt{Term} \mid \ast
\end{aligned}\]
#+end_definition

The use of these new terms is formalized by the following extended set
of typing rules.

 1) The $(var)$ rule simply makes explicit the type of a variable from
    the context.
    \begin{prooftree}
    \LeftLabel{($var$)}
    \AXC{}
    \UIC{$\Gamma, x:A \vdash x:A$}
    \end{prooftree}

 2) The $(abs)$ gives the type of a \lambda-abstraction as the type of
    functions from the variable type to the result type. It acts as
    a constructor of function terms.
    \begin{prooftree}
    \LeftLabel{$(abs)$}
    \AXC{$\Gamma, x:A \vdash M : B$}
    \UIC{$\Gamma \vdash \lambda x.M : A \to B$}
    \end{prooftree}

 3) The $(app)$ rule gives the type of a well-typed application of a
    lambda term. A term $f : A \to B$ applied to a term $a : A$ is a term
    of type $B$. It acts as a destructor of function terms.
    \begin{prooftree}
    \LeftLabel{$(app)$}
    \AXC{$\Gamma \vdash f : A \to B$}
    \AXC{$\Gamma \vdash a : A$}
    \BIC{$\Gamma \vdash f a : B$}
    \end{prooftree}

 4) The $(pair)$ rule gives the type of a pair of elements. It acts as
    a constructor of pair terms.
    \begin{prooftree}
    \LeftLabel{$(pair)$}
    \AXC{$\Gamma \vdash a : A$}
    \AXC{$\Gamma \vdash b :  B$}
    \BIC{$\Gamma \vdash \pair{a,b} : A \times B$}
    \end{prooftree}

 5) The $(\pi_1)$ rule extracts the first element from a pair. It acts as
    a destructor of pair terms.
    \begin{prooftree}
    \LeftLabel{$(\pi_1)$}
    \AXC{$\Gamma \vdash m : A \times B$}
    \UIC{$\Gamma \vdash \pi_1\ m : A$}
    \end{prooftree}

 6) The $(\pi_1)$ rule extracts the second element from a pair. It acts as
    a destructor of pair terms.
    \begin{prooftree}
    \LeftLabel{$(\pi_2)$}
    \AXC{$\Gamma \vdash m : A \times B$}
    \UIC{$\Gamma \vdash \pi_2\ m : B$}
    \end{prooftree}

 7) The $(inl)$ rule creates a union type from the left side type of
    the sum. It acts as a constructor of union terms.
    \begin{prooftree}
    \LeftLabel{$(inl)$}
    \AXC{$\Gamma \vdash a : A$}
    \UIC{$\Gamma \vdash \mathrm{inl}\ a : A + B$}
    \end{prooftree}

 8) The $(inr)$ rule creates a union type from the right side type of
    the sum. It acts as a constructor of union terms.
    \begin{prooftree}
    \LeftLabel{$(inr)$}
    \AXC{$\Gamma \vdash b : B$}
    \UIC{$\Gamma \vdash \mathrm{inr}\ b : A + B$}
    \end{prooftree}

 9) The $(case)$ rule extracts a term from an union and applies the appropiate
    deduction on any of the two cases
    \begin{prooftree}
    \LeftLabel{$(case)$}
    \AXC{$\Gamma \vdash m : A + B$}
    \AXC{$\Gamma, a:A \vdash n : C$}
    \AXC{$\Gamma, b:B \vdash p : C$}
    \TIC{$\Gamma \vdash (\mathrm{case}\ m\ \mathrm{of}\ [a].n;\ [b].p) : C$}
    \end{prooftree}

 10) The $(\ast)$ rule simply creates the only element of $1$. It is a constructor
     of the unit type.
     \begin{prooftree}
     \LeftLabel{$(\ast)$}
     \AXC{$$}
     \UIC{$\Gamma \vdash \ast : 1$}
     \end{prooftree}

 11) The $(abort)$ rule extracts a term of any type from the void type.
     \begin{prooftree}
     \LeftLabel{$(abort)$}
     \AXC{$\Gamma \vdash M : 0$}
     \UIC{$\Gamma \vdash \mathrm{abort}_A\ M : A$}
     \end{prooftree} 

     The abort function must be understood as the unique function going
     from the empty set to any given set.

**** Beta-eta reductions in extended typed lambda calculus        :ignore:
The \beta-reduction of terms is defined the same way as for the untyped
\lambda-calculus; except for the inclusion of \beta-rules governing the
new terms, each for every new destruction rule.

  1) Function application, $(\lambda x.M)N \to_{\beta} M[N/x]$.
  2) First projection, $\pi_1 \left\langle M,N \right\rangle \to_{\beta} M$.
  3) Second projection, $\pi_2 \left\langle M,N \right\rangle \to_{\beta} N$.
  4) Case rule, $(\mathrm{case}\ m\ \mathrm{of}\ [a].N;\ [b].P) \to_{\beta} N a$ if $m$ is of the form $m = \mathrm{inl}\ a$; and
     $(\mathrm{case}\ m\ \mathrm{of}\ [a].N;\ [b].P) \to_{\beta} P b$ if $m$ is of the form $m = \mathrm{inr}\ b$.

On the other side, new \eta-rules are defined, each for every new construction rule.

  1) Function extensionality, $\lambda x.M x \to_{\eta} M$.
  2) Definition of product, $\langle \pi_1 M, \pi_{2} M \rangle \to_{\eta} M$.
  3) Uniqueness of unit, $M \to_{\eta} \ast$.
  4) Case rule, $(\mathrm{case}\ m\ \mathrm{of}\ [a].P[ \mathrm{inl}\ a/c ];\ [b].P[ \mathrm{inr}\ b/c ]) \to_{\eta} P[m/c]$.

*** Natural deduction
The natural deduction is a logical system due to Gentzen. We introduce
it here following cite:selinger13 and cite:wadler15. It relationship
with the STLC will be made explicit on the [[*Propositions as types][next section]].

We will use the logical binary connectives $\to,\land,\lor$, and two
given propositions, $\top,\bot$ representing truth and falsity. The
rules defining natural deduction come in pairs; there are introductors
and eliminators for every connective. Every introductor uses a set of
assumptions to generate a formula and every eliminator gives a way to
extract precisely that set of assumptions.

 1) Every axiom on the context can be used.

    \begin{prooftree}
    \RightLabel{(Ax)}
    \AXC{}
    \UIC{$\Gamma,A \vdash A$}
    \end{prooftree}

 2) Introduction and elimination of the $\to$ connective. Note that the
    elimination rule corresponds to /modus ponens/.

    \begin{prooftree}
    \RightLabel{($I_{\to}$)}
    \AXC{$\Gamma, A \vdash B$}
    \UIC{$\Gamma \vdash A \to B$}
    \RightLabel{($E_{\to}$)}
    \AXC{$\Gamma \vdash A \to B$}
    \AXC{$\Gamma \vdash A$}
    \BIC{$\Gamma \vdash B$}
    \noLine
    \BIC{}
    \end{prooftree}

 3) Introduction and elimination of the $\land$ connective. Note that the
    introduction in this case takes two assumptions, and there are
    two different elimination rules.

    \begin{prooftree}
    \RightLabel{($I_{\land}$)}
    \AXC{$\Gamma \vdash A$}
    \AXC{$\Gamma \vdash B$}
    \BIC{$\Gamma \vdash A \land B$}
    \RightLabel{($E_{\land}^1$)}
    \AXC{$\Gamma \vdash A \land B$}
    \UIC{$\Gamma \vdash A$}
    \RightLabel{($E_{\land}^2$)}
    \AXC{$\Gamma \vdash A \land B$}
    \UIC{$\Gamma \vdash B$}
    \noLine
    \TIC{}
    \end{prooftree}

 4) Introduction and elimination of the $\lor$ connective. Here, we need
    two introduction rules to match the two assumptions we use on the
    eliminator.

    \begin{prooftree}
    \RightLabel{($I_{\lor}^1$)}
    \AXC{$\Gamma \vdash A$}
    \UIC{$\Gamma \vdash A \lor B$}
    \RightLabel{($I_{\lor}^2$)}
    \AXC{$\Gamma \vdash B$}
    \UIC{$\Gamma \vdash A \lor B$}
    \RightLabel{($E_{\lor}$)}
    \AXC{$\Gamma \vdash A \lor B$}
    \AXC{$\Gamma,A \vdash C$}
    \AXC{$\Gamma,B \vdash C$}
    \TIC{$\Gamma \vdash C$}
    \noLine
    \TIC{}
    \end{prooftree}

 5) Introduction for $\top$. It needs no assumptions and, consequently,
    there is no elimination rule for it.

    \begin{prooftree}
    \RightLabel{($I_{\top}$)}
    \AXC{}
    \UIC{$\Gamma \vdash \top$}
    \end{prooftree}

 6) Elimination for $\bot$. It can be eliminated in all generality, and,
    consequently, there are no introduction rules for it. This elimination
    rule represents the /"ex falsum quodlibet"/ principle that says that
    falsity implies anything.

    \begin{prooftree}
    \RightLabel{($E_{\bot}$)}
    \AXC{$\Gamma \vdash \bot$}
    \UIC{$\Gamma \vdash C$}
    \end{prooftree}

Proofs on natural deduction are written as deduction trees, and they
can be simplified according to some simplification rules, which can
be applied anywhere on the deduction tree. On these rules, a chain
of dots represents any given part of the deduction tree.

  1) An implication and its antecedent can be simplified using the
     antecedent directly on the implication.

    \begin{prooftree}
    \AXC{$[A]$}\noLine
    \UIC{$\vdots^{1}$}\noLine
    \UIC{$B$}
    \UIC{$A \to B$}
    \AXC{$\vdots^2$}\noLine
    \UIC{$A$}
    \BIC{$B$}

    \UIC{$\vdots$}\noLine
    \AXC{$\Longrightarrow$}
    \UIC{}\noLine\UIC{}\noLine\UIC{}\noLine

    \AXC{$\vdots^{2}$}\noLine
    \UIC{$A$}\noLine
    \UIC{$\vdots^{1}$}\noLine
    \UIC{$B$}
    \UIC{$\vdots$}\noLine
    \noLine
    \TIC{}
    \end{prooftree}

  2) The introduction of an unused conjunction can be simplified
     as

    \begin{prooftree}
    \AXC{$\vdots^{1}$}\noLine
    \UIC{$A$}
    \AXC{$\vdots^{2}$}\noLine
    \UIC{$B$}
    \BIC{$A \land B$}
    \UIC{$A$}
    \UIC{$\vdots$}\noLine
    \AXC{$\Longrightarrow$}
    \UIC{}\noLine\UIC{}\noLine\UIC{}\noLine
    \AXC{$\vdots^{1}$}\noLine
    \UIC{$A$}
    \UIC{$\vdots$}\noLine
    \noLine
    \TIC{}
    \end{prooftree}

    and, similarly, on the other side as

    \begin{prooftree}
    \AXC{$\vdots^{1}$}\noLine
    \UIC{$A$}
    \AXC{$\vdots^{2}$}\noLine
    \UIC{$B$}
    \BIC{$A \land B$}
    \UIC{$B$}
    \UIC{$\vdots$}\noLine
    \AXC{$\Longrightarrow$}
    \UIC{}\noLine\UIC{}\noLine\UIC{}\noLine
    \AXC{$\vdots^{2}$}\noLine
    \UIC{$B$}
    \UIC{$\vdots$}\noLine
    \noLine
    \TIC{}
    \end{prooftree}

  3) The introduction of a disjunction followed by its elimination can
     be also simplified

    \begin{prooftree}
    \AXC{$\vdots^{1}$}\noLine
    \UIC{$A$}
    \UIC{$A+B$}
    \AXC{$[A]$}\noLine
    \UIC{$\vdots^2$}\noLine
    \UIC{$C$}
    \AXC{$[B]$}\noLine
    \UIC{$\vdots^3$}\noLine
    \UIC{$C$}
    \TIC{$C$}

    \UIC{$\vdots$}\noLine
    \AXC{$\Longrightarrow$}
    \UIC{}\noLine\UIC{}\noLine\UIC{}\noLine

    \AXC{$\vdots^{1}$}\noLine
    \UIC{$A$}\noLine
    \UIC{$\vdots^{2}$}\noLine
    \UIC{$C$}
    \UIC{$\vdots$}\noLine
    \noLine
    \TIC{}
    \end{prooftree}

    and a similar pattern is used on the other side of the disjunction

    \begin{prooftree}
    \AXC{$\vdots^{1}$}\noLine
    \UIC{$B$}
    \UIC{$A+B$}
    \AXC{$[A]$}\noLine
    \UIC{$\vdots^2$}\noLine
    \UIC{$C$}
    \AXC{$[B]$}\noLine
    \UIC{$\vdots^3$}\noLine
    \UIC{$C$}
    \TIC{$C$}

    \UIC{$\vdots$}\noLine
    \AXC{$\Longrightarrow$}
    \UIC{}\noLine\UIC{}\noLine\UIC{}\noLine

    \AXC{$\vdots^{1}$}\noLine
    \UIC{$B$}\noLine
    \UIC{$\vdots^{3}$}\noLine
    \UIC{$C$}
    \UIC{$\vdots$}\noLine
    \noLine
    \TIC{}
    \end{prooftree}

*** Propositions as types
In 1934, Curry observed in cite:curry34 that the type of a function
$(A \to B)$ could be read as an implication and that the existence of a
function of that type was equivalent to the provability of the proposition.
Previously, the *Brouwer-Heyting-Kolmogorov interpretation* of intuitionistic
logic had given a definition of what it meant to be a proof of an intuinistic
formula, where a proof of the implication $(A \to B)$ was a function converting
a proof of $A$ into a proof of $B$. It was not until 1969 that Howard pointed
a deep correspondence between the simply-typed \lambda-calculus and the
natural deduction at three levels

  1) propositions are types.
  2) proofs are programs.
  3) simplification of proofs is the evaluation of programs.

In the case of STLC and natural deduction, the correspondence starts when
we describe the following isomorphism between types and propositions.

\begin{center}\begin{tabular}{c|c}
Types & Propositions \\
\hline
Unit type ($1$) & Truth ($\top$) \\
Product type ($\times$) & Conjunction ($\land$) \\
Union type ($+$) & Disjunction ($\lor$) \\
Function type ($\to$) & Implication ($\to$) \\
Empty type ($0$) & False ($\bot$)
\end{tabular}\end{center}

Now it is easy to notice that every [[*Natural deduction][deduction rule]] for a proposition has a
correspondence with a [[*Extending the simply typed \lambda-calculus][typing rule]]. The only distinction between them is the
appearance of \lambda-terms on the first set of rules. As every typing rule
results on the construction of a particular kind of \lambda-term, they can
be interpreted as encodings of proof in the form of derivation trees. That is,
terms are proofs of the propositions represented by their types.

Under this interpretation, 
*/simplification rules are precisely the \beta-reduction rules/*.
This makes execution of \lambda-calculus
programs correspond to proof simplification on natural deduction.
The Curry-Howard correspondence is then not only a simple bijection
between types and propositions, but a deeper isomorphism regarding the
way they are constructed, used in derivations, and simplified.

#+attr_latex: :options [Curry-Howard example]
#+begin_exampleth
As an example of this duality, we will write a proof/term of the proposition/type =A → B + A=
and we are going to simplify/compute it using proof simplification rules/\beta-rules.
Similar examples can be found in cite:wadler15.

We start with the following derivation tree; in which terms are colored in
$\cterms{red}$ and types are colored in $\ctypes{blue}$
\begin{prooftree}\EnableBpAbbreviations
\AXC{$\cterms{b }\ctypes{:: [A+B]}$}
\AXC{$\cterms{c }\ctypes{:: A}$}
\RightLabel{$(inr)$}
\UIC{$\cterms{inr c }\ctypes{:: B+A}$}
\AXC{$\cterms{c }\ctypes{:: B}$}
\RightLabel{$(inl)$}
\UIC{$\cterms{inl c }\ctypes{:: B+A}$}
\RightLabel{$(case)$}
\TIC{$\cterms{case b of [c].inr c; [c].inl c }\ctypes{:: B+A}$}
\RightLabel{$(abs)$}
\UIC{$\cterms{λb.case b of [c].inr c; [c].inl c }\ctypes{:: A+B \to B+A}$}

\AXC{$\cterms{a }\ctypes{:: A}$}
\RightLabel{$(inl)$}
\UIC{$\cterms{inl a }\ctypes{:: A+B}$}
\RightLabel{$(app)$}
\BIC{$\cterms{(λb.case b of [c].inr c; [c].inl c)(inl a) }\ctypes{:: B+A}$}
\RightLabel{$(abs)$}
\UIC{$\cterms{λa.(λb.case b of [c].inr c; [c].inl c) (inl a) }\ctypes{:: A \to B + A}$}
\end{prooftree}

which is encoded by the term =λa.(λc.case c of [a].inr a; [b].inl b) (λz.inl z)=.
We apply the simplification rule/\beta-rule of the implication/function application
to get

\begin{prooftree}\EnableBpAbbreviations
\AXC{$\cterms{z }\ctypes{:: A}$}
\RightLabel{$(inl)$}
\UIC{$\cterms{inl z }\ctypes{:: A+B}$}
\AXC{$\cterms{a }\ctypes{:: A}$}
\RightLabel{$(inr)$}
\UIC{$\cterms{inr a }\ctypes{:: B+A}$}
\AXC{$\cterms{b }\ctypes{:: B}$}
\RightLabel{$(inl)$}
\UIC{$\cterms{inl b }\ctypes{:: B+A}$}
\RightLabel{$(case)$}
\TIC{$\cterms{case (inl z) of [a].inr a; [b].inl b }\ctypes{:: B+A}$}
\RightLabel{$(abs)$}
\UIC{$\cterms{λ z.case (inl z) of [a].inr a; [b].inl b }\ctypes{:: A \to B +A}$}
\end{prooftree}

which is encoded by the term =λa.case (inl a) of (inr) (inl)=. We finally
apply the =case= simplification/reduction rule to get

\begin{prooftree}\EnableBpAbbreviations
\AXC{$\cterms{a }\ctypes{:: A}$}
\RightLabel{$(inr)$}
\UIC{$\cterms{inr a }\ctypes{:: B+A}$}
\RightLabel{$(abs)$}
\UIC{$\cterms{λ a.inr a }\ctypes{:: A \to B + A}$}
\end{prooftree}

which is encoded by =λa.(inr a)=.

On the chapter on [[*Mikrokosmos][Mikrokosmos]], we develop a \lambda-calculus interpreter
which is able to check and simplify proofs in intuitionistic logic.
This example could be checked and simplified by this interpreter as
it is shown in image [[mikrogentzen]]. 

#+caption: Curry-Howard example in Mikrokosmos.
#+name: mikrogentzen
[[./images/mikrogentzen.png]]
#+end_exampleth

# Extending the Curry-Howard correspondence in other type systems
** Other type systems
*** TODO Hindley-Milner
*** TODO Gödel's System T
*** TODO System F                                                :noexport:
**** TODO System F is strongly normalizing
*** TODO Type algebra
# Type algebra should be studied, at least, on System F.
# Properties of type algebra can be proved in Agda.

**** Lists, trees and generating functions
**** Derivatives and one-hole contexts
**** Seven trees in one
*** \lambda-cube
The *\lambda-cube* is a taxonomy for Church-style type systems given
by Barendregt in cite:barendregt92. It describes eight type systems
based on the \lambda-calculus along three axes, representing three
properties of the systems. These properties are

  1) *parametric polymorphism*, terms that depend on types. This is
     achieved via universal quantification over types. It allows type
     variables and binders for them. An example is the following parametric
     identity function
     \[
     \mathrm{id} \equiv \Lambda \tau . \lambda x . x : \forall \tau . \tau \to \tau, 
     \]
     that can be applied to any particular type $\sigma$ to obtain the 
     specific identity function for that type as
     \[
     \mathrm{id}_{\sigma} \equiv \lambda x.x : \sigma \to \sigma.
     \]

     *System F* is the simplest type system on the cube implementing
     polymorphism.

  2) *type operators*, types that depend on types.

  3) *dependent types*, types that depend on terms.

# Pierce
# Lectures on the Curry-Howard isomorphism
# Introduction to generalized type systems - Barendregt

# https://en.wikipedia.org/wiki/System_F#System_F.CF.89

\[\begin{tikzcd}[column sep=small]
&&& |[label={above:\lcubett{System F$\omega$}}]| \systemfo \ar{rr}
&&  |[label=above:\lcubett{CoC},label=above:\phantom{System Fo}]| \systemcoc 
& \\
\phantom{.}  
&&  |[label={left:\lcubett{System F}}]| \systemf \ar{ur}\ar{rr} 
&&  \systemfp \ar{ur}
&& \\ 
&&& \systemo \ar{rr}\ar{uu} 
&&  |[label=right:\lcubett{wCoC}]| \systemlpo \ar{uu}
&&  \phantom{\lambda}\phantom{PQW}  \\
\ar[\lcred]{uu}[\lcred]{\text{\parbox{2cm}{\centering terms depend on types}}}
&&  |[label=below:\lcubett{STLC}]| \stlc \ar{uu}\ar{rr}\ar{ur} 
&&  |[label=below:\lcubett{DTLC}]| \systemlp \ar{uu}\ar{ur} 
&& \phantom{.} \\ 
&&  \ar[\lcred]{rr}[swap,\lcred]{\text{\parbox{2cm}{\centering types depend on terms}}} 
&& \phantom{.} 
& \ar[\lcred]{ur}[swap,\lcred]{\text{\parbox{2cm}{\centering types depend on types}}} 
&
\end{tikzcd}\]

The following type systems

 * *Simply typed \lambda-calculus* ($\stlc$);
 * *System F* ($\systemf$);
 * typed \lambda-calculus with *dependent types* ($\systemlp$);
 * typed \lambda-calculus with *type operators* ($\systemo$);
 * *System F-omega* ($\systemfo$);
   
The \lambda-cube is generalized by the theory of pure type systems.

All systems on the \lambda-cube are strongly normalizing.

# https://cstheory.stackexchange.com/questions/7561/whats-the-relation-and-difference-between-calculus-of-inductive-constructions-a
A different approach to higher-order type systems will be presented in the
chapter on Type Theory.

*** TODO Pure type systems
In particular *System F* is equivalent to the single-sorted pure system $\lambda 2$.
# https://www.ps.uni-saarland.de/extras/fscd17/

*** TODO Subtyping (?)

*** TODO Inductive and coinductive definitions
* Mikrokosmos (abstract)                                             :ignore:
#+LATEX: \ctparttext{\color{black}\begin{center}
We have developed *Mikrokosmos*, an untyped and simply typed \lambda-calculus interpreter
written in the purely functional programming language Haskell cite:hudak07_haskell.
It aims to provide students with a tool to learn and understand \lambda-calculus
and the relation between logic and types.
#+LATEX: \end{center}}

* Mikrokosmos
** Programming environment
*** The Haskell programming language
**** Haskell as a programming choice                              :ignore:
*Haskell* is the purely functional programming language of our choice
to implement Mikrokosmos, our \lambda-calculus interpreter. Its own
design is heavily influenced by the \lambda-calculus and is a
general-purpose language with a rich ecosystem and plenty of
consolidated libraries[fn:hackagelibs] in areas such as parsing,
testing or system interaction; matching the requisites of our
project. In the following sections, we describe this ecosystem in more
detail.

[fn:hackagelibs]: In the central package archive of the Haskell community,
Hackage, a categorized list of libraries can be found: https://hackage.haskell.org/packages/

**** History of Haskell                                           :ignore:
In the 1980s, many lazy programming languages were independently being
written by researchers such as /Miranda/, /Lazy ML/, /Orwell/, /Clean/
or /Daisy/. All of them were similar in expressive power, but their
differences were holding back the efforts to communicate ideas on
functional programming.  A comitee was created in 1987 with the
mission of designing a common lazy functional language. Several
versions of the language were developed, and the first standarized
reference of the language was published in the *Haskell 98 Report*,
whose revised version can be read on cite:haskell98. Its more popular
implementation is the *Glasgow Haskell Compiler (GHC)*; an open source
compiler written in Haskell and C. The complete history of Haskell and
its design decisions is detailed on cite:hudak07_haskell.

**** Haskell's properties                                         :ignore:
Haskell is

 1. *strongly and statically typed*, meaning that it only compiles
    well-typed programs and it does not allow implicit type
    casting. The compiler will generate an error if a term is
    non-typeable.

 2. *lazy*, with /non-strict semantics/, meaning that it will not
    evaluate a term or the argument of a function until it is needed.
    In cite:hughes89, John Hughes, codesigner of the language, argues
    for the benefits of a lazy functional language, which could solve
    the traditional efficiency problems on functional programming.

 3. *purely functional*. As the evaluation order is demand-driven and
    not explicitly known, it is not possible in practice to perform
    ordered input/output actions or any other side-effects by relying
    on the evaluation order. This helps modularity of the code,
    testing and verfication.

 4. *referentially transparent*. As a consequence of its purity, every
    term on the code could be replaced by its definition without
    changing the global meaning of the program. This allows equational
    reasoning with rules that are directly derived from \lambda-calculus.

 5. based on *System F\omega* with some restrictions. Crucially, it
    implements *System F* adding quantification over type operators
    even if it does not allow abstraction on type operators. The GHC
    Haskell compiler, however, allows the user the ability to activate
    extensions that implement dependent types.
    # https://stackoverflow.com/a/21220357/2552681

#+ATTR_LATEX: :options [A first example in Haskell]
#+BEGIN_exampleth
This example shows the basic syntax and how its type system and
its implicit laziness can be used.

#+BEGIN_SRC haskell
-- The type of the term can be declared.
-- Polymorphic type variables are allowed.
id :: a -> a
-- Functions are defined equationally
id x = x
-- This definition performs short circuit evaluation thanks
-- to laziness. The unused argument can be omitted.
(&&) :: Bool -> Bool -> Bool
True  && x = x
False && _ = False
-- Laziness also allows infinite data structures.
-- We can define the list of all natural numbers.
nats :: [Integer]
nats = 1 : map (+1) nats
#+END_SRC
#+END_exampleth

**** Haskell's syntax                                             :ignore:
Where most imperative languages use semicolons to separate sequential
commands, Haskell has no notion of sequencing, and programs are
written in a purely declarative way. A Haskell program essentially
consist on a series of definitions (of both types and terms) and type
declarations. The following example shows the definition of a binary
tree and its preorder as

#+BEGIN_SRC haskell
-- A tree is either empty or a node with two subtrees.
data Tree a = Empty | Node a (Tree a) (Tree a)
-- The preorder function takes a tree and returns a list
preorder :: Tree a -> [a]
preorder Empty            = []
preorder (Node x lft rgt) = preorder lft ++ [x] ++ preorder rgt
#+END_SRC

We can see on the previous example that function definitions allow
/pattern matching/, that is, data constructors can be used in
definitions to decompose values of the type. This increases readability
when working with algebraic data types.

While infix operators are allowed, function application is
left-associative in general. Definitions using partial application are
allowed, meaning that functions on multiple arguments can use currying
and can be passed only one of its arguments to define a new
function. For example, a function that squares every number on a list
could be written in two ways as

#+BEGIN_SRC haskell
squareList :: [Int] -> [Int]
squareList list = map square list
squareList' :: [Int] -> [Int]
squareList' = map square
#+END_SRC

where the second one, because of its simplicity, is usually
preferred. 

**** Type classes, monads                                         :ignore:
A characteristic piece of Haskell are *type classes*, which allow 
defining common interfaces for different types. In the following
example, we define =Monad= as the type class of types with suitably
typed =return= and =bind= operators.

#+BEGIN_SRC haskell
class Monad m where
  return :: a   -> m a
  (>>=)  :: m a -> (a -> m b) -> m b
#+END_SRC

And lists, for example, are monads in this sense.

#+BEGIN_SRC haskell
instance Monad [] where
  return x = [x]               -- returns a one-element list
  xs >>= f = concat (map f xs) -- map and concatenation
#+END_SRC

Haskell uses monads in varied forms. They are used in I/O, error
propagation and stateful computations. Another characteristical syntax
bit of Haskell is the =do= notation, which provides a nicer, cleaner
way to work with types that happen to be monads. The following example
uses the list monad to compute the list of Pythagorean triples.

#+BEGIN_SRC haskell
pythagorean = do
  a <- [1..]               -- let a be any natural
  b <- [1..a]              -- let b be a natural between 1 and a
  c <- [1..b]              -- let c be a natural between 1 and b
  guard (a^2 == b^2 + c^2) -- filter the list
  return (a,b,c)           -- return matching tuples
#+END_SRC

Note that this list is infinite. As the language is lazy, this does not
represent a problem: the list will be evaluated only on demand.

For a more detailed treatment of monads, and their relation to
categorical monads, see the chapter on Category Theory and the chapter
on Type Theory, where we will program with monads in Agda.

# [[http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.22.2636][CiteSeerX — Faking It: Simulating Dependent Types in Haskell]]
*** Cabal, Stack and Haddock
The Mikrokosmos documentation as a Haskell library is included
in its own code. It uses *Haddock*, a tool that generates documentation
from annotated Haskell code; it is the /de facto/ standard in for Haskell
software.

Dependencies and packaging details for Mikrokosmos are specified in
a file distributed with the source code called =mikrokosmos.cabal=.
It is used by the package managers *stack* and *cabal* to provide the
necessary libraries even if they are not available system-wide. The
*stack* tool is also used to package the software, which is uploaded
to /*Hackage*/.

*** Testing
*Tasty* is the Haskell testing framework of our choice for this
project. It allows the user to create a comprehensive test suite
combining multiple types of tests. The Mikrokosmos code is testing
using the following techniques

  * *unit tests*, in which individual core functions are tested
    independently of the rest of the application;

  * *property-based testing*, in which multiple test cases are
    created automatically in order to verfiy that a specified
    property always holds.

  * *golden tests*, a special case of unit tests in which the expected
    results of an IO action, as described on a file, are checked to
    match the actual ones.
    
We are using the *HUnit* library for unit tests. It tests particular
cases of type inference, unification and parsing. The following is an
example of unit test, as found in =tests.hs=. It checks that the type
inference of the identity term is correct.

#+BEGIN_SRC haskell
-- Checks that the type of λx.x is exactly A → A
testCase "Identity type inference" $
  typeinference (Lambda (Var 1)) @?= Just (Arrow (Tvar 0) (Tvar 0))
#+END_SRC

We are using the *QuickCheck* library for property-based tests. It
tests transformation properties of lambda expressions. In the following
example, it tests that any deBruijn expression keeps is meaning when
is translated into a \lambda-term.

#+BEGIN_SRC haskell
-- Tests that naming lambda expressions does not alter their meaning
QC.testProperty "Expression -> named -> expression" $
  \expr -> toBruijn emptyContext (nameExp expr) == expr
#+END_SRC

We are using the *tasty-golden* package for golden tests.  Mikrokosmos
can be passed a file as an argument to interpret it and show only the
results. This feature is used to create a golden test in which the
correct interpretation of a file.  This file is called =testing.mkr=,
and contains library definitions and multiple tests. Its expected
output is =testing.golden=. For example, the following Mikrokosmos
code can be found on the file

#+BEGIN_SRC haskell
:types on
caseof (inr 3) (plus 2) (mult 2)
#+END_SRC

and the expected output is

#+BEGIN_SRC haskell
-- types: on
-- λa.λb.(a (a (a (a (a (a b)))))) ⇒ 6 :: (A → A) → A → A
#+END_SRC

*** Version control and continuous integration
Mikrokosmos uses *git* as its version control system and the code,
which is licensed under GPLv3, can be publicly accessed on the
following GitHub repository:

#+begin_center
https://github.com/M42/mikrokosmos
#+end_center

Development takes place on the =development= git branch and permanent
changes are released into the =master= branch. Some more minor
repositories have been used in the development; they directly
depend on the main one

 * https://github.com/m42/mikrokosmos-js
 * https://github.com/M42/jupyter-mikrokosmos
 * https://github.com/M42/mikrokosmos-lib

The code uses the *Travis CI* continuous integration system to run
tests and check that the software builds correctly after each change
and in a reproducible way on a fresh Linux installation provided by
the service.
** Implementation of \lambda-expressions
*** De Bruijn indexes
Nicolaas Govert *De Bruijn* proposed in cite:debruijn81 a way of defining \lambda-terms modulo
\alpha-conversion based on indices.  The main idea of De Bruijn
indices is to remove all variables from binders and replace every
variable on the body of an expression with a number, called /index/,
representing the number of \lambda-abstractions in scope between the
ocurrence and its binder.

Consider the following example, the \lambda-term
\[ \lambda x.(\lambda y.\ y (\lambda z.\ y z)) (\lambda t.\lambda z.\ t x)
\]
can be written with de Bruijn indices as
\[
\lambda\ (\lambda(1 \lambda(2 1))\ \lambda\lambda(2 3)\ ).
\]

De Bruijn also proposed a notation for the \lambda-calculus
changing the order of binders and \lambda-applications.  A review on
the syntax of this notation, its advantages and De Bruijn indexes, can be found in
cite:kamareddine01. In this section, we are going to describe De Bruijn
indexes but preserve the usual notation of \lambda-terms; that is, the /De Bruijn/
/indexes/ and the /De Bruijn notation/ are different concepts and we are going to
use only the former.

#+attr_latex: :options [De Bruijn indexed terms]
#+begin_definition
We define recursively the set of \lambda-terms using de Bruijn notation
following this BNF
\[ \mathtt{Exp} ::= \mathbb{N}
 \mid (\lambda\ \mathtt{Exp})
 \mid (\mathtt{Exp}\ \mathtt{Exp})
\]
#+end_definition

Our internal definition closely matches the formal one. The names of
the constructors here are =Var=, =Lambda= and =App=:

#+BEGIN_SRC haskell
-- | A lambda expression using DeBruijn indexes.
data Exp = Var Integer -- ^ integer indexing the variable.
         | Lambda Exp  -- ^ lambda abstraction
         | App Exp Exp -- ^ function application
         deriving (Eq, Ord)
#+END_SRC

This notation avoids the need for the Barendregt's variable convention and
the \alpha-reductions. It will be useful to implement \lambda-calculus without
having to worry about the specific names of variables.

*** Substitution
We define the [[*Free and bound variables, substitution][substitution]] operation needed for the [[*\beta-reduction][\beta-reduction]] on
de Bruijn indices. In order to define the substitution of the n-th
variable by a \lambda-term $P$ on a given term, we must

 * find all the ocurrences of the variable. At each level of scope
   we are looking for the successor of the number we were looking
   for before.

 * decrease the higher variables to reflect the disappearance of
   a lambda.

 * replace the ocurrences of the variables by the new term, taking
   into account that free variables must be increased to avoid them
   getting captured by the outermost lambda terms. 

In our code, we apply =subs= to any expression. When it is applied to
a \lambda-abstraction, the index and the free variables of the
replaced term are increased with =incrementFreeVars=; whenever it is
applied to a variable, the previous cases are taken into consideration.

#+BEGIN_SRC haskell
-- | Substitutes an index for a lambda expression
subs :: Integer -> Exp -> Exp -> Exp
subs n p (Lambda e) = Lambda (subs (n+1) (incrementFreeVars 0 p) e)
subs n p (App f g)  = App (subs n p f) (subs n p g)
subs n p (Var m)
  | n == m    = p         -- The lambda is replaced directly  
  | n <  m    = Var (m-1) -- A more exterior lambda decreases a number
  | otherwise = Var m     -- An unrelated variable remains untouched
#+END_SRC

Then \beta-reduction can be defined using this =subs= function.

#+BEGIN_SRC haskell
betared :: Exp -> Exp
betared (App (Lambda e) x) = substitute 1 x e
betared e = e
#+END_SRC

*** De Bruijn-terms and \lambda-terms
The internal language of the interpreter uses de Bruijn expressions,
while the user interacts with it using lambda expressions with alphanumeric
variables. Our definition of a \lambda-expression with variables will be
used in parsing and output formatting.

#+BEGIN_SRC haskell
data NamedLambda = LambdaVariable String                    
                 | LambdaAbstraction String NamedLambda     
                 | LambdaApplication NamedLambda NamedLambda
#+END_SRC

**** Lambda to deBruijn                                           :ignore:
The translation from a natural \lambda-expression to de Bruijn notation
is done using a dictionary which keeps track of the bounded variables

#+BEGIN_SRC haskell
tobruijn :: Map.Map String Integer -- ^ names of the variables used
         -> Context                -- ^ names already binded on the scope
         -> NamedLambda            -- ^ initial expression
         -> Exp
-- Every lambda abstraction is inserted in the variable dictionary,
-- and every number in the dictionary increases to reflect we are entering
-- a deeper context.
tobruijn d context (LambdaAbstraction c e) = 
     Lambda $ tobruijn newdict context e
        where newdict = Map.insert c 1 (Map.map succ d)

-- Translation distributes over applications.
tobruijn d context (LambdaApplication f g) = 
     App (tobruijn d context f) (tobruijn d context g)

-- We look for every variable on the local dictionary and the current scope.
tobruijn d context (LambdaVariable c) =
  case Map.lookup c d of
    Just n  -> Var n
    Nothing -> fromMaybe (Var 0) (MultiBimap.lookupR c context)
#+END_SRC

**** deBruijn to Lambda                                           :ignore:
while the translation from a de Bruijn expression to a natural one is done
considering an infinite list of possible variable names and keeping a list
of currently-on-scope variables to name the indices.

#+BEGIN_SRC haskell
-- | An infinite list of all possible variable names 
-- in lexicographical order.
variableNames :: [String]
variableNames = concatMap (`replicateM` ['a'..'z']) [1..]

-- | A function translating a deBruijn expression into a 
-- natural lambda expression.
nameIndexes :: [String] -> [String] -> Exp -> NamedLambda
nameIndexes _    _   (Var 0) = LambdaVariable "undefined"
nameIndexes used _   (Var n) = 
  LambdaVariable (used !! pred (fromInteger n))
nameIndexes used new (Lambda e) = 
  LambdaAbstraction (head new) (nameIndexes (head new:used) (tail new) e)
nameIndexes used new (App f g) = 
  LambdaApplication (nameIndexes used new f) (nameIndexes used new g)
#+END_SRC

*** Evaluation
As we proved on Corollary [[cor-leftmosttheorem]], the leftmost reduction
strategy will find the leftmost reduction strategy if it
exists. Consequently, we will implement it using a function that
simply applies the leftmost possible reductions at each step. This
will allow us to show how the interpreter performs step-by-step
evaluations to the final user, as discussed in the [[*Verbose mode][verbose mode]] section.

#+BEGIN_SRC haskell
-- | Simplifies the expression recursively.
-- Applies only one parallel beta reduction at each step.
simplify :: Exp -> Exp
simplify (Lambda e)           = Lambda (simplify e)
simplify (App (Lambda f) x)   = betared (App (Lambda f) x)
simplify (App (Var e) x)      = App (Var e) (simplify x)
simplify (App a b)            = App (simplify a) (simplify b)
simplify (Var e)              = Var e

-- | Applies repeated simplification to the expression until it stabilizes and
-- returns all the intermediate results.
simplifySteps :: Exp -> [Exp]
simplifySteps e
  | e == s    = [e]
  | otherwise = e : simplifySteps s
  where s = simplify e
#+END_SRC

From the code we can see that the evaluation finishes whenever the
expression stabilizes. This can happen in two different cases

  * there are no more possible \beta-reductions, and the algorithm
    stops.
  * \beta-reductions do not change the expression. The computation
    would lead to an infinite loop, so it is immediately stopped.
    An common example of this is the \lambda-term $(\lambda x.x x)(\lambda x.x x)$.

*** Principal type inference
The interpreter implements the [[*Unification and type inference][unification and type inference]] algorithms
described in Lemma [[lemma-unification]] and Theorem [[thm-typeinfer]]. Their
recursive nature makes them very easy to implement directly on Haskell.

**** Type templates and substitutions                             :ignore:
We implement a simply-typed lambda calculus with [[*Curry-style types][Curry-style typing]]
and type templates. Our type system has

  * an unit type;
  * a void type;
  * product types;
  * union types;
  * and function types.

#+BEGIN_SRC haskell
-- | A type template is a free type variable or an arrow between two
-- types; that is, the function type.
data Type = Tvar Variable
          | Arrow Type Type
          | Times Type Type
          | Union Type Type
          | Unitty
          | Bottom
          deriving (Eq)
#+END_SRC

We will work with substitutions on type templates. They can be directly
defined as functions from types to types. A basic substitution that
inserts a given type on the place of a variable will be our building
block for more complex ones.

#+BEGIN_SRC haskell
type Substitution = Type -> Type

-- | A basic substution. It changes a variable for a type
subs :: Variable -> Type -> Substitution
subs x typ (Tvar y)
  | x == y    = typ
  | otherwise = Tvar y
subs x typ (Arrow a b) = Arrow (subs x typ a) (subs x typ b)
subs x typ (Times a b) = Times (subs x typ a) (subs x typ b)
subs x typ (Union a b) = Union (subs x typ a) (subs x typ b)
subs _ _ Unitty = Unitty
subs _ _ Bottom = Bottom
#+END_SRC

**** Unification                                                  :ignore:
Unification will be implemented making extensive use of the =Maybe=
monad. If the unification fails, it will return an error value, and
the error will be propagated to the whole computation. The algorithm
is exactly the same that was defined in Lemma [[lemma-unification]].

#+BEGIN_SRC haskell
-- | Unifies two types with their most general unifier. Returns the substitution
-- that transforms any of the types into the unifier.
unify :: Type -> Type -> Maybe Substitution
unify (Tvar x) (Tvar y)
  | x == y    = Just id
  | otherwise = Just (subs x (Tvar y))
unify (Tvar x) b
  | occurs x b = Nothing
  | otherwise  = Just (subs x b)
unify a (Tvar y)
  | occurs y a = Nothing
  | otherwise  = Just (subs y a)
unify (Arrow a b) (Arrow c d) = unifypair (a,b) (c,d)
unify (Times a b) (Times c d) = unifypair (a,b) (c,d)
unify (Union a b) (Union c d) = unifypair (a,b) (c,d)
unify Unitty Unitty = Just id
unify Bottom Bottom = Just id
unify _ _ = Nothing

-- | Unifies a pair of types
unifypair :: (Type,Type) -> (Type,Type) -> Maybe Substitution
unifypair (a,b) (c,d) = do
  p <- unify b d
  q <- unify (p a) (p c)
  return (q . p)
#+END_SRC

**** Type inference                                               :ignore:
The type inference algorithm is more involved. It takes a list
of fresh variables, a type context, a lambda expression and a
constraint on the type, expressed as a type template. It outputs
a substitution. As an example, the following code shows the type
inference algorithm for function types.

#+BEGIN_SRC haskell
-- | Type inference algorithm. Infers a type from a given context and expression
-- with a set of constraints represented by a unifier type. The result type must
-- be unifiable with this given type.
typeinfer :: [Variable] -- ^ List of fresh variables
          -> Context    -- ^ Type context
          -> Exp        -- ^ Lambda expression whose type has to be inferred
          -> Type       -- ^ Constraint
          -> Maybe Substitution

typeinfer (x:vars) ctx (App p q) b = do -- Writing inside the Maybe monad.
  sigma <- typeinfer (evens vars) ctx                  p (Arrow (Tvar x) b)
  tau   <- typeinfer (odds  vars) (applyctx sigma ctx) q (sigma (Tvar x))
  return (tau . sigma)
  where
    -- The list of fresh variables has to be split into two
    odds [] = []
    odds [_] = []
    odds (_:e:xs) = e : odds xs
    evens [] = []
    evens [e] = [e]
    evens (e:_:xs) = e : evens xs
#+END_SRC

The final form of the type inference algorithm will use a
normalization algorithm shortening the type names and will apply the
type inference to the empty type context. The complete code can be
found on the [[*Mikrokosmos complete code][Appendix]].

**** Gentzen deduction trees                                      :ignore:
A generalized version of the type inference algorithm is used
to generate derivation trees from terms, as it was described in
[[*Propositions as types][Propositions as types]].

In order to draw these diagrams in Unicode characters, a data type for
character blocks has been defined. A monoidal structure is defined
over them; blocks can be joined vertically and horizontally; and
every deduction step can be drawn independently.

#+BEGIN_SRC haskell
newtype Block = Block { getBlock :: [String] }
  deriving (Eq, Ord)

instance Monoid Block where
  mappend = joinBlocks -- monoid operation, joins blocks vertically
  mempty  = Block [[]] -- neutral element

-- Type signatures
joinBlocks :: Block -> Block -> Block
stackBlocks :: String -> Block -> Block -> Block
textBlock :: String -> Block
deductionBlock :: Block -> String -> [Block] -> Block
box :: Block -> Block
#+END_SRC

** User interaction
*** Monadic parser combinators
A common approach to building parsers in functional programming is to
model parsers as functions. Higher-order functions on parsers act as
/combinators/, which are used to implement complex parsers in a
modular way from a set of primitive ones. In this setting, parsers
exhibit a monad algebraic structure, which can be used to simplify
the combination of parsers. A technical report on *monadic parser combinators*
can be found on cite:hutton96.

The use of monads for parsing is discussed firstly in cite:Wadler85,
and later in cite:Wadler90 and cite:hutton98. The parser type is
defined as a function taking a =String= and returning a list of pairs,
representing a successful parse each. The first component of the pair
is the parsed value and the second component is the remaining
input. The Haskell code for this definition is

#+BEGIN_SRC haskell
newtype Parser a = Parser (String -> [(a,String)])

parse :: Parser a -> String -> [(a,String)]
parse (Parser p) = p

instance Monad Parser where
  return x = Parser (\s -> [(x,s)])
  p >>= q  = Parser (\s -> 
               concat [parse (q x) s' | (x,s') <- parse p s ])
#+END_SRC

where the monadic structure is defined by =bind= and =return=. Given a
value, the =return= function creates a parser that consumes no input
and simply returns the given value. The =>>== function acts as a sequencing
operator for parsers. It takes two parsers and applies the second one
over the remaining inputs of the first one, using the parsed values on
the first parsing as arguments.

An example of primitive *parser* is the =item= parser, which consumes a
character from a non-empty string. It is written in Haskell code as

#+BEGIN_SRC haskell
item :: Parser Char
item = Parser (\s -> case s of 
                       "" -> []
                       (c:s') -> [(c,s')])
#+END_SRC

and an example of *parser combinator* is the =many= function, which
creates a parser that allows one or more applications of the given
parser

#+BEGIN_SRC haskell
many :: Paser a -> Parser [a]
many p = do
  a  <- p
  as <- many p
  return (a:as)
#+END_SRC

in this example =many item= would be a parser consuming all characters
from the input string.

*** Parsec
*Parsec* is a monadic parser combinator Haskell library described in
cite:leijen2001. We have chosen to use it due to its simplicity and
extensive documentation. As we expect to use it to parse user live
input, which will tend to be short, performance is not a critical
concern. A high-performace library supporting incremental parsing,
such as *Attoparsec* cite:attoparsec, would be suitable otherwise.
*** Verbose mode
As we explained previously on the Evaluation section, the simplification
can be analyzed step-by-step. The interpreter allows us to see the
complete evaluation when the =verbose= mode is activated. To activate
it, we can execute =:verbose on= in the interpreter.

The difference can be seen on the following example.

#+BEGIN_EXAMPLE
mikro> plus 1 2
λa.λb.(a (a (a b))) ⇒ 3

mikro> :verbose on
verbose: on
mikro> plus 1 2
((plus 1) 2)
((λλλλ((4 2) ((3 2) 1)) λλ(2 1)) λλ(2 (2 1)))
(λλλ((λλ(2 1) 2) ((3 2) 1)) λλ(2 (2 1)))
λλ((λλ(2 1) 2) ((λλ(2 (2 1)) 2) 1))
λλ(λ(3 1) (λ(3 (3 1)) 1))
λλ(2 (λ(3 (3 1)) 1))
λλ(2 (2 (2 1)))

λa.λb.(a (a (a b))) ⇒ 3
#+END_EXAMPLE

The interpreter output can be colored to show specifically where it
is performing reductions. It is activated by default, but can be deactivated
by executing =:color off=. The following code implements /verbose mode/ in both
cases.

#+BEGIN_SRC haskell
-- | Shows an expression, coloring the next reduction if necessary
showReduction :: Exp -> String
showReduction (Lambda e)         = "λ" ++ showReduction e
showReduction (App (Lambda f) x) = betaColor (App (Lambda f) x)
showReduction (Var e)            = show e
showReduction (App rs x)         = 
  "(" ++ showReduction rs ++ " " ++ showReduction x ++ ")"
showReduction e                  = show e
#+END_SRC

*** SKI mode
Every \lambda-term can be written in terms of SKI combinators.
SKI combinator expressions can be defined as a binary tree having
S, K, and I as possible leafs.

#+BEGIN_SRC haskell
data Ski = S | K | I | Comb Ski Ski
#+END_SRC

The SKI-abstraction and bracket abstraction algorithms are implemented
on Mikrokosmos, and they can be used by activating the /ski mode/ with
=:ski on=. When this mode is activated, every result is written in terms
of SKI combinators.

#+begin_example
mikro> 2
λa.λb.(a (a b)) ⇒ S(S(KS)K)I ⇒ 2
mikro> and
λa.λb.((a b) a) ⇒ SSK ⇒ and
#+end_example

The code implementing these algorithms follows directly from the
theoretical version in cite:Hindley08.

#+BEGIN_SRC haskell
-- | Bracket abstraction of a SKI term, as defined in Hindley-Seldin
-- (2.18).
bracketabs :: String -> Ski -> Ski
bracketabs x (Cte y) = if x == y then I else Comb K (Cte y)
bracketabs x (Comb u (Cte y))
  | freein x u && x == y = u
  | freein x u           = Comb K (Comb u (Cte y))
  | otherwise            = Comb (Comb S (bracketabs x u)) (bracketabs x (Cte y))
bracketabs x (Comb u v)
  | freein x (Comb u v)  = Comb K (Comb u v)
  | otherwise            = Comb (Comb S (bracketabs x u)) (bracketabs x v)
bracketabs _ a           = Comb K a


-- | SKI abstraction of a named lambda term. From a lambda expression
-- creates a SKI equivalent expression. The following algorithm is a
-- version of the algorithm (9.10) on the Hindley-Seldin book.
skiabs :: NamedLambda -> Ski
skiabs (LambdaVariable x)      = Cte x
skiabs (LambdaApplication m n) = Comb (skiabs m) (skiabs n)
skiabs (LambdaAbstraction x m) = bracketabs x (skiabs m)
#+END_SRC
 
** Usage
*** Installation
The complete Mikrokosmos suite is divided in multiple parts:

 1) the *Mikrokosmos interpreter*, written in Haskell;
 2) the *Jupyter kernel*, written in Python;
 3) the *CodeMirror Lexer*, written in Javascript;
 4) the *Mikrokosmos libraries*, written in the Mikrokosmos language;
 5) the *Mikrokosmos-js* compilation, which can be used in web browsers.

These parts will be detailed on the following sections. A system that
already satisfies all dependencies (Stack, Pip and Jupyter), can install
Mikrokosmos using the following script, which is detailed on this section

#+BEGIN_SRC sh
# Mikrokosmos interpreter
stack install mikrokosmos
# Jupyter kernel for Mikrokosmos
sudo pip install imikrokosmos
# Libraries
git clone https://github.com/M42/mikrokosmos-lib.git ~/.mikrokosmos
#+END_SRC

**** Mikrokosmos interpreter :ignore:
The *Mikrokosmos interpreter* is listed in the central Haskell
package archive, /Hackage/ [fn:hackage]. The packaging of Mikrokosmos
has been done using the *cabal* tool; and the configuration of the
package can be read on the file =mikrokosmos.cabal= on the Mikrokosmos
code. As a result, Mikrokosmos can be installed using the *cabal* and
*stack* Haskell package managers. That is,

#+BEGIN_SRC sh
# With cabal
cabal install mikrokosmos
# With stack
stack install mikrokosmos
#+END_SRC

**** Mikrokosmos Jupyter kernel :ignore:
The *Mikrokosmos Jupyter kernel* is listed in the central Python
package archive. Jupyter is a dependency of this kernel, which only
can be used in conjunction with it. It can be installed with the
=pip= package manager as

#+BEGIN_SRC sh
sudo pip install imikrokosmos
#+END_SRC

and the installation can be checked by listing the available Jupyter
kernels with

#+BEGIN_SRC sh
jupyter kernelspec list
#+END_SRC

**** Mikrokosmos libraries :ignore:
The *Mikrokosmos libraries* can be downloaded directly from its GitHub
repository. [fn:mikrokosmoslibgit] They have to be placed under
=~/.mikrokosmos= if we want them to be locally available or under
=/usr/lib/mikrokosmos= if we want them to be globally available.

#+BEGIN_SRC sh
git clone https://github.com/M42/mikrokosmos-lib.git ~/.mikrokosmos
#+END_SRC

**** Complete script :ignore:
The following script installs the complete Mikrokosmos suite on a
fresh system. It has been tested under =Ubuntu 16.04.3 LTS (Xenial
Xerus)=.

#+BEGIN_SRC sh
# 1. Installs Stack, the Haskell package manager
wget -qO- https://get.haskellstack.org | sh
STACK=$(which stack)

# 2. Installs the ncurses library, used by the console interface
sudo apt install libncurses5-dev libncursesw5-dev

# 3. Installs the Mikrokosmos interpreter using Stack
$STACK setup
$STACK install mikrokosmos

# 4. Installs the Mikrokosmos standard libraries
sudo apt install git
git clone https://github.com/M42/mikrokosmos-lib.git ~/.mikrokosmos

# 5. Installs the IMikrokosmos kernel for Jupyter
sudo apt install python3-pip
sudo -H pip install --upgrade pip
sudo -H pip install jupyter
sudo -H pip install imikrokosmos
#+END_SRC

[fn:hackage]: Hackage can be accesed in: http://hackage.haskell.org/
and the Mikrokosmos package can be found in https://hackage.haskell.org/package/mikrokosmos
[fn:mikrokosmoslibgit]: The repository can be accessed in: https://github.com/M42/mikrokosmos-lib.git

*** Mikrokosmos interpreter
Once installed, the Mikrokosmos \lambda interpreter can be opened from
the terminal with the =mikrokosmos= command. It will enter a /read-eval-print loop/
where \lambda-expressions and interpreter commands can be evaluated.

#+BEGIN_EXAMPLE
$> mikrokosmos
Welcome to the Mikrokosmos Lambda Interpreter!
Version 0.5.0. GNU General Public License Version 3.
mikro> _
#+END_EXAMPLE

The interpreter evaluates every line as a lambda expression. Examples
on the use of the interpreter can be read on the following
sections. Apart from the evaluation of expressions, the interpreter
accepts the following commands

  * =:quit= and =:restart=, stop the interpreter;
  * =:verbose= activates /verbose mode/;
  * =:ski= activates /SKI mode/;
  * =:types= changes between untyped and simply typed \lambda-calculus;
  * =:color= deactivates colored output;
  * =:load= loads a library.

The Figure [[mikrosession]] is an example session on the mikrokosmos interpreter.

#+caption: Mikrokosmos interpreter session.
#+name: mikrosession
[[./images/mikrosession.png]]

*** Jupyter kernel
The *Jupyter Project* cite:jupyter is an open source project providing
support for interactive scientific computing. Specifically, the
Jupyter Notebook provides a web application for creating interactive
documents with live code and visualizations. 

We have developed a Mikrokosmos kernel for the Jupyter Notebook,
allowing the user to write and execute arbitrary Mikrokosmos code
on this web application. An example session can be seen on Figure
[[jupytersession]].

#+caption: Jupyter notebook Mikrokosmos session.
#+name: jupytersession
[[./images/jupytersession.png]]

The implementation is based on the =pexpect= library for Python.  It
allows direct interaction with any REPL and collects its results.
Specifically, the following Python lines represent the central idea of
this implementation

#+BEGIN_SRC python
# Initialization
mikro = pexpect.spawn('mikrokosmos')
mikro.expect('mikro>')

# Interpreter interaction
# Multiple-line support
output = ""
for line in code.split('\n'):
    # Send code to mikrokosmos
    self.mikro.sendline(line)
    self.mikro.expect('mikro> ')

    # Receive and filter output from mikrokosmos
    partialoutput = self.mikro.before
    partialoutput = partialoutput.decode('utf8')
    output = output + partialoutput
#+END_SRC

A =pip= installable package has been created following the
Python Packaging Authority guidelines. [fn:pypaguide] This allows
the kernel to be installed directly using the =pip= python package manager.

#+BEGIN_SRC bash
sudo -H pip install imikrokosmos
#+END_SRC

[fn:pypaguide]: The PyPA packaging user guide can be found in its official
page: https://packaging.python.org/

*** CodeMirror lexer
*CodeMirror* [fn:codemirror] is a text editor for the browser
implemented in Javascript. It is used internally by the Jupyter
Notebook.

A CodeMirror lexer for Mikrokosmos has been written. It uses
Javascript regular expressions and signals the ocurrence of any kind
of operator to CodeMirror. It enables syntax highlighting for Mikrokosmos
code on Jupyter Notebooks. It comes bundled with the kernel specification
and no additional installation is required.

#+BEGIN_SRC javascript
	CodeMirror.defineSimpleMode("mikrokosmos", {
	    start: [
	    // Comments
	    {regex: /\#.*/,
            token: "comment"},
	    // Interpreter
            {regex: /\:load|\:verbose|\:ski|\:restart|\:types|\:color/,
            token: "atom"},
	    // Binding
	    {regex: /(.*?)(\s*)(=)(\s*)(.*?)$/,
	    token: ["def",null,"operator",null,"variable"]},
	    // Operators
	    {regex: /[=!]+/,
            token: "operator"},
	    ],
	    meta: {
		dontIndentStates: ["comment"],
		lineComment: "#"
	    }
	}
#+END_SRC

[fn:codemirror]: Documentation for CodeMirror can be found in its
official page: https://codemirror.net/

*** JupyterHub
*JupyterHub* manages multiple instances of independent single-user Jupyter
notebooks. We used it to serve Mikrokosmos notebooks and tutorials to
students studying \lambda-calculus.

In order to install Mikrokosmos on a server and use it as =root= user,
we need

  * to clone the libraries into =/usr/lib/mikrokosmos=. They should be
    available system-wide.

  * to install the Mikrokosmos interpreter into =/usr/local/bin=. In
    this case, we chose not to install Mikrokosmos from source, but simply
    copy the binaries and check the availability of the =ncurses= library.

  * to install the Mikrokosmos Jupyter kernel as usual.

Our server used a SSL certificate; and OAuth autentication via GitHub.
Mikrokosmos tutorials were installed for every student.

*** Calling Mikrokosmos from Javascript
The GHCjs[fn:ghcjs] compiler allows transpiling from Haskell to Javascript.
Its foreign function interface allows a Haskell function to be passed as
a continuation to a Javascript function.

A particular version of the =Main.hs= module of Mikrokosmos was written in
order to provide a =mikrokosmos= function, callable from Javascript. This
version includes the standard libraries automatically and reads blocks of
texts as independent Mikrokosmos commands. The relevant use of the foreign
function interface is showed in the following code

#+BEGIN_SRC haskell
foreign import javascript unsafe "mikrokosmos = $1"
    set_mikrokosmos :: Callback a -> IO ()
#+END_SRC

which provides =mikrokosmos= as a Javascript function once the code is
transpiled. In particular, the following is an example of how to call
Mikrokosmos from Javascript

#+BEGIN_SRC javascript
button.onclick = function () {
   editor.save();
   outputcode.getDoc().setValue(mikrokosmos(inputarea.value).mkroutput);
   textAreaAdjust(outputarea);
}
#+END_SRC

A small script has been written in Javascript to help with the task of
embedding Mikrokosmos into a web page. It and can be included directly
from

=https://m42.github.io/mikrokosmos-js/mikrobox.js=

using GitHub as a CDN. It will convert any HTML script tag written as
follows

#+BEGIN_SRC html
<div class="mikrojs-console">
<script type="text/mikrokosmos">
(λx.x)
... your code
</script>
</div>
#+END_SRC

into a CodeMirror pad where Mikrokosmos can be executed. The Mikrokosmos
tutorials are an example of this feature and can be seen on Figure [[mikrokosmosjstutorial]].

#+caption: Mikrokosmos embedded into a web page.
#+name: mikrokosmosjstutorial
[[./images/mikrokosmosjs.png]]

[fn:ghcjs]: The GHCjs documentation is available on its web page https://github.com/ghcjs/ghcjs

** Programming in the untyped \lambda-calculus
# Untyped \lambda-calculus in programming languages

This section explains how to use the untyped \lambda-calculus to
encode data structures and useful data, such as booleans, linked lists,
natural numbers or binary trees. All this is done on pure \lambda-calculus
avoiding the addition of new syntax or axioms.

This presentation follows the Mikrokosmos tutorial on \lambda-calculus, which
aims to teach how it is possible to program using untyped \lambda-calculus
without discussing technical topics such as those we have discussed on
the chapter on [[*Untyped \lambda-calculus][untyped \lambda-calculus]]. It also follows
the exposition on cite:selinger13 of the usual Church encodings.

All the code on this section is valid Mikrokosmos code.

*** Basic syntax
In the interpreter, \lambda-abstractions are written with the symbol =\=,
representing a \lambda. This is a convention used on some functional languages
such as Haskell or Agda. Any alphanumeric string can be a variable and
can be defined to represent a particular \lambda-term using the === operator.

As a first example, we define the identity function (=id=), function 
composition (=compose=) and a constant function on two arguments which
always returns the first one untouched (=const=).

#+BEGIN_SRC haskell
id = \x.x
compose = \f.\g.\x.f (g x)
const = \x.\y.x
#+END_SRC

Evaluation of terms will be denoted with presented as comments to the
code,

#+BEGIN_SRC haskell
compose id id
-- [1]: λa.a ⇒ id
#+END_SRC

It is important to notice that multiple argument functions are defined as
higher one-argument functions which return another functions as arguments.
These intermediate functions are also valid \lambda-terms. For example

#+BEGIN_SRC haskell
discard = const id
#+END_SRC

is a function that discards one argument and returns the identity, =id=.
This way of defining multiple argument functions is called the *currying*
of a function in honor to the american logician Haskell Curry in cite:haskell58.
It is a particular instance of a deeper fact: exponentials are defined
by the following adjunction
\[
\hom(A \times B, C) \cong \hom(A, \hom(B,C)).
\]

*** A technique on inductive data encoding
Over this presentation, we will implicitly use a technique on the
majority of our data encodings which allows us to write an encoding
for any algebraically inductive generated data. This technique is used
without comment on cite:selinger13 and represents the basis of what is
called the *Church encoding* of data in \lambda-calculus.

We start considering the usual inductive representation of
a data type with constructors, as we do when representing a
syntax with a BNF, for example,
\[
\mathtt{Nat} ::= \mathtt{Zero} \mid \mathtt{Succ}\ \mathtt{Nat}.
\]
Or, in general
\[
\mathtt{D} ::= C_1 \mid C_2 \mid C_3 \mid \dots
\]

It is not possible to directly encode constructors on
\lambda-calculus. Even if we were able, they would have, in theory, no
computational content; the data structure would not
be reduced under any \lambda-term, and we would need at least the 
ability to pattern match on the constructors to define functions
on them. Our \lambda-calculus would need to be extended with
additional syntax for every new data structure.

This technique, instead, defines a data term as a function on
multiple arguments representing the missing constructors. In our example, 
the number $2$, which would be written as $\mathtt{Succ}(\mathtt{Succ}(\mathtt{Zero}))$,
would be encoded as
\[
2 = \lambda s.\ \lambda z.\ s (s (z)).
\]

In general, any instance of the data structure $\mathtt{D}$ would be encoded as a
\lambda-expression depending on all its constuctors
\[
\lambda c_{1}.\ \lambda c_{2}.\ \lambda c_{3}.\ \dots\ \lambda c_{n}. (\textit{term}).
\]

This acts as the definition of an initial algebra over the
constructors and lets us compute by instantiating this algebra on
particular cases. Particular examples are described on the following
sections.
# Link to categories

*** Booleans
Booleans can be defined as the data generated by a pair of constuctors
\[\mathtt{Bool} ::= \mathtt{True} \mid \mathtt{False}.
\]

Consequently, the Church encoding of booleans takes these constructors as
arguments and defines

#+BEGIN_SRC haskell
true  = \t.\f.t
false = \t.\f.f
#+END_SRC

**** If-else interpretation                                       :ignore:
Note that =true= and =const= are exactly the same term up to
\alpha-conversion. The same thing happens with =false= and =alwaysid=.
The absence of types prevents us to make any effort to discriminate
between these two uses of the same \lambda-term. Another side-effect
of this definition is that our =true= and =false= terms can be interpreted
as binary functions choosing between two arguments, that is,

  * $\mathtt{true}(a,b) = a$
  * $\mathtt{false}(a,b) = b$

We can test this interpretation on the interpreter to get

#+BEGIN_SRC haskell
true id const
false id const
--- [1]: id
--- [2]: const
#+END_SRC

This inspires the definition of an =ifelse= combinator as the identity

#+BEGIN_SRC haskell
ifelse = \b.b
(ifelse true) id const
(ifelse false) id const
--- [1]: id
--- [2]: const
#+END_SRC

**** Logic gates                                                  :ignore:
The usual logic gates can be defined profiting from this interpretation
of booleans

#+BEGIN_SRC haskell
and = \p.\q.p q p
or = \p.\q.p p q
not = \b.b false true
xor = \a.\b.a (not b) b
implies = \p.\q.or (not p) q

xor true true
and true true
--- [1]: false
--- [2]: true
#+END_SRC

*** Natural numbers
**** Peano natural numbers                                        :ignore:
Our definition of natural numbers is inspired by the Peano natural numbers.
We use two constructors

 * zero is a natural number, written as Z;
 * the successor of a natural number is a natural number, written as S;

and the BNF we defined when discussing how to [[*A technique on inductive data encoding][encode inductive data]].

#+BEGIN_SRC haskell
0    = \s.\z.z
succ = \n.\s.\z.s (n s z)
#+END_SRC

This definition of =0= is trivial: given a successor function and a
zero, return zero. The successor function seems more complex, but
it uses the same underlying idea: given a number, a successor and a
zero, apply the successor to the interpretation of that number using
the same successor and zero.

We can then name some natural numbers as

#+BEGIN_SRC haskell
1 = succ 0
2 = succ 1
3 = succ 2
4 = succ 3
5 = succ 4
6 = succ 5
...
#+END_SRC

even if we can not define an infinite number of terms as we might wish.

**** Interpretation as higher-order functions                     :ignore:
The interpretation the natural number $n$ as a higher order function
is a function taking an argument =f= and applying them $n$ times over
the second argument.

#+BEGIN_SRC haskell
5 not true
4 not true
double = \n.\s.\z.n (compose s s) z
double 3
--- [1]: false
--- [2]: true
--- [3]: 6
#+END_SRC

**** Addition and multiplication                                  :ignore:
Addition $n+m$ applies the successor $m$ times to $n$; and multiplication
$nm$ applies the $n\text{-fold}$ application of the successor $m$ times to $0$.

#+BEGIN_SRC haskell
plus = \m.\n.\s.\z.m s (n s z)
mult = \m.\n.\s.\z.m (n s) z
plus 2 1
mult 2 4
--- [1]: 3
--- [2]: 8
#+END_SRC

*** The predecessor function and predicates on numbers
**** Predecessor                                                  :ignore:
The predecessor function is much more complex than the previous ones.
As we can see, it is not trivial how could we compute the predecessor
using the limited form of induction that Church numerals allow.

Stephen Kleene, one of the students of Alonzo Church only discovered
how to write the predecessor function after thinking about it for a
long time (and he only discovered it while a long visit at the
dentist's, which is the reason why this definition is often called the
*/wisdom tooth trick/*, see cite:crossley75). We will use a slightly
different version of the definition that does not depend on a pair
datatype.

We will start defining a /reverse composition/ operator, called
=rcomp=; and we will study what happens when it is composed to itself;
that is

#+BEGIN_SRC haskell
rcomp = \f.\g.\h.h (g f)
\f.3 (inc f)
\f.4 (inc f)
\f.5 (inc f)
--- [1]: λa.λb.λc.c (a (a (b a)))
--- [2]: λa.λb.λc.c (a (a (a (b a))))
--- [3]: λa.λb.λc.c (a (a (a (a (b a)))))
#+END_SRC

will allow us now to use the =b= argument to discard the first instance
of the =a= argument and return the same number wihtout the last constructor.
Thus, our definition of =pred= is

#+BEGIN_SRC haskell
pred = \n.\s.\z.(n (inc s) (\x.z) (\x.x))
#+END_SRC

**** Predicates                                                   :ignore:
From the definition of =pred=, some predicates on numbers can be
defined. The first predicate will be a function distinguishing a
successor from a zero. It will be user later to build more complex
ones. It is built by appliying a =const false= function =n= times to a
true constant. Only if it is applied =0= times, it will return a true
value.

#+BEGIN_SRC haskell
iszero = \n.(n (const false) true)
iszero 0
iszero 2
--- [1]: true
--- [2]: false
#+END_SRC

From this predicate, we can derive predicates on equality and ordering.

#+BEGIN_SRC haskell
leq = \m.\n.(iszero (minus m n))
eq  = \m.\n.(and (leq m n) (leq n m))
#+END_SRC

*** Lists and trees
We would need two constructors to represent a list: a =nil= signaling
the end of the list and a =cons=, joining an element to the head of
the list. An example of list would be
\[\mathtt{cons}\ 1\ (\mathtt{cons}\ 2\ (\mathtt{cons}\ 3\ \mathtt{nil})).\]

Our definition takes those two constructors into account
#+BEGIN_SRC haskell
nil  = \c.\n.n
cons = \h.\t.\c.\n.(c h (t c n))
#+END_SRC
and the interpretation of a list as a higher-order function is its
=fold= function, a function taking a binary operation and an initial
element and appliying the operation repeteadly to every element on
the list.

\[\mathtt{cons}\ 1\ (\mathtt{cons}\ 2\ (\mathtt{cons}\ 3\ \mathtt{nil}))
\overset{fold\ plus\ 0}\longrightarrow 
\mathtt{plus}\ 1\ (\mathtt{plus}\ 2\ (\mathtt{plus}\ 3\ 0)) = 6\]

The =fold= operation and some operations on lists can be defined
explicitly as

#+BEGIN_SRC haskell
fold = \c.\n.\l.(l c n)
sum  = fold plus 0
prod = fold mult 1
all  = fold and true
any  = fold or false
length = foldr (\h.\t.succ t) 0

sum (cons 1 (cons 2 (cons 3 nil)))
all (cons true (cons true (cons true nil)))
--- [1]: 6
--- [2]: true
#+END_SRC

**** Map and filter                                               :ignore:
The two most commonly used particular cases of fold and frequent examples
of the functional programming paradigm are =map= and =filter=.

  - The *map* function applies a function =f= to every element on a
    list.
  - The *filter* function removes the elements of the list that do not
    satisfy a given predicate. It /filters/ the list, leaving only
    elements that satisfy the predicate.

They can be defined as follows.

#+BEGIN_SRC haskell
map    = \f.(fold (\h.\t.cons (f h) t) nil)
filter = \p.(foldr (\h.\t.((p h) (cons h t) t)) nil)
#+END_SRC

On =map=, given a =cons h t=, we return a =cons (f h) t=; and given a
=nil=, we return a =nil=. On =filter=, we use a boolean to decide at
each step whether to return a list with a head or return the tail
ignoring the head.

#+BEGIN_SRC haskell
mylist = cons 1 (cons 2 (cons 3 nil))
sum (map succ mylist)
length (filter (leq 2) mylist)
--- [1]: 9
--- [2]: 2
#+END_SRC
**** Binary trees                                                 :ignore:
Lists have been defined using two constructors and *binary trees* will
be defined using the same technique. The only difference with lists is
that the =cons= constructor is replaced by a =node= constructor, which
takes two binary trees as arguments. That is, a binary tree is

 * an empty tree; or
 * a node, containing a label, a left subtree, and a right subtree.

Defining functions using a fold-like combinator is again very simple
due to the chosen representation. We need a variant
of the usual function acting on three arguments, the label, the right
node and the left node.

#+BEGIN_SRC haskell
-- Binary tree definition
node = \x.\l.\r.\f.\n.(f x (l f n) (r f n))
-- Example on natural numbers
mytree    = node 4 (node 2 nil nil) (node 3 nil nil)
triplesum = \a.\b.\c.plus (plus a b) c
mytree triplesum 0
--- [1]: 9
#+END_SRC

**** TODO The universal properties of fold, map and filter
*** Fixed points
A fixpoint combinator is a term representing a higher-order function
that, given any function =f=, solves the equation
\[
\mathtt{x = f\ x}
\]
for =x=, meaning that, if =fix f= is the fixpoint of =f=, the following
sequence of equations holds
\[
\mathtt{fix}\ f =
f (\mathtt{fix}\ f) =
f ( f (\mathtt{fix}\ f)) =
f ( f ( f (\mathtt{fix}\ f))) =
\dots
\]

Such a combinator actually exists; it can be defined and used as
#+BEGIN_SRC haskell
fix != (\f.(\x.f (x x)) (\x.f (x x)))
fix (const id)
--- [1]: id
#+END_SRC

Examples of its applications are a /factorial/ function or a
/fibonacci/ function, as in
#+BEGIN_SRC haskell
fact != fix (\f.\n.iszero n 1 (mult n (f (pred n))))
fib  != fix (\f.\n.iszero n 1 (plus (f (pred n)) (f (pred (pred n)))))
fact 3
fib 3
--- [1]: 6
--- [2]: 5
#+END_SRC
Note the use of =iszero= to stop the recursion.

The =fix= function cannot be evaluated without arguments into a closed
form, so we have to delay the evaluation of the expression when we
bind it using =!==. Our evaluation strategy, however, will always find
a way to reduce the term if it is possible, as we saw in Corollary
[[cor-leftmosttheorem]]; even if it has intermediate irreducible terms.

#+BEGIN_SRC haskell
fix              -- diverges
true  id fix     -- evaluates to id
false id fix     -- diverges
#+END_SRC

Other examples of the interpreter dealing with non terminating functions
include infinite lists as in the following examples, where we take the
first term of an infinite list without having to evaluate it
completely or compare an infinite number arising as the fix point of
the successor function with a finite number.

#+BEGIN_SRC haskell
-- Head of an infinite list of zeroes
head = fold const false
head (fix (cons 0))
-- Compare infinity with other numbers
infinity != fix succ
leq infinity 6
---- [1]: 0
---- [2]: false
#+END_SRC

These definitions unfold as

 * $\mathtt{fix\ (cons\ 0) = cons\ 0\ (cons\ 0\ (cons\ 0\ \dots))}$, an infinite list of zeroes;
 * $\mathtt{fix\ succ\ \ \ \ \ \ = succ\ (succ\ (succ\ \dots))}$, an infinite natural number.

** Programming in the simply typed \lambda-calculus
This section explains how to use the simply typed \lambda-calculus to
encode compound data structures and proofs in intuitionistic logic.
We will use the interpreter as a typed language and, at the same time,
as a proof assistant for the intuitionistic propositional logic.

This presentation of simply typed structures follows the Mikrokosmos
tutorial and the previous sections on [[*Simply typed \lambda-calculus][simply typed \lambda-calculus]].
All the code on this section is valid Mikrokosmos code.

*** Function types and typeable terms
Types can be activated with the commmand =:types on=. If types are activated,
the interpreter will [[*Principal type inference][infer]] the principal type every term before its evaluation.
The type will then be displayed after the result of the computation.

#+attr_latex: :options [Typed terms on Mikrokosmos]
#+begin_exampleth
The following are examples of already defined terms on lambda calculus and
their corresponding types. It is important to notice how our previously
defined booleans have two different types; while our natural numbers will
have all the same type except from zero, whose type is a generalization on
the type of the natural numbers.

#+begin_src haskell
id    --- [1]: λa.a ⇒ id, I, ifelse :: A → A
true  --- [2]: λa.λb.a ⇒ K, true :: A → B → A
false --- [3]: λa.λb.b ⇒ nil, 0, false :: A → B → B
0     --- [4]: λa.λb.b ⇒ nil, 0, false :: A → B → B
1     --- [5]: λa.λb.(a b) ⇒ 1 :: (A → B) → A → B
2     --- [6]: λa.λb.(a (a b)) ⇒ 2 :: (A → A) → A → A
S     --- [7]: λa.λb.λc.((a c) (b c)) ⇒ S :: (A → B → C) → (A → B) → A → C
K     --- [8]: λa.λb.a ⇒ K, true :: A → B → A
#+end_src
#+end_exampleth

If a term is found to be non-typeable, Mikrokosmos will output an error
message signaling the fact. In this way, the evaluation of \lambda-terms
which could potentially not terminate is prevented. Only typed \lambda-terms
will be evaluated while the option =:types= is on; this ensures the termination
of every computation on typed terms.

#+attr_latex: :options [Non-typeable terms on Mikrokosmos]
#+begin_exampleth
Fixed point operators are a common example of non typeable terms. Its evaluation
on untyped \lambda-calculus would not terminate; and the type inference algorithm
fails on them.

#+BEGIN_SRC haskell
fix
--- Error: non typeable expression
fix (\f.\n.iszero n 1 (plus (f (pred n)) (f (pred (pred n))))) 3
--- Error: non typeable expression
#+END_SRC

Note that the evaluation of compound \lambda-expressions where the fixpoint
operators appear applied to other terms can terminate, but the terms are
still non typeable.
#+end_exampleth

*** Product, union, unit and void types
Until this point, we have only used the function type. We are working on the
implicational fragment of the STLC we described on the first [[*Typing rules for the simply typed \lambda-calculus][typing rules]].
We are now going to extend the type system in the same sense we [[*Extending the simply typed \lambda-calculus][extended]] the
STLC. The following types are added to the type system

| Type | Name          | Description                       |
|------+---------------+-----------------------------------|
| =→=  | Function type | Functions from a type to another. |
| =×=  | Product type  | Cartesian product of types.       |
| =+=  | Union type    | Disjoint union of types.          |
| =⊤=  | Unit type     | A type with exactly one element.  |
| =⊥=  | Void type     | A type with no elements.          |

And the following typed constructors are added to the language,

| Constructor | Type                              | Description               |
|-------------+-----------------------------------+---------------------------|
| =(-,-)=     | =A → B → A × B=                   | Pair of elements          |
| =fst=       | =(A × B) → A=                     | First projection          |
| =snd=       | =(A × B) → B=                     | Second projection         |
| =inl=       | =A → A + B=                       | First inclusion           |
| =inr=       | =B → A + B=                       | Second inclusion          |
| =caseof=    | =(A + B) → (A → C) → (B → C) → C= | Case analysis of an union |
| =unit=      | =⊤=                               | Unital element            |
| =abort=     | =⊥ → A=                           | Empty function            |
| =absurd=    | =⊥ → ⊥=                           | Particular empty function |

which correspond to the constructors we described on previous
sections. The only new addition is the =absurd= function, which is
only a particular case of =abort= useful when we want to make explicit
that we are deriving an instance of the empty type. This addition will
only make the logical interpretation on the following sections
clearer.

#+attr_latex: :options [Extended STLC on Mikrokosmos]
#+begin_exampleth
The following are examples of typed terms and functions on Mikrokosmos
using the extended typed constructors. The following terms are presented

  * a function swapping pairs, as an example of pair types.
  * two-case analysis of a number, deciding whether to multiply it by two
    or to compute its predecessor.
  * difference between =abort= and =absurd=.
  * example term containing the unit type.

#+BEGIN_SRC haskell
:load types
swap = \m.(snd m,fst m)
swap
--- [1]: λa.((SND a),(FST a)) ⇒ swap :: (A × B) → B × A
caseof (inl 1) pred (mult 2)
caseof (inr 1) pred (mult 2)
--- [2]: λa.λb.b ⇒ nil, 0, false :: A → B → B
--- [3]: λa.λb.(a (a b)) ⇒ 2 :: (A → A) → A → A
\x.((abort x),(absurd x))
--- [4]: λa.((ABORT a),(ABSURD a)) :: ⊥ → A × ⊥
#+END_SRC

Now it is possible to define a new encoding of the booleans with an
uniform type. The type =⊤ + ⊤= has two inhabitants, =inl ⊤= and =inr
⊤=; and they can be used by case analysis.

#+BEGIN_SRC haskell
btrue = inl unit
bfalse = inr unit
bnot = \a.caseof a (\a.bfalse) (\a.btrue)
bnot btrue
--- [1]: (INR UNIT) ⇒ bfalse :: A + ⊤
bnot bfalse
--- [2]: (INL UNIT) ⇒ btrue :: ⊤ + A
#+END_SRC
#+end_exampleth

With these extended types, Mikrokosmos can be used as a proof checker on
first-order intuitionistic logic by virtue of the Curry-Howard
correspondence.

*** A proof in intuitionistic logic
Under the logical interpretation of Mikrokosmos, we can transcribe proofs in
intuitionistic logic to \lambda-terms and check them on the interpreter.

#+begin_theorem
In intuitionistic logic, the double negation of the LEM holds for every
proposition, that is,
\[
\forall A\colon \neg \neg (A \lor \neg A)
\]
#+end_theorem
#+begin_proof
Suppose $\neg (A \lor \neg A)$. We are going to prove first that, under this
specific assumption, $\neg A$ holds. If $A$ were true, $A \lor \neg A$ would be true and we
would arrive to a contradition, so $\neg A$. But then, if we have $\neg A$ we also have
$A \lor \neg A$ and we arrive to a contradiction with the assumption. We should conclude
that $\neg \neg (A \lor \neg A)$.
#+end_proof

Note that this is, in fact, an intuitionistic proof. Although it seems
to use the intuitionistically forbidden technique of proving by
contradiction, it is actually only proving a negation.  There is a
difference between assuming $A$ to prove $\neg A$ and assuming $\neg
A$ to prove $A$: the first one is simply a proof of a negation, the
second one uses implicitly the law of excluded middle.

This can be translated to the Mikrokosmos implementation of simply
typed \lambda-calculus as the term
#+BEGIN_SRC haskell
notnotlem = \f.absurd (f (inr (\a.f (inl a))))
notnotlem
--- [1]: λa.(ABSURD (a (INR λb.(a (INL b))))) :: ((A + (A → ⊥)) → ⊥) → ⊥
#+END_SRC
whose type is precisely $\mathtt{((A + (A \to \bot)) \to \bot) \to \bot}$. The derivation tree
can be seen directly on the interpreter as Figure [[mikrogentzen]] shows.

#+caption: Proof of the double negation of LEM.
#+name: mikrogentzen
[[./images/mikrogentzen2.png]]

* Category theory (abstract)                                         :ignore:
* Category theory
** Categories
We will think of a category as the algebraic structure that captures
the notion of composition. A category will be built from some sort
of objects linked by composable arrows; to which associativity and
identity laws will apply.

Thus, a category has to rely in some notion of /*collection*/. When
interpreted inside set-theory, it is common to use the term to denote
some unspecified formal notion of compilation of entities that could
be given by sets or proper classes. We will want to define categories
whose objects are all the possible sets and we will need the objects
to form a proper class in order to avoid inconsistent results such as
the Russell's paradox.

*** Definition of category
**** Categories, objects and morphisms                            :ignore:
#+attr_latex: :options [Category]
#+begin_definition
A *category* ${\cal C}$, as defined in cite:maclane78, is given by

 * ${\cal C}_0$ (sometimes denoted $\mathrm{obj}({\cal C})$ or simply ${\cal C}$), a /collection/ whose
   elements are called *objects*, and
 * ${\cal C}_1$, a /collection/ whose elements are called *morphisms*.

Every morphism $f \in {\cal C}_1$ is assigned two objects: a
*domain*, written as $\mathrm{dom}(f) \in {\cal C}_0$, and a
*codomain*, written as $\mathrm{cod}(f) \in {\cal C}_0$; a common
notation for such morphism is
\[
f \colon \mathrm{dom}(f) \to \mathrm{cod}(f).
\]

Given two morphisms $f \colon A \to B$ and $g \colon B \to C$, there
exists a *composition morphism*, written as $g \circ f \colon A \to C$.
Morphism composition is a binary associative operation with
identity elements $\id_{A}\colon A \to A$, that is
\[
h \circ (g \circ f) = (h \circ g) \circ f
\quad\text{ and }\quad
f \circ \id_A = f = \id_B \circ f,
\]
for any $f,g,h$, composable morphisms.
#+end_definition

**** Definition of hom-sets and small categories                  :ignore:
#+attr_latex: :options [Hom-sets]
#+begin_definition
The *hom-set* of two objects $A,B$ on a category is the collection of morphisms 
between them. It is written as $\hom(A,B)$. The set of *endomorphisms*
of an object $A$ is defined as $\mathrm{end}(A) = \hom(A,A)$.
#+end_definition

We can use a subscript, as in $\hom_{{\cal C}}(A,B)$ to explicitly
specify the category we are working in when necessary.

#+attr_latex: :options [Small and locally small categories]
#+begin_definition
A category is said to be *small* if the collections ${\cal C}_0,{\cal C}_1$ of objects and morphisms
are both sets (instead of proper classes). It is said to be *locally small* if every
hom-set is actually a set.
#+end_definition

*** Morphisms
**** Morphisms: introduction                                      :ignore:
Objects in category theory are an atomic concept and can be only
studied by their morphisms; that is, by how they are related to all
the objects of the category. Thus, the essence of a category is given
not by its objects, but by its morphisms and how composition is
defined.

It is so much so, that we will consider two objects essentially
equivalent (and we will call them /*isomorphic*/) whenever they relate
to other objects in the exact same way; that is, whenever an
invertible morphism between them exists. This will constitute an
equivalence relation on the category.

In a certain sense, morphisms are an abstraction of the notion of the
structure-preserving homomorphisms that are defined between algebraic
structures. From this perspective, /*monomorphisms*/ and
/*epimorphisms*/ can be thought as abstractions of the usual injective
and surjective homomorphisms. We will see, however, how some
properties that we take for granted, such as ''isomorphism'' meaning
exactly the same as ''both injective and surjective'' are not true in
general.

**** Isomorphisms                                                 :ignore:
#+attr_latex: :options [Isomorphisms]
#+begin_definition
A morphism $f : A \to B$ is an *isomorphism* if an inverse morphism $f^{-1} : B \to A$
such that

  * $f^{-1} \circ f = \id_{A}$,
  * $f \circ f^{-1} = \id_{B}$;

exists.
#+end_definition

We call *automorphisms* to these morphisms which are both endomorphisms
and isomorphisms.

#+attr_latex: :options [Unicity of inverses]
#+begin_proposition
<<prop-unicityinverse>>
If the inverse of a morphism exists, it is unique. In fact, if a
morphism has a left-side inverse and a right-side inverse, they are
both-side inverses and they are equal.
#+end_proposition
#+begin_proof
Given $f : A \to B$ with inverses $g_1,g_2 : B \to A$; we have that
\[
g_1 = g_1 \circ \id_A = g_1 \circ (f \circ g_2) = 
(g_1 \circ f) \circ g_2 =
\id \circ g_2 = g_2.
\]
We have used associativity of composition, neutrality of the identity 
and the fact that $g_1$ is a left-side inverse and $g_2$ is a 
right-side inverse.
#+end_proof

#+begin_definition
Two objects are *isomorphic* if an isomorphism between them exists.
We write $A \cong B$ when $A$ and $B$ are isomorphic.
#+end_definition

#+attr_latex: :options [Isomorphy is an equivalence relation]
#+begin_proposition
The relation of being isomorphic is an equivalence relation. In
particular,

 * the identity, $\id = \id^{-1}$;
 * the inverse of an isomorphism, $(f^{-1})^{-1} = f$;
 * and the composition of isomorphisms, $(f \circ g)^{-1} = g^{-1} \circ f^{-1}$;

are all isomorphisms.
#+end_proposition
#+BEGIN_proof
We can check that those are in fact inverses. From their existance
follows

 * reflexivity, $A \cong A$;
 * symmetry, $A \cong B$ implies $B \cong A$;
 * transitivity, $A \cong B$ and $B \cong C$ imply $A \cong C$.
#+END_proof

**** Monomorphisms and epimorphisms                               :ignore:
#+attr_latex: :options [Monomorphisms and epimorphisms]
#+begin_definition
A *monomorphism* is a left-cancellable morphism, that is, $f : A \to B$ is
a monomorphism if, for every $g,h : B \to A$, 
\[ f \circ g = f \circ h  \implies g = h.
\]
An *epimorphism* is a right-cancellable morphism, that is, $f : A \to B$ is
an epimorphism if, for every $g,h : B \to A$,
\[ g \circ f = h \circ f \implies g = h.
\]
A morphism that is a monomorphism and an epimorphism at the same time is
called a *bimorphism*.
#+end_definition

#+begin_remark
A morphism can be a bimorphism without being an isomorphism. We will
cover [[*Examples of categories][examples]] of this fact later.
#+end_remark

**** Retractions and sections                                     :ignore:
#+attr_latex: :options [Retractions and sections]
#+begin_definition
A *retraction* is a left inverse, that is, a morphism that has a right inverse;
conversely, a *section* is a right inverse, a morphism that has a left inverse.
#+end_definition

By virtue of Proposition [[prop-unicityinverse]], a morphism that is both
a retraction and a section is an isomorphism. Thus, not every
epimorphism is a section and not every monomorphism is a retraction.

*** Terminal objects, products and coproducts
/*Products and coproducts*/ are very widespread notions in mathematics.
Whenever a new structure is defined, it is common to wonder what the
product or union of two of these structures would be. Examples of
products are the cartesian product of sets, the product topology or
the product of abelian groups; examples of coproducts are the disjoint
union of sets, topological sum or the free product of groups.

We will abstract categorically these notions in terms of /universal/
/properties/. This viewpoint, however, is an important shift with
respect to how these properties are usually defined. We will not
define the product of two objects in terms of its internal structure
(categorically, objects are atomic and do not have any); but in terms
of all the other objects, that is, in terms of the complete structure of the
category. This turns inside-out the focus of definitions.  Moreover,
objects defined in terms of universal properties are usually not
uniquely determined, but only determined up to isomorphism. This
reinforces our previous idea of considering two isomorphic objects in
a category as /essentialy/ the same object.

Initial and terminal objects will be a first example of this viewpoint
based on universal properties.

**** Terminal objects                                             :ignore:
#+attr_latex: :options [Initial object]
#+begin_definition
An object $I$ is an *initial object* if every object is the domain of exactly
one morphism from it. That is, for every object $A$ exists an unique morphism
$i_A \colon I \to A$.
#+end_definition

#+attr_latex: :options [Terminal object]
#+begin_definition
An object $T$ is a *terminal object* (also called /final object/) if every object
is the codomain of exactly one morphism to it. That is, for every object $A$
exists an unique $t_A \colon A \to T$.
#+end_definition

#+attr_latex: :options [Zero object]
#+begin_definition
A *zero object* is an object which is both initial and terminal at the
same time.
#+end_definition

#+attr_latex: :options [Initial and final objects are essentially unique]
#+begin_proposition
<<prop-initialfinalunique>>
Initial and final objects in a category are essentially unique; that
is, any two initial objects are isomorphic and any two final objects
are isomorphic.
#+end_proposition
#+begin_proof
If $A,B$ were initial objects, by definition, there would be only one
morphism $f : A \to B$ and only one morphism $g : B \to A$. Moreover, there
would be only an endomorphism in $\mathrm{End}(A)$ and $\mathrm{End}(B)$ which should be
the identity. That implies,

  * $f \circ g = \id$,
  * $g \circ f = \id$.

As a consequence, $A \cong B$. A similar proof can be written for the terminal
object.
#+end_proof

Note, however, that these objects may not exist in any given category.

**** Products                                                     :ignore:
# TODO: The definition of product was given by MacLane on 1949. (Awodey on CTF2.0)

#+attr_latex: :options [Product object]
#+begin_definition
<<def-product>>
An object $C$ is the *product* of two objects $A,B$ on a category if there
are two morphisms
\[\begin{tikzcd}
A & C \rar{\pi_B}\lar[swap]{\pi_A} & B
\end{tikzcd}\]
such that, for any other object $D$ with two morphisms $f_1 : D \to A$ and
$f_2 : D \to B$, an unique morphism $h : D \to C$, such that $f_1 = \pi_A \circ h$
and $f_2 = \pi_B \circ h$. Diagramatically,
\[\begin{tikzcd}[column sep=tiny]
& D \dar[dashed]{\exists! h} \ar[bend left]{ddr}{f_2}\ar[bend right,swap]{ddl}{f_1} & \\
& C \drar{\pi_B}\dlar[swap]{\pi_A} & \\
A && B
\end{tikzcd}\]
#+end_definition

Note that the product of two objects does not have to exist on a category;
but when it exists, it is essentially unique. In fact, we will be able later
to construct a category in which the product object is the final object of
the category and Proposition [[prop-initialfinalunique]] can be applied. We will
write /the/ product object of $A,B$ as $A \times B$.

**** Coproducts                                                   :ignore:
#+attr_latex: :options [Coproduct object]
#+begin_definition
An object $C$ is the *coproduct* of two objects $A,B$ on a category if there
are two morphisms
\[\begin{tikzcd}
A \rar{i_A} & C & B \lar[swap]{i_B}
\end{tikzcd}\]
such that, for any other object $D$ with two morphisms $f_1 : D \to A$ and
$f_2 : D \to B$, an unique morphism $h : D \to C$, such that $f_1 = i_A \circ h$
and $f_2 = i_B \circ h$. Diagramatically,
\[\begin{tikzcd}[column sep=tiny]
& D & \\
& C \uar[dashed]{\exists! h}  & \\
A \ar[bend left]{uur}{f_1}\urar{i_A} && B \ular[swap]{i_B} \ar[bend right,swap]{uul}{f_2} &.
\end{tikzcd}\]
#+end_definition

The same discussion we had earlier for the product can be rewritten here for
the coproduct only reversing the direction of the arrows. We will write /the/
coproduct of $A,B$ as $A \amalg B$. As we will see later, the notion of a coproduct
is dual to the notion of product; and the same proofs can be applied on
both cases, only by reversing the arrows.

*** Examples of categories
**** Discrete categories                                          :ignore:
#+attr_latex: :options [Discrete categories]
#+begin_exampleth
A category is *discrete* if it has no other morphisms than the identities.
A discrete category is uniquely defined by the class of its objects and
every class of objects defines a discrete category. Thus, discrete categories
are classes or sets, without any additional categorical structure.
#+end_exampleth

**** Monoids, groups, groupoids                                   :ignore:
#+attr_latex: :options [Monoids, groups]
#+begin_exampleth
A single-object category is a *monoid*. A monoid in which
every morphism is an isomorphism is a *group*.
#+end_exampleth

This definition is equivalent to the usual definition of monoid if we
take the morphisms as elements of the monoid and composition of
morphisms as the monoid operation. Groupoids are also a particular
case of categories.

#+ATTR_LATEX: :options [Groupoids]
#+BEGIN_exampleth
A category in which every morphism is an isomorphism is a *groupoid*.
#+END_exampleth

**** Posets                                                       :ignore:
#+attr_latex: :options [Partially ordered sets]
#+begin_exampleth
Every partial ordering defines a category in which the elements are the
objects and an only morphism between two objects $\rho_{a,b} : a \to b$ exists 

In particular, every ordinal can be seen as a partially ordered set
and defines a category.
#+end_exampleth

For example, if we take the finite ordinal $[n] = (0 < \dots < n)$, it
could be interpreted as the category given by the following diagram
\[\begin{tikzcd}
0 \rar\arrow[loop above]\ar[bend right]{rr}\ar[bend right]{rrrr} &
1 \rar\arrow[loop above]\ar[bend right]{rrr} &
2 \rar\arrow[loop above]\ar[bend right]{rr} &
\dots \rar &
n \arrow[loop above]
\end{tikzcd}\]
in which every object $p$ has an identity arrow and a unique arrow
to every $q$ such that $p \leq q$. Note how the composition of arrows can
be only defined in a single way.

In a partially ordered set, the product of two objects would be its
join, the coproduct would be its meet and the initial and terminal
objects would be the greatest and the least element, respectively.

**** TODO Concrete categories                                     :ignore:
**** Category of Sets                                             :ignore:
#+attr_latex: :options [The category of sets]
#+begin_exampleth
The category $\Set$ is defined as the category with all the
possible sets as objects and functions between them as morphisms. It
is trivial to check associativity of composition and the existence of
the identity function for any set.

In this category, the product is given by the usual cartesian product
\[
A \times B = \big\{ (a,b) \mid a \in A,\ b \in B \big\},
\]
with the projections $\pi_A(a,b) = a$ and $\pi_B(a,b) = b$. We can easily
check that, if we have $f : C \to A$ and $g : C \to B$, there is a unique
function given by $h(c) = (f(c),g(c))$ such that $\pi_A \circ h = f$ and
$\pi_B \circ h = g$.

The initial object in $\Set$ is given by the empty set $\varnothing$: given any set $A$,
the only function of the form $f : \varnothing \to A$ is the empty one. The final
object, however, is only defined up to isomorphism: given any set with
a single object $\{\ast\}$, there exists a unique function of the form $f : A \to \varnothing$
for any set $A$; namely, the one defined as $\forall a \in A: f(a) = \ast$. Every
two sets with one objects are terminal objects and they are trivially
isomorphic.

Similarly, the coproduct is defined only to isomorphism. The coproduct
of two sets $A,B$ is given by its disjoint union $A \sqcup B$; but this union
can be defined in many different (but equivalent) ways. For instance,
we can add a label to the elements of each sets before joining them in
order to ensure that this will be a disjoint union; that is,
\[
A \sqcup B = \left\{ (a,0) \mid a \in A \right\} \cup \left\{ (b,1) \mid b \in B \right\}
\]
with the inclusions $i_A(a) = (a,0)$ and $i_B(b) = (b,1)$, is a possible
coproduct. Given any two functions $f : A \to C$ and $g : A \to C$, there
exists a unique function $h : A \sqcup B \to C$, given by
\[ h(x,n) = 
\left\{ \begin{array}{ll}
   f(x) & \mbox{if } n = 0, \\
   g(x) & \mbox{if } n = 1,
\end{array}\right.
\]
such that $f = h \circ i_A$ and $g = h \circ i_B$.

The category of sets is a very special category, whose properties we
will study in detail later.
#+end_exampleth

**** Category of Groups                                           :ignore:
#+attr_latex: :options [The category of groups]
#+begin_exampleth
The category $\mathsf{Grp}$ is defined as the category with groups as objects
and group homomorphisms between them as morphisms.
#+end_exampleth

**** Category of R-modules                                        :ignore:
#+attr_latex: :options [The category of R-modules]
#+begin_exampleth
The category $R\text{-Mod}$ is defined as the category with $R\text{-modules}$ as
objects and module homomorphisms between them as morphisms. We know
that the composition of module homomorphisms and the identity are
also module homomorphisms.

In particular, abelian groups form a category as $\mathbb{Z}\text{-modules}$.
#+end_exampleth

**** Category of Topological spaces                               :ignore:
#+attr_latex: :options [The category of topological spaces]
#+begin_exampleth
The category $\mathsf{Top}$ is defined as the category with
topological spaces as objects and continuous functions between them as
morphisms.
#+end_exampleth

# Product topology

** Functors and natural transformations
#+begin_quote
"Category" has been defined in order to define "functor" and "functor"
has been defined in order to define "natural transformation".

   -- *Saunders MacLane*, /Categories for the working mathematician/, cite:maclane42.
#+end_quote

Functors and natural transformations were defined for the first time
by Eilenberg and MacLane in cite:maclane42 while studying Čech
cohomology. While initially they were devised mainly as a language for
studying homology, they have proven its foundational value with the
passage of time.

*** Functors
A */functor/* will be interpreted as a homomorphism of categories
preserving their structure. As we discussed in the previous section,
the structure of a category is given by the composition of morphisms.

**** Definition of functor                                        :ignore:
#+attr_latex: :options [Functor]
#+begin_definition
Given two categories ${\cal C}$ and ${\cal D}$, a *functor* between 
them, $F : {\cal C} \to {\cal D}$, is given by

  * an *object function*, $F : \mathrm{obj}({\cal C}) \to \mathrm{obj}({\cal D})$;
  * and an *arrow function*, $F : (A \to B) \to (FA \to FB)$ for any two
    objects $A,B$ of the category;

such that

  * $F(\id_A) = \id_{FA}$, identities are preserved;
  * $F(f \circ g) = Ff \circ Fg$, the functor respects composition.
#+end_definition

**** Composition of functors                                      :ignore:
Functors can be composed as we did with morphisms. In fact, a category
of categories can be defined, having functors as morphisms. In order to
avoid paradoxes, we will only define the category of all small categories
as a non-small category so it will not contain itself.

#+attr_latex: :options [Composition of functors]
#+begin_definition
Given two functors $F \colon {\cal C} \to {\cal B}$ and $G \colon {\cal B} \to {\cal A}$, their composite functor
$G \circ F : {\cal C} \to {\cal A}$ is given by the composition of object and arrow functions
of the functors. This composition is trivially associative.
#+end_definition

#+attr_latex: :options [Identity functor]
#+begin_definition
The identity functor on a category $I_{{\cal C}}\colon {\cal C} \to {\cal C}$ is given by identity object
and arrow functions. It is trivially neutral with respect to composition.
#+end_definition

#+ATTR_LATEX: :options [The category of categories]
#+BEGIN_definition
The category $\Cats$ is defined as the category of (small) categories as
objects and functors as morphisms.
#+END_definition

**** Full and faithfull functors                                  :ignore:
#+attr_latex: :options [Full functor]
#+begin_definition
A functor $F$ is *full* if the arrow map between any pair of objects
is surjective. That is, if every $g : FA \to FB$ is of the form $Ff$
for some morphism $f \colon A \to B$.
#+end_definition

#+attr_latex: :options [Faithful functor]
#+begin_definition
A functor $F$ is *faithful* if the arrow map between any pair of objects
is injective. That is, if, for every two arrows $f_1,f_2 : A \to B$, $Ff_1 = Ff_2$
implies $f_1 = f_2$.
#+end_definition

It is easy to notice that the composition of faithful (respectively,
full) functors is again a faithful functor (respectively, full).

Note that a faithful functor needs not to be injective on objects nor
on morphisms. In particular, if $A,A',B,B'$ are four different objects,
it could be the case that $FA = FA'$ and $FB = FB'$; and, if $f : A \to B$ and
$f' : A' \to B'$ were two morphisms, it could be the case that $Ff = Ff'$.

**** Isomorphisms of categories                                   :ignore:
#+attr_latex: :options [Isomorphism of categories]
#+begin_definition
An *isomorphism of categories* is a functor $T$ whose object and arrow functions
are bijections. Equivalently, it is a functor $T$ such that there exists an /inverse/
functor $S$ such that $T \circ S$ and $S \circ T$ are identity functors.
#+end_definition

However, the notion of isomorphism of categories may be too strict.
Sometimes, it will suffice if the two compositions $T \circ S$ and $S \circ T$
are not exactly the identity functor, but isomorphic in some sense
to it. We will develop these weaker notions in the next section.

**** TODO Subcategories                                           :ignore:

*** Natural transformations
#+attr_latex: :options [Natural transformation]
#+begin_definition
A *natural transformation* between two functors with the same domain
and codomain, $\alpha\colon F \todot G$, is a family of morphisms parameterized by 
the objects of the domain category, $\alpha_C\colon FC \to GC$ such that the
following diagram commutes
\[\begin{tikzcd}
C \dar{f} & & SC \rar{\tau_C}\dar[swap]{Sf} & TC \dar{Tf} \\
C' & & SC' \rar{\tau_{C'}} & TC'
\end{tikzcd}\]
for every arrow $f : C \to C'$.
#+end_definition

It is also said that the family of morphisms is /natural/ in its
parameter. This naturality property is what allows us to translate a
commutative diagram from a functor to another.
\[\begin{tikzcd}
A\arrow{dd}{h}\drar{f} &   & & F A\arrow{dd}{F h}\drar{F f} \arrow{rrr}{\tau A} &     & & G A\arrow{dd}{}\drar{G f} &     \\
  & B \dlar{g} & &     & F B \dlar{F g} \arrow{rrr}{\tau B} & &     & G B \dlar{G g} \\
C &   & & F C \arrow{rrr}{\tau C} &     & & G C &     \\
\end{tikzcd}\]

#+attr_latex: :options [Natural isomorphism]
#+begin_definition
A *natural isomorphism* is a natural transformation in which every component,
every morphism of the parameterized family, is invertible.
#+end_definition

The inverses of a natural transformation form another natural
transformation, whose naturality follows from the naturality of the
original transformation.

**** TODO [#A] Equivalence of categories                          :ignore:
*** Composition of natural transformations
**** Vertical composition                                         :ignore:
#+attr_latex: :options [Vertical composition of natural transformations]
#+begin_definition
The *vertical composition* of two natural transformations $\tau : S\to T$
and $\sigma : R \to S$, denoted by $\tau \cdot \sigma$ is the family of morphisms defined by the objectwise
composition of the components of the two natural transformations, that is

\[\begin{tikzcd}
Rc \rar{Rf}\dar{\sigma_c}\arrow[swap,bend right=90]{dd}{(\tau \circ \sigma)_c} &
Rc' \dar{\sigma_{c'}} \arrow[bend left=90]{dd}{(\tau \circ \sigma)_{c'}} \\
Sc \rar{Sf}\dar{\tau_c}  &
Sc' \dar{\tau_{c'}} \\
Tc \rar{Tf}  &  Tc' 
\end{tikzcd}\]
#+end_definition

#+attr_latex: :options [Vertical composition is a natural transformation]
#+begin_proposition
The vertical composition of two natural transformations is in fact
a natural transformation.
#+end_proposition
#+begin_proof
Naturality of the composition follows from the naturality of its two
factors. In other words, the commutativity of the external square on
the above diagram follows from the commutativity of the two internal
squares.
#+end_proof

**** Horizontal composition                                       :ignore:
#+attr_latex: :options [Horizontal composition of natural transformations]
#+begin_definition
The *horizontal composition* of two natural transformations $\tau \colon S \to T$ and
$\tau' \colon S' \to T'$, with domains and codomains as in the following diagram
\[\begin{tikzcd}
C 
\arrow[bend left=50]{r}[name=U,below]{}{S}
\arrow[bend right=50]{r}[name=D]{}[swap]{T}
& 
B \arrow[Rightarrow,from=U,to=D]{}{\tau}
\arrow[bend left=50]{r}[name=UU,below]{}{S'}
\arrow[bend right=50]{r}[name=DD]{}[swap]{T'}
&
C \arrow[Rightarrow,from=UU,to=DD]{}{\sigma}
\end{tikzcd}\]

is denoted by $\tau' \circ \tau \colon S'S \to T'T$ and is defined as the family of morphisms
given by $\tau' \circ \tau = T' \tau \circ \tau' = \tau' \circ S'\tau$, that is, by the diagonal of the 
following commutative square
\[\begin{tikzcd}
S'Sc\rar{\tau'_{Sc}} \drar{\scriptsize{(\tau' \circ \tau)_c}} \dar[swap]{S'\tau_c} & T'Sc \dar{T' \tau_c} \\
S'Tc\rar{\tau'_{Tc}} & T'Tc
\end{tikzcd}\]
#+end_definition

#+attr_latex: :options [Horizontal composition is a natural transformation]
#+begin_proposition
The horizontal composition of two natural transformations is in fact
a natural transformation.
#+end_proposition
#+begin_proof
It is natural as the following diagram is the composition of two
naturality squares
\[\begin{tikzcd}
S'Sc \rar{S'\tau} \dar{S'Sf} &
S'Tc \rar{\tau'}  \dar{S'Tf} &
T'Tc \dar{T'Tf} \\
S'Sb \rar{S'\tau} &
S'Tb \rar{\tau'} &
T'Tb
\end{tikzcd}\]

defined respectively by the naturality of $S'\tau$ and $\tau'$.
#+end_proof

**** TODO [#A] Interchange law                                    :ignore:
#+ATTR_LATEX: :options [Interchange law]
#+BEGIN_proposition
<<prop-interchangelaw>>

#+END_proposition

** Constructions on categories
*** Product categories
**** Product category                                             :ignore:
#+attr_latex: :options [Product category]
#+begin_definition
The *product category* of two categories ${\cal C}$ and ${\cal D}$, denoted by ${\cal C} \times {\cal D}$ is a
category having

  * pairs $\left\langle c,d \right\rangle$ as objects, where $c \in {\cal C}$ and $d \in {\cal D}$;
  * and pairs $\pair{f,g} : \pair{c,d} \to \pair{c',d'}$ as morphisms, where $f \colon c \to c'$ and
    $g \colon d \to d'$ are morphisms in their respective categories.

The identity morphism of any object $\pair{c,d}$ is $\pair{\id_c, \id_d}$, and composition is
defined componentwise as
\[
\pair{f',g'} \circ \pair{f,g} = \pair{f' \circ f,g' \circ g}.
\]
#+end_definition

We also define *projection functors* $P\colon {\cal C} \times {\cal D} \to {\cal C}$ and $Q : {\cal C} \times {\cal D} \to {\cal D}$
on arrows as $P\pair{f,g} = f$ and $Q\pair{f,g} = g$. Note that this definition of
product, using these projections, would be the product of two categories on a
category of categories with functors as morphisms.

**** Product of functors                                          :ignore:
#+attr_latex: :options [Product of functors]
#+begin_definition
The *product functor* of two functors $F\colon {\cal C} \to {\cal C}'$ and $G \colon {\cal D} \to {\cal D}'$ is a
functor $F \times G \colon {\cal C} \times {\cal D} \to {\cal C}' \times {\cal D}'$ which can be defined

  * on objects as $(F \times G)\pair{c,d} = \pair{Fc,Gd}$;
  * and on arrows as $(F \times G)\pair{f,g} = \pair{Ff,Gg}$.
#+end_definition

It can be seen as the unique functor making the following diagram
commute

\[\begin{tikzcd}
{\cal C} \dar{F} &
{\cal C} \times {\cal D}  \rar{Q}\lar[swap]{P} \dar[dashed]{F \times G}&
{\cal D} \dar{G} \\
{\cal C}' &
{\cal C}' \times {\cal D}' \rar[swap]{Q'}\lar{P'}&
{\cal D}'
\end{tikzcd}\]

In this sense, the $\times$ operation is itself a functor acting on objects
and morphisms of the $\mathtt{Cat}$ category of all categories.

**** Bifunctors                                                   :ignore: 
# Definition of Bifunctors
A *bifunctor* is a functor from a product category; and it also can be
seen as a functor on two variables. As we will show in the following
proposition, it is completely determined by the two families of functors
that we obtain when we fix any of the elements.

#+ATTR_LATEX: :options [Conditions for the existence of bifunctors]
#+BEGIN_proposition
Let ${\cal B}, {\cal C}, {\cal D}$ categories with two families of functors
\[
\{L_c \colon {\cal B} \to {\cal D}\}_{c \in {\cal C}}
\quad\text{ and }\quad
\{M_b \colon {\cal C} \to {\cal D}\}_{b \in {\cal B}},
\]
such that $M_b(c) = L_c(b)$ for all $b,c$.
A bifunctor $S \colon {\cal B} \times {\cal C} \to {\cal D}$ such that $S(-,c) = L_c$ and $S(b,-) = M_b$ 
for all $b,c$ exists if and only if for every $f \colon b \to b'$ and $g \colon c \to c'$,
\[
M_{b'}g \circ L_cf = L_{c'}f \circ M_bg.
\]
#+END_proposition
#+BEGIN_proof
If the equality holds, the bifunctor can be defined as $S(b,c) = M_b(c) = L_c(b)$
in objects and as $S(f,g) = M_{b'}g \circ L_cf = L_{c'}f \circ M_bg$ on morphisms. This
bifunctor preserves identities, as 
\[
S(\id_b,\id_c) = M_b(\id_c) \circ L_c(\id_b) = \id_{M_b(c)} \circ \id_{L_c(b)} = \id,
\]
and it preserves composition, as
\[
S(f',g') \circ S(f,g) =
Mg' \circ Lf' \circ Mg \circ Lf =
Mg' \circ Mg \circ Lf' \circ Lf =
S(f'\circ f, g' \circ g)
\]
for any composable morphisms $f,f',g,g'$. On the other hand, if a bifunctor exists,
\[\begin{aligned}
M_{b'}(g) \circ L_c(f) 
&= S(\id_{b'}, g) \circ S(f, \id_c)
= S(\id_{b'} \circ f, g \circ \id_c) \\
&= S(f \circ \id_b, \id_{c'} \circ g)
= S(f,\id_{c'}) \circ S(\id_b,g) \\
&= L_{c'}(f) \circ M_b(f).
\end{aligned}\]
#+END_proof

**** Naturality for bifunctors :ignore:
#+ATTR_LATEX: :options [Naturality for bifunctors]
#+BEGIN_proposition
Given $S,S'$ bifunctors, $\alpha_{b,c} \colon S(b,c) \to S'(b,c)$ is a natural transformation
if and only if $\alpha(b,c)$ is natural in $b$ for each $c$ and natural in $c$ for each $b$.
#+END_proposition
#+BEGIN_proof
If $\alpha$ is natural, in particular, we can use the identities to prove that
it must be natural in its two components
\[\begin{tikzcd}
S(b,c) \rar{\alpha_{b,c}}\dar[swap]{S\pair{f,\id_c}} & 
S'(b,c) \dar{S'\pair{f,\id_c}} &&
S(b,c) \rar{\alpha_{b,c}} \dar[swap]{S\pair{\id_b,g}} &
S'(b,c) \dar{S'\pair{\id_b,g}} \\
S(b',c) \rar{\alpha_{b',c}} &
S'(b',c) &&
S(b,c') \rar{\alpha_{b,c'}} &
S'(b,c') \\
\end{tikzcd}\]
If both components of $\alpha$ are natural, the naturality of the natural
transformation follows from the composition of these two squares
\[\begin{tikzcd}
S(b,c)   \rar{\alpha_{b,c}} \dar[swap]{S\pair{f,\id_{c}}} &
S'(b,c)  \dar{S'\pair{f,\id_c}} \\
S(b',c)  \rar{\alpha_{b',c}} \dar[swap]{S\pair{\id_{b'},g}} &
S'(b',c) \dar{S'\pair{\id_{b'},g}} \\
S(b',c') \rar{\alpha_{b',c'}}  &
S'(b',c') \\
\end{tikzcd}\]
where each square is commutative by the naturality of each component
of $\alpha$.
#+END_proof

**** TODO [#C] Examples of product categories
*** Opposite categories and contravariant functors
**** Opposite category                                            :ignore:
#+attr_latex: :options [Opposite category]
#+begin_definition
The *opposite category* ${\cal C}^{op}$ of a category ${\cal C}$ is a category with the
same objects as ${\cal C}$ but with all its arrows reversed. That is, for each
morphism $f : A \to B$, there exists a morphism $f^{op} : B \to A$ in ${\cal C}^{op}$.
Composition is defined as
\[
f^{op} \circ g^{op} = (g\circ f)^{op},
\]
exactly when the composite $g \circ f$ is defined in ${\cal C}$.
#+end_definition

Reversing all the arrows is a process that directly translates every
property of the category into its */dual/* property. A morphism $f$ is a
monomorphism if and only if $f^{op}$ is an epimorphism; a terminal object
in ${\cal C}$ is an initial object in ${\cal C}^{op}$ and a right inverse becomes a left
inverse on the opposite category. This process is also an /involution/,
where $(f^{op})^{op}$ can be seen as $f$ and $({\cal C}^{op})^{op}$ is trivially isomorphic to ${\cal C}$.

**** Contravariant functors                                       :ignore:
#+attr_latex: :options [Contravariant functor]
#+begin_definition
A *contravariant* functor from ${\cal C}$ to ${\cal D}$ is a functor from the opposite category,
that is, $F \colon {\cal C}^{op}\to {\cal D}$. Non-contravariant functors are often called *covariant*
functors, to emphasize the difference.
#+end_definition

#+ATTR_LATEX: :options [Hom functors]
#+BEGIN_exampleth
In a locally small category ${\cal C}$, the *Hom-functor* is the bifunctor
\[
\hom \colon {\cal C}^{op} \times {\cal C} \to \Sets,
\]
defined as $\hom(a,b)$ for any two objects $a,b \in {\cal C}$. Given $f \colon a \to a'$
and $g \colon b \to b'$, this functor is defined on any $p \in \hom(a,b)$ as
\[
\hom(f,g)(p) = f \circ p \circ g \in \hom(a',b').
\]

Partial applications of the functors give rise to

 * $\hom(a,-)$, a covariant functor for any fixed $a \in {\cal C}$. Given $g \colon b \to b'$,
   \[
   \hom(a,f) \colon \hom(a,b) \to \hom(a,b')
   \]
   is defined as the postcomposition with $g$, that we write as $- \circ g$.

 * $\hom(-,b)$, a contravariant functor for any fixed $b \in {\cal C}$. Given $f \colon a \to a'$,
   \[
   \hom(f,b) \colon \hom(a',b) \to \hom(a,b)
   \]
   is defined as the precomposition with $f$, that we write as $f \circ -$.

This kind of functor, contravariant on the first variable and covariant on
the second is usually called a *profunctor*.
#+END_exampleth

*** Functor categories
#+ATTR_LATEX: :options [Functor category]
#+BEGIN_definition
Given two categories ${\cal B},{\cal C}$, the *functor category* ${\cal B}^{{\cal C}}$ has all functors
${\cal C} \to {\cal B}$ as objects and natural transformations between them as morphisms.
#+END_definition

If we consider the category of small categories $\mathtt{Cat}$, there is
a profunctor $-^{-} \colon \mathtt{Cat}^{op} \times \mathtt{Cat} \to \mathtt{Cat}$ sending any two categories
to their functor category.

In cite:lawvere09, multiple examples of usual mathematical
constructions in terms of functor categories can be found.

**** Category of graphs                                           :ignore:
Graphs, for instance, can be seen as functors; and graphs
homomorphisms as the natural transformations between them.

#+ATTR_LATEX: :options [Graphs as functors]
#+BEGIN_exampleth
We consider the category given by two objects and two non-identity
morphisms,
\[\begin{tikzcd}
\cdot
\rar[bend left]
\rar[bend right,swap] & 
\cdot
\end{tikzcd}\]
usually called $\downarrow\downarrow$. To define a functor from this category to $\Sets$
amounts to choose two sets $E,V$ (not necessarily different) called
the set of /edges/ and the set of /vertices/; and two functions $s,t \colon E \to V$,
called /source/ and /target/. That is, our usual definition of directed
multigraph,
\[\begin{tikzcd}
E
\rar[bend left]{s}
\rar[bend right,swap]{t} & 
V
\end{tikzcd}\]
can be seen as an object in the category $\Sets^{\downarrow\downarrow}$. Note how a natural transformation
between two graphs $(E,V)$ and $(E',V')$ is a pair of morphisms $\alpha_E \colon E \to E'$
and $\alpha_V \colon V \to V'$ such that $s \circ \alpha_E = \alpha_V \circ s$ and $t \circ \alpha_E = \alpha_V \circ t$.
This provides a convenient notion of graph homomorphism: a pair of morphisms
preserving the incidence of edges. We can call $\mathtt{Graph}$ to this functor category.
#+END_exampleth

**** TODO Examples
**** TODO Pointed Sets (?)
**** TODO The category of all categories
**** TODO Sheaves!
*** Comma categories
#+ATTR_LATEX: :options [Comma category]
#+BEGIN_definition
Let ${\cal C},{\cal D},{\cal E}$ be categories with functors $T \colon {\cal E} \to {\cal C}$ and $S \colon {\cal D} \to {\cal C}$.
The *comma category* $(T \downarrow S)$ has

  * morphisms of the form $f \colon Te \to Sd$ as objects, for $e \in {\cal E}, d \in {\cal D}$;
  * and pairs $\pair{k,h} \colon f \to f'$, where $k \colon e \to e'$ and $h \colon d \to d'$ such
    that $f' \circ Tk = Sh \circ f$, as arrows.

Diagramatically, a morphism in this category is a commutative diagram
\[\begin{tikzcd}
\color{black!50}{Te} \rar{Tk} \dar[swap,color=black!50]{f} & \color{black!50}{Te'}\dar[,color=black!50]{f'} \\
\color{black!50}Sd \rar{Sh}                & \color{black!50}{Sd'}          \\
\end{tikzcd}\]
where the objects of the category are drawn in grey.
#+END_definition
# \[\begin{tikzcd}[remember picture]
# Te \rar{Tk}\dar[swap]{f} & Te'  \dar{f'} \\
# Sd \rar{Sh}              & Sd'          \\
# \end{tikzcd}
# \begin{tikzpicture}[remember picture,overlay]
# \node (A) at ([xshift=0.1cm]\tikzcdmatrixname-1-1) {};
# \node (B) at ([xshift=-0.1cm]\tikzcdmatrixname-2-1) {};
# \node (C) at ([xshift=0.1cm]\tikzcdmatrixname-1-2) {};
# \node (D) at ([xshift=-0.1cm]\tikzcdmatrixname-2-2) {};
# \node[draw=black!50, fit=(A) (B)]() {};
# \node[draw=black!50, fit=(C) (D)]() {};
# \end{tikzpicture}\]


**** Slice category                                               :ignore:
# Motivation

#+ATTR_LATEX: :options [Slice category]
#+BEGIN_definition
A *slice category* is a particular case of a comma category $(T \downarrow S)$
in which $T = \mathrm{Id}$ is the identity functor and $S$ is a functor from the
terminal category, a category with only one object and its identity
morphism.
#+END_definition

A functor from the terminal category simply chooses an object
of the category.
If we call $a = S(\ast)$, objects of this category are morphisms
$f \colon c \to a$, where $c \in {\cal C}$; and morphisms are $\pair{k} \colon f \to f'$, where
$k \colon c \to c'$ such that $f' \circ k = f$. Diagramatically a morphism is
drawn as
\[\begin{tikzcd}[remember picture, column sep=tiny]
\color{black!50}{c} \ar{rr}{Tk}\drar[swap,color=black!50]{f} & &
\color{black!50}{c'} \dlar[color=black!50]{f'} \\
              & \color{black!50}{a} &          \\
\end{tikzcd}\]

This slice category is conventionally written as $({\cal C} \downarrow a)$. In
general, we write $(T \downarrow a)$ when $S$ is a functor from the terminal
category picking an object $a$; and we write $({\cal C} \downarrow S)$ when $T$ is
the identity functor.

**** Coslice category                                             :ignore:
#+ATTR_LATEX: :options [Coslice category]
#+BEGIN_definition
*Coslice categories* are the categorical dual of slice categories. It
is the particular case of a comma category $(T \downarrow S)$ in which $S = \mathrm{Id}$
is the identity functor and $T$ is a functor from the terminal category,
a category with only one object and its identity morphism.
#+END_definition

If we call $a = T(\ast)$, objects of this category are morphisms
$f \colon c \to a$, where $c \in {\cal C}$; and morphisms are $\pair{k} \colon f \to f'$, where
$k \colon c \to c'$ such that $k \circ f' = f$. Diagramatically a morphism is
drawn as
\[\begin{tikzcd}[remember picture, column sep=tiny]
& \color{black!50}{a} \drar[color=black!50]{f}\dlar[swap,color=black!50]{f'} &\\
\color{black!50}{c} \ar{rr}{Sk} & & \color{black!50}{c'} \\
\end{tikzcd}\]

This slice category is conventionally written as $(a \downarrow {\cal C})$. In
general, we write $(a \downarrow S)$ when $T$ is a functor from the terminal
category picking an object $a$; and we write $(T \downarrow {\cal C})$ when $S$ is
the identity functor.

**** Arrow category                                               :ignore:
#+ATTR_LATEX: :options [Arrow category]
#+BEGIN_definition
*Arrow categories* are a particular case of comma categories $(T \downarrow S)$
in which both functors are the identity. They are usually written as ${\cal C}^{\to}$.
#+END_definition

Objects in this category are morphisms in ${\cal C}$, and morphisms in this
category are commutative squares in ${\cal C}$. Diagramatically,
\[\begin{tikzcd}
\color{black!50}{a}\rar{k}\dar[swap,color=black!50]{f} & 
\color{black!50}{b} \dar[color=black!50]{f'} \\
\color{black!50}{a'}\rar{h} & \color{black!50}{b'}
\end{tikzcd}\]
.

** Universality and limits
*** Universal arrows
# Every arrow to the given functor S factors uniquely through the
# universal arrow.

#+attr_latex: :options [Universal arrow]
#+begin_definition
A *universal arrow* from $c$ to $S$ is a morphism $u \colon c \to Sr$ such that
for every $g \colon c \to Sd$ exists a unique morphism $f \colon r \to d$ making this diagram
commute
\[\begin{tikzcd}
& Sd & d \\
c \rar[swap]{u}\urar{g} & Sr \uar[swap,dashed]{Sf} & r \uar[dashed,swap]{\exists! f} &.
\end{tikzcd}\]
#+end_definition

Note how an universal arrow is, equivalently, the initial object of
the comma category $(c \downarrow S)$. Thus, universal arrows must be unique up
to isomorphism.

#+attr_latex: :options [Universality in terms of hom-sets]
#+begin_proposition
The arrow $u \colon c \to Sr$ is universal iff $f \mapsto Sf \circ u$ is a bijection
$\hom(r,d) \cong \hom(c,Sd)$ natural in $d$. Any natural bijection of this
kind is determined by a unique universal arrow.
#+end_proposition
#+begin_proof
Bijection follows from the definition of universal arrow, and
naturality follows from $S(gf)\circ u = Sg \circ Sf \circ u$.

Given a bijection $\varphi$, we define $u = \varphi(\id_r)$.
By naturality we have the bijection $\varphi(f) = Sf \circ u$, every arrow
is written in this way.
#+end_proof

The categorical dual of an universal arrow from an object to a functor
is the notion of universal arrow from a functor to an object. Note how,
particularly in this case, we avoid the name /couniversal arrow/; as
both arrows are representing what we usually call a /universal property/.

#+ATTR_LATEX: :options [Dual universal arrow]
#+BEGIN_definition
A *universal arrow* from $S$ to $c$ is a morphism $v \colon Sr \to c$ such that
for every $g \colon Sd \to c$ exists a unique morphism $f \colon d \to r$ making this diagram
commute
\[\begin{tikzcd}
d\dar[dashed,swap]{\exists! f} & Sd \dar[swap,dashed]{Sf}\drar{g} & \\
r & Sr \rar[swap]{v} & c
\end{tikzcd}\]
#+END_definition

# TODO Universal from a functor to another
**** TODO Examples
# TODO Equivalence relations
*** Representability
#+attr_latex: :options [Representation of a functor]
#+begin_definition
A *representation* of $K \colon D \to \Sets$ is a natural isomorphism
\[
\psi\colon \hom_{D}(r,-) \cong K.
\]

A functor is /representable/ if it has a representation. An object $r$ is
called a /representing object/. $D$ must have small hom-sets.
#+end_definition

#+attr_latex: :options [Representations in terms of universal arrows]
#+begin_proposition
If $u \colon \ast \to Kr$ is a universal arrow for a functor $K\colon D \to \Sets$, then
$f \mapsto K(f)(u\ast)$ is a representation. Every representation is obtained
in this way.
#+end_proposition
#+begin_proof
We know that $\hom(\ast, X) \xrightarrow{.} X$ is a natural isomorphism in $X$; in particular
$\hom(\ast, K-) \xrightarrow{.} K-$. Every representation is built then as

\[ \hom_{D}(r,-) \cong \hom(\ast,K-) \cong K, \]

for every natural isomorphism $D(r,-) \cong \Sets(\ast,K-)$. But every natural
isomorphism of this kind is a [[*Universal arrows as natural bijections][universal arrow]].
#+end_proof

*** Yoneda Lemma
#+attr_latex: :options [Yoneda Lemma]
#+begin_lemma
<<lemma-yoneda>>
For any $K\colon D \to \Sets$ and $r \in D$, there is a bijection
\[
y \colon \mathrm{Nat}(\hom_{D}(r,-), K) \cong Kr
\]
sending the natural transformation $\alpha \colon \hom_{D}(r,-) \xrightarrow{.} K$
to the image of the identity, $\alpha_r(\id_r)$.
#+end_lemma
#+begin_proof
The complete natural transformation $\alpha$ is determined by $\alpha_r(\id_r)$.
By naturality, given any $f \colon r \to s$,
\[\begin{tikzcd}[row sep={8mm,between origins},column sep={12mm,between origins}]
\hom(r,r) \arrow{rrrr}{\alpha_r} \arrow[swap]{ddd}{f \circ -}
    & & & & Kr \arrow{ddd}{Kf} \\  
    & \id_r \arrow[rr, |->] \arrow[d, |->] & & \alpha_r(\id_r) \arrow[d, |->] & \\[1cm]
    & f \arrow[rr, |->] & & \alpha_s(f) \\
\hom(r,s) \arrow[swap]{rrrr}{\alpha_s} & & & & Ks\\  
\end{tikzcd}\]
it must be the case that $\alpha_s(f) = Kf(\alpha_r(\id_r))$.
#+end_proof

#+attr_latex: :options [Characterization of natural transformations between representable functors]
#+begin_corollary
Given $r,s \in D$, any natural transformation $\hom(r,-) \xrightarrow{.} \hom(s,-)$ is
of the form $- \circ h$ for a unique morphism $h\colon s \to r$.
#+end_corollary
#+begin_proof
Using Yoneda Lemma (Lemma [[lemma-yoneda]]), we know that
\[
\mathrm{Nat}(\hom_D(r,-), \hom_D(s,-)) \cong \hom_D(s,r),
\]
sending the natural transformation to a morphism $\alpha(id_r) = h \colon s \to r$. The
rest of the natural transformation is determined as $- \circ h$ by naturality.
#+end_proof

#+attr_latex: :options [Naturality of the Yoneda Lemma]
#+begin_proposition
The bijection on the [[lemma-yoneda][Yoneda Lemma]] (Lemma [[lemma-yoneda]]),
\[
y \colon \mathrm{Nat}(\hom_{D}(r,-), K) \cong Kr,
\]
is a natural isomorphism between two functors from $\Sets^D \times D$
to $\Sets$.
#+end_proposition
#+begin_proof
We define $N \colon \Sets^D \times D \to \Sets$ on objects as
$N\pair{r,K} = \Nat(\hom(r,-),K)$.
Given $f \colon r \to r'$ and $F \colon K \todot K'$, the functor is defined on 
morphisms as
\[
N\pair{f,F}(\alpha) =
F \circ \alpha \circ (- \circ f) \in
\Nat(\hom(r',-),K),
\]
where $\alpha \in \Nat(\hom(r,-),K)$.
We define $E \colon \Sets^D \times D \to \Sets$  on objects as
$E\pair{r,K} = Kr$.
Given $f \colon r \to r'$ and $F \colon K \todot K'$, the functor is defined on morphisms as
\[
E\pair{f,F}(a) =
F(Kf(a)) = K'f(Fa) \in
K'r',
\]
where $a \in Kr$, and the equality holds because of the naturality
of $F$.

The naturality of $y$ is equivalent to the commutativity of the
following diagram
\[\begin{tikzcd}[column sep=huge]
\Nat(\hom(r,-),K) \rar{y}\dar[swap]{N\pair{f,F}}&
Kr \dar{E\pair{f,F}} \\
\Nat(\hom(r',-),K') \rar{y} &
K'r'
\end{tikzcd}\]
where, given any $\alpha \in \Nat(\hom(r,-),K)$, it follows from naturality
of $\alpha$ that
\[\begin{aligned}
y\big(N\pair{f,F}(\alpha)\big)
&= y\big (F \circ \alpha \circ (- \circ f) \big)
= F \circ \alpha \circ (- \circ f) (\id_{r'})
= F(\alpha(f)) \\
&= F(\alpha(\id_{r'} \circ f))
= F(Kf(\alpha_r(\id_r)))
= E\pair{f,F}\alpha_r(\id_{r}) \\
&= E\pair{f,F}\big(y (\alpha)\big).
\end{aligned}\]
#+end_proof

#+begin_definition
In the conditions of the [[lemma-yoneda][Yoneda Lemma]] the *Yoneda functor*,
$Y \colon D^{op} \to \Sets^{D}$, is defined with the arrow function
\[
\left(f \colon s \to r\right) \mapsto 
\Big(\hom_D(f,-) \colon \hom_D(r,-) \to \hom_D(s,-)\Big).
\]
It can be also written as $Y' \colon D \to \Sets^{D^{op}}$.
#+end_definition

#+begin_proposition
The Yoneda functor is full and faithful.
#+end_proposition
#+begin_proof
By [[lemma-yoneda][Yoneda Lemma]], we know that
\[
y \colon \Nat(\hom(r,-), \hom(s,-)) \cong \hom(s,r)
\]
is a bijection, where $y(\hom(f,-)) = f$.
#+end_proof

# TODO: The Yoneda functor is a currying of the hom functor.

*** Limits
**** Motivation: products                                         :ignore:
In the definition of product, we chose two objects of the category, we
considered all possible */\red{cones}/* over two objects and we picked the
universal one. Diagramtically,
\[\begin{tikzcd}[column sep=tiny]
&&& && d \dar[dashed]{\exists!} \ar[bend left,color=myred]{ddr}\ar[bend right,swap,color=myred]{ddl} & \\
& c \drar[color=myred]\dlar[color=myred] & &&& a \times b \drar[color=myred]\dlar[color=myred] & \\
a && b &\phantom{gap}& a && b
\end{tikzcd}\]
$c$ is a cone and $a \times b$ is the universal one: every cone factorizes through
it. In this particular case, the base of each cone is given by two objects;
or, in other words, by the image of a functor from the discrete category with
only two objects, called the /index category/.

We will be able to create new constructions on categories by formalizing
the notion of cone and generalizing to arbitrary bases, given as functors
from arbitrariliy complex index categories.
Constant functors are the first step into formalizing the notion of
/cone/.

**** Constant functors                                            :ignore:
#+ATTR_LATEX: :options [Constant functor]
#+BEGIN_definition
The *constant functor* $\Delta \colon {\cal C} \to {\cal C}^{{\cal J}}$ sends each object $c \in {\cal C}$ to a
constant functor $\Delta c \colon {\cal J} \to {\cal C}$ defined as

 * the constantly-$c$ function for objects, $\Delta(j) = c$;
 * and the constantly-$\id_c$ function for morphisms, $\Delta(f) = \id_{c}$.

The constant functor sends a morphism $g \colon c \to c'$ to a natural
transformation $\Delta g \colon \Delta c \to \Delta c'$ whose components are all $g$.
#+END_definition

We could say that $\Delta c$ squeezes the whole category ${\cal J}$ into $c$. A
natural transformation from this functor to some other $F \colon {\cal J} \to {\cal C}$
should be regarded as a */\red{cone}/* from the object $c$ to a copy
of ${\cal J}$ inside the category ${\cal C}$; as the following diagram exemplifies
\[\begin{tikzcd}[column sep=tiny, row sep=tiny]
&&& &&& & c \ar[color=myred]{dd}\ar[bend left, color=myred]{ddr}\ar[bend right, color=myred]{dddl}\ar[bend left, color=myred]{dddrr} &&\\
&\phantom{gap}&& &&& & \phantom{gap} &&\\
& \cdot \ar{ld}\ar{r}\ar{drr} & \cdot \ar{dr}  & \ar[Rightarrow, shorten <= 3mm, shorten >= 14mm]{uurrrr}[xshift=-3mm]{\Delta}  &&&
& \cdot \ar{ld}\ar{r}\ar{drr} & \cdot \ar{dr} & \\
\cdot \arrow{rrr}[description, yshift=-5mm]{\big{\cal J}} &&& 
\cdot \ar[Rightarrow, yshift=3mm,swap, shorten <= 2mm, shorten >= 2mm]{rrr}{F}
&\phantom{gap}&&
\cdot \arrow{rrr}[description, yshift=-5mm]{\big{F \cal J}}  &   &   & \cdot \\
%&{\cal J}&& &&& &F{\cal J}&&
\end{tikzcd}\]
The components of the natural transformation appear highlighted in
the diagram. The naturality of the transformation implies that each
triangle
\[\begin{tikzcd}[column sep=tiny,row sep=small]
&\cdot
\drar[color=myred, shorten >= -1.5mm, shorten <= -2mm]
\dlar[color=myred, shorten >= -1.5mm, shorten <= -2mm] &\\
\cdot 
\ar[shorten >= -1mm, shorten <= -1mm]{rr} &&
\cdot
\end{tikzcd}\]
on that cone must be commutative. Thus, natural transformations are
a way to recover all the information of an arbitrary /*index category*/ ${\cal J}$ that
was encoded in $c$ by the constant functor. As we did with products,
we want to find the cone that best encodes that information; a universal
cone, such that every other cone factorizes through it. Diagramatically
an $r$ such that, for each $d$,
\[\begin{tikzcd}[column sep=tiny, row sep=tiny]
&d 
\ar[color=myred,bend right]{dddd}
\ar[bend left, color=myred]{ddddr}
\ar[bend right, color=myred]{dddddl}
\ar[bend left, color=myred]{dddddrr}
\ar[dashed]{dd}{\exists!} &&\\
&&\phantom{d}&\\
& r \ar[color=myred]{dd}\ar[bend left, color=myred]{ddr}\ar[bend right, color=myred]{dddl}\ar[bend left, color=myred]{dddrr} &&\\
& \phantom{gap} &&\\
& \cdot \ar{ld}\ar{r}\ar{drr} & \cdot \ar{dr} & \\
\cdot \ar{rrr} &   &   & \cdot \\
\end{tikzcd}\]
That factorization will be represented in the formal definition of
limit by a universal natural transformation between the two constant
functors.

**** Definition of limit                                          :ignore:
#+ATTR_LATEX: :options [Limit]
#+BEGIN_definition
The *limit* of a functor $F \colon {\cal J} \to {\cal C}$ is an object $r \in {\cal C}$ such that
there exists a universal arrow $v \colon \Delta r \todot F$ from $\Delta$ to $F$. It is
usually written as $r = \Limit F$.
#+END_definition

That is, for every natural transformation $w \colon \Delta d \todot F$, there is a unique
morphism $f \colon d \to r$ such that
\[\begin{tikzcd}
d\dar[dashed,swap]{\exists! f} & \Delta d \dar[swap,dashed]{\Delta f}\drar{w} & \\
r & \Delta r \rar[swap]{v} & F
\end{tikzcd}\]
commutes. This reflects directly on the universality of the cone we described
earlier and proves that limits are unique up to isomorphism.

By choosing different index categories, we will be able to define multiple
different constructions on categories as limits.

*** Examples of limits
**** Equalizers                                                   :ignore:
For our first example, we will take the following category,
called $\downarrow\downarrow$ as index category,
\[\begin{tikzcd}
\cdot
\rar[bend left]
\rar[bend right] & 
\cdot
\end{tikzcd}\]
A functor $F \colon \downarrow\downarrow \to {\cal C}$ is a pair of parallel arrows in ${\cal C}$.
Limits of functors from this category are called *equalizers*.
With this definition, the *equalizer* of two parallel arrows
$f,g \colon a \to b$ is an object $\mathrm{eq}(f,g)$ with a morphism
$e \colon \mathrm{eq}(f,g) \to a$ such that $f \circ e = g \circ e$; and such that
any other object with a similar morphism factorizes uniquely
through it
\[\begin{tikzcd}[column sep=tiny]
&
d
\dar[dashed]{\exists!}
\arrow[bend right, swap, color=myred]{ddl}{e'}
\arrow[bend left, color=myred]{ddr}  &\\& 
\operatorname{eq}(f,g)
\drar[color=myred]
\dlar[swap, color=myred]{e} &\\
a 
\arrow[bend left=15]{rr}{f}
\arrow[bend right=15, swap]{rr}{g} &&
b
\end{tikzcd}\]
note how the right part of the cone is completely determined
as $f \circ e$. Because of this, equalizers can be written without
specifying it, and the diagram can be simplified to
\[\begin{tikzcd}
\mathrm{eq}(f,g) 
\rar{e} & 
a 
\arrow[bend left=15]{r}{f}
\arrow[bend right=15, swap]{r}{g} &
b \\
d 
\ar[swap]{ur}{e'} 
\ar[dashed]{u}{\exists!} &&&
\end{tikzcd}\]
.

#+ATTR_LATEX: :options [Equalizers in Sets]
#+BEGIN_exampleth
The equalizer of two parallel functions $f,g \colon A \to B$ in $\Sets$ 
is $\left\{ x \in A \mid f(a) = g(a) \right\}$
with the inclusion morphism. Given any other function $h \colon D \to A$
such that $f \circ h = g \circ h$, we know that $f(h(d)) = g(h(d))$ for
any $d \in D$. Thus, $h$ can be factorized through the equalizer.
\[\begin{tikzcd}
\left\{ x \in A \mid f(a) = g(a) \right\}
\rar[hook]{i} & 
A 
\arrow[bend left=15]{r}{f}
\arrow[bend right=15, swap]{r}{g} &
B \\
D 
\ar[swap]{ur}{h} 
\ar[dashed]{u}{\exists!} &&&
\end{tikzcd}\]
#+END_exampleth

#+ATTR_LATEX: :options [Kernels]
#+BEGIN_exampleth
In the category of abelian groups, the kernel of a function $f$,
$\mathrm{ker}(f)$, is the equalizer of $f \colon G \to H$ and a function sending
each element to the zero element of $H$. The same notion of kernel
can be defined in the category of $R\mhyphen\text{Modules}$, for any ring $R$.
#+END_exampleth

**** Pullbacks                                                    :ignore:
*Pullbacks* are defined as limits whose index category is $\cdot \to \cdot \gets \cdot$.
Any functor from that category is a pair of arrows with a common
codomain; and the pullback is the universal cone over them.
\[\begin{tikzcd}
& d
\arrow[bend right,swap,color=myred]{ddl}{p'}
\arrow[bend left,color=myred]{ddr}{q'}
\arrow[bend right,color=myred]{dd}
\arrow[dashed]{d}{\exists!} &\\
& a
\drar[bend left,swap,color=myred]{q}
\dlar[bend right,color=myred]{p}
\arrow[color=myred]{d} & \\
x\rar[swap]{f} & z & y\lar{g} \\
\end{tikzcd}\]
Again, the central arrow of the diagram is determined as
$f \circ q = g \circ p$; so it can be ommited in the diagram. The usual
definition of a pullback for two morphisms $f \colon x \to z$ and
$g \colon y \to z$ is pair of morphisms $p \colon a \to x$ and $q \colon a \to y$
such that $f \circ q = g \circ p$ which are also universal, that is,
given any pair of morphisms $p' \colon d \to x$ and $q' \colon d \to y$,
there exists a unique $u \colon d \to a$ making the diagram commute.
Usually we write the pullback object as $x \times_z y$ and we write
this property diagramatically as
\[\begin{tikzcd}
d
\ar[dashed]{dr}{\exists! u} 
\ar[bend left]{drr}{p'} 
\ar[swap,bend right]{ddr}{q'} &&\\
& x \times_z y
\dar[swap]{q}
\rar{p} &
x
\dar{f} \\
& y
\rar[swap]{g} &
z
\end{tikzcd}\]
The square in this diagram is usually called a 
/pullback square/, and the pullback object is usually called a
/fibered product/.

# TODO: Example of pullback
# TODO: Equalizers are a particular example of pullbacks

**** TODO Arbitrary products                                      :ignore:
**** TODO Powers                                                  :ignore:
**** TODO Posets
*** Colimits
A colimit is the dual notion of a limit. We could consider 
cocones to be the dual of cones and pick the universal one.
Once an index category ${\cal J}$ and a base category ${\cal C}$ are fixed,
a */\blue{cocone}/* is a natural transformation from a functor on the
base category to a constant functor. Diagramatically,
\[\begin{tikzcd}[column sep=tiny, row sep=tiny]
&&& &&& & c &&\\
&\phantom{gap}&& &&& & \phantom{gap} &&\\
& \cdot \ar{ld}\ar{r}\ar{drr} & \cdot \ar{dr}  & \ar[Rightarrow, shorten <= 3mm, shorten >= 14mm]{uurrrr}[xshift=-3mm]{\Delta}  &&&
& \cdot \ar{ld}\ar{r}\ar{drr}\ar[color=myblue]{uu} & \cdot \ar{dr} \ar[color=myblue,bend right]{uul} & \\
\cdot \arrow{rrr}[description, yshift=-5mm]{\big{\cal J}} &&& 
\cdot \ar[Rightarrow, yshift=3mm,swap, shorten <= 2mm, shorten >= 2mm]{rrr}{F}
&\phantom{gap}&&
\cdot \arrow[color=myblue, bend left]{uuur} \arrow{rrr}[description, yshift=-5mm]{\big{F \cal J}} &&&
\cdot \arrow[color=myblue, bend right]{uuull}\\
\end{tikzcd}\]
is an example of a cocone, and the universal one would be the
$r$, such that, for each cone $d$,
\[\begin{tikzcd}[column sep=tiny, row sep=tiny]
&d  &&\\
&&\phantom{d}&\\
& r \ar[dashed,swap]{uu}{\exists!}
&&\\
& \phantom{gap} &&\\
& \cdot 
\ar{ld}\ar{r}\ar{drr} 
\ar[color=myblue,bend left]{uuuu}
\ar[color=myblue]{uu}&
\cdot \ar{dr} 
\ar[bend right, color=myblue]{uuuul} \ar[color=myblue,bend right]{uul}& \\
\cdot \ar{rrr} 
\ar[bend left, color=myblue]{uuuuur}
\ar[bend left, color=myblue]{uuur} &&& 
\cdot
\ar[bend right, color=myblue]{uuull}
\ar[bend right, color=myblue]{uuuuull} \\
\end{tikzcd}\]
and naturality implies that each triangle
\[\begin{tikzcd}[column sep=tiny,row sep=small]
&\cdot &\\
\cdot
\ar[color=myblue, shorten >= -1.5mm, shorten <= -2mm]{ur}
\ar[shorten >= -1mm, shorten <= -1mm]{rr} &&
\cdot
\ar[color=myblue, shorten >= -1.5mm, shorten <= -2mm]{ul}
\end{tikzcd}\]
commutes.

#+attr_latex: :options [Colimits]
#+begin_definition
The *colimit* of a functor $F \colon J \to {\cal C}$ is an object $r \in {\cal C}$ such that there exists
a universal arrow $u \colon F \todot \Delta r$ from $F$ to $\Delta$. It is usually written as $r = \Colimit F$.
#+end_definition

That is, for every natural transformation $w \colon F \todot \Delta d$, there is a unique
morphism $f \colon r \to d$ such that
\[\begin{tikzcd}
d & \Delta d  & \\
r\uar[dashed]{\exists! f} &
\Delta r \uar[dashed]{\Delta f} &
F \lar{v}\ular[swap]{w}
\end{tikzcd}\]
commutes. This reflects directly on the universality of the cocone we described
earlier and proves that colimits are unique up to isomorphism.

*** Examples of colimits
**** Coequalizers                                                 :ignore:
*Coequalizers* are the dual of /equalizers/; colimits of functors
from $\downarrow\downarrow$. The coequalizer of two parallel arrows is an object
$\mathrm{coeq}(f,g)$ with a morphism $e \colon b \to \mathrm{coeq}(f,g)$ such that $e \circ f = e \circ g$;
and such that any other object with a similar morphism factorizes uniquely
through it
\[\begin{tikzcd}[column sep=tiny]
&
d &\\& 
\operatorname{coeq}(f,g)
\uar[dashed]{\exists!} &\\
a
\ar[bend left, color=myblue, swap]{uur}
\ar[color=myblue]{ur}
\arrow[bend left=15]{rr}{f}
\arrow[bend right=15, swap]{rr}{g} &&
b
\ar[color=myblue, swap]{ul}{e}
\ar[color=myblue, bend right, swap]{uul}{e'}
\end{tikzcd}\]
as the right part of the cocone is completely determined by
the left one, the diagram can be written as
\[\begin{tikzcd}
a 
\ar[bend left=15]{r}{f}
\ar[bend right=15, swap]{r}{g} &
b
\ar{r}{e}
\ar[swap]{dr}{e'}&
\mathrm{coeq}(f,g) \ar[dashed]{d}{\exists!} \\
&&
d 
\end{tikzcd}\]

#+ATTR_LATEX: :options [Coequalizers in Sets]
#+BEGIN_exampleth
The coequalizer of two parallel functions $f,g \colon A \to B$ in
$\Sets$ is $B /(\sim_{f=g})$, where $\sim_{f = g}$ is the minimal equivalence
relation in which we have $f(a) \sim g(a)$ for each $a \in A$.
Given any other function $h \colon B \to D$ such that $h(f(a)) = h(g(a))$,
it can be factorized in a unique way by $h' \colon B/\sim_{f=g} \to D$.
\[\begin{tikzcd}
A
\ar[bend left=15]{r}{f}
\ar[bend right=15, swap]{r}{g} &
B
\ar{r}{e}
\ar[swap]{dr}{e'}&
B/(\sim_{f=g}) \ar[dashed]{d}{\exists!} \\
&&
D 
\end{tikzcd}\]
#+END_exampleth
**** Pushouts                                                     :ignore:
*Pushouts* are the dual of pullbacks; colimits whose index category
is $\cdot \gets \cdot \to \cdot$, that is, the dual of the index category for pullbacks.
Diagramatically,
\[\begin{tikzcd}
& d
&\\
& a
\arrow[dashed,swap]{u}{\exists!} & \\
x 
\urar[bend left,color=myblue]{p}
\arrow[bend left,color=myblue]{uur}{p'} & 
z 
\lar{f}
\rar[swap]{g}
\arrow[color=myblue]{u}
\arrow[bend left,color=myblue]{uu} &
y
\ular[bend right,swap,color=myblue]{q}
\arrow[bend right,swap,color=myblue]{uul}{q'} \\
\end{tikzcd}\]
and we can define the pushout of two morphisms $f \colon z \to x$ and
$g \colon z \to y$ as a pair of morphisms $p \colon x \to a$ and $q \colon y \to a$ such
that $p \circ f = q \circ g$ which are also universal, that is, given any
pair of morphisms $p' \colon x \to d$ and $q' \colon y \to d$, there exists a unique
$u \colon a \to d$ making the diagram commute.
\[\begin{tikzcd}
z
\rar{g}
\dar[swap]{f} & 
y \dar{q}\ar[bend left]{rdd}{q'} & \\
x \rar{p}\ar[bend right,swap]{drr}{p'} &
x \amalg_z y \ar[dashed,swap]{dr}{\exists! u} & \\
& & d
\end{tikzcd}\]
The square in this diagram is usually called a 
/pushout square/, and the pullback object is usually called a
/fibered coproduct/.

# TODO Seifert Van-Kampen
**** TODO Arbitrary coproducts                                    :ignore:
**** TODO Copowers                                                :ignore:
**** TODO Posets                                                  :ignore:
** Adjoints, monads and algebras
# TODO: Lawvere's notation

*** Adjunctions
**** Definition of adjunction                                     :ignore:
#+attr_latex: :options [Adjunction]
#+begin_definition
An *adjunction* from categories ${\cal X}$ to ${\cal Y}$ is a pair of functors
$F\colon {\cal X} \to {\cal Y}$, $G\colon {\cal Y} \to {\cal X}$ with a natural bijection
\[
\varphi \colon \hom(Fx,y) \cong \hom(x,Gy),
\]
natural in both $x\in {\cal X}$ and $y \in {\cal Y}$. We write it as $F \dashv G$.
#+end_definition

Naturality of $\varphi$ means that both
\[\begin{tikzcd}
\hom(Fx,y)
\rar{\varphi_{x,y}}
\dar[swap]{- \circ Fh} & 
\hom(x,Gy)
\dar{- \circ h} & 
\hom(Fx,y)
\rar{\varphi_{x,y}}
\dar[swap]{k \circ -} &
\hom(x,Gy)
\dar{Gk \circ -} \\
\hom(Fx',y) 
\rar[swap]{\varphi_{x,y}} &
\hom(x',Gy) &
\hom(Fx,y')
\rar[swap]{\varphi_{x,y}} &
\hom(x,Gy')
\end{tikzcd}\]
commute for every $h \colon x \to x'$ and $k \colon y \to y'$. That is, for every $f \colon Fx \to y$,
$\varphi(f) \circ h = \varphi(f \circ Fh)$ and $Gk \circ \varphi(f) = \varphi(k \circ f)$. Equivalently, $\varphi^{-1}$ is natural
and that means that, for every $g \colon x \to Gy$, $k \circ \varphi^{-1}(g) = \varphi^{-1}(Gk \circ g)$ and
$\varphi^{-1}(g) \circ Fh = \varphi^{-1}(g \circ h)$.

In the following two propositions, we will characterize all this
information in terms of natural transformations made up of universal
arrows.

**** Unit and counit of an adjunction                             :ignore:
#+attr_latex: :options [Unit and counit of an adjunction]
#+begin_proposition
An adjunction determines a *unit* and a *counit*;

 1) the *unit* is natural transformation made with universal arrows $\eta\colon \Id \todot GF$, where
    the right adjunct of each $f \colon Fx \to y$ is
    \[
    \varphi f = Gf \circ \eta_x \colon x \to Gy;
    \]
 2) the *counit* is natural transformation made with universal arrows $\varepsilon \colon FG \todot \Id$, where
    the left adjunct of each $g \colon x \to Gy$ is
    \[
    \varphi^{-1}g = \varepsilon \circ Fg \colon Fx \to y;
    \]

that follow the *triangle identities* $G \varepsilon \circ \eta G = \id_G$ and $\varepsilon F \circ F\eta = \id_{F}$.
# TODO: draw the triangle idenntities
# TODO: Write adjunctions as logical deductions (Lawvere's notation)
#+end_proposition
#+BEGIN_proof
We define $\eta_x = \varphi(\id_{Fx})$ and $\varepsilon_x = \varphi^{-1}(\id_{Gx})$. The right and left adjunct formulas
and the naturality of $\eta$ and $\varepsilon$ can be deduced from the naturality of $\varphi$,
\[\begin{aligned}
GFh \circ \eta_x &=
GFh \circ \varphi(\id_{Fx}) =
\varphi(Fh) =
\varphi(\id_{Fx}) \circ h =
\eta_x \circ h;\\
\varepsilon_x \circ FGk &=
\varphi^{-1}(\id_{Fx}) \circ FGk =
\varphi^{-1}(Gk) =
k \circ \varphi^{-1}(\id_{Gx}) =
k \circ \varepsilon_{x}.
\end{aligned}\]
Finally, the triangle identities also follow from naturality of $\varphi$,
\[\begin{aligned}
\id &= \varphi(\varepsilon) = G\varepsilon \circ \eta; \\
\id &= \varphi^{-1}(\eta) = \varepsilon \circ F\eta.
\end{aligned}\]
#+END_proof

**** Characterization of adjunctions                              :ignore:
#+attr_latex: :options [Characterization of adjunctions]
#+begin_proposition
Each adjunction is completely determined by any of the following data,

 1) functors $F,G$ and $\eta\colon 1 \todot GF$ where $\eta_x\colon x \to GFx$ is universal to $G$.
 2) functor $G$ and universals $\eta_x \colon x \to GF_0 x$, where $F_0x \in {\cal Y}$.
 3) functors $F,G$ and $\varepsilon\colon FG \todot 1$ where $\varepsilon_a\colon FGa \to a$ is universal from $F$.
 4) functor $F$ and universals $\varepsilon_a\colon FG_0a \to a$, creating a functor $G$.
 5) functors $F,G$, with natural transformations satisfiying the triangle
    identities $G\varepsilon \circ \eta G = \id$ and $\varepsilon F \circ F\eta = \id$.
#+end_proposition
#+begin_proof
 1) Universality of $\eta_x$ gives a isomorphism $\varphi \colon \hom(Fx,y) \cong \hom(x,Gy)$ between 
    the arrows in the following diagram
    \[\begin{tikzcd}
    & Gy & y \\
    x \rar[swap]{\eta_x}\urar{f} & GFx \uar[swap,dashed]{Gg} & Fx \uar[dashed,swap]{\exists! g}
    \end{tikzcd}\]
    defined as $\varphi(g) = Gg \circ \eta_x$. This isomorphism is natural in $x$; for every
    $h \colon x' \to x$ we know by naturality of $\eta$ that $Gg \circ \eta \circ h = G(g \circ Fh) \circ \eta$.
    The isomorphism is also natural in $y$; for every $k \colon y \to y'$ we know by
    functoriality of $G$ that $Gh\circ Gg \circ \eta = G(h \circ g) \circ \eta$.

 2) We can define a functor $F$ on objects as $Fx = F_0x$. Given any
    $h \colon x \to x'$, we can use the naturality of $\eta$ to define
    $Fh$ as the unique arrow making this diagram commute
    \[\begin{tikzcd}
    & GFx' & Fx' \\
    x \rar[swap]{\eta_x}\urar{\eta_{x'} \circ h} & GFx \uar[swap,dashed]{GFh} & Fx \uar[dashed,swap]{\exists! Fh}
    \end{tikzcd}\]
    and this choice makes $F$ a functor, as it can be checked in the
    following diagrams using the existence and uniqueness given by
    the universality of $\eta$ in both cases
    \[\begin{tikzcd}
    &     &    & x'' \rar{\eta_{x''}}  & GFx''  & Fx'' \\
    & GFx & Fx & x'  \uar{h'} \rar{\eta_{x'}}  & GFx'  \uar[swap]{GFh'}  & Fx' \uar[dashed]{\exists! Fh'} \\
    x \rar[swap]{\eta_x}\urar{\eta_{x}} & GFx \uar[swap,dashed]{\id} & Fx \uar[dashed,swap]{\id} & x \rar{\eta_{x}}\uar{h} & GFx \uar[swap]{GFh} & Fx \uar[dashed]{\exists! Fh'} \ar[dashed,swap,bend right]{uu}{\exists! F(h' \circ h)}
    \end{tikzcd}\]

 3) The proof is dual to that of 1.

 4) The proof is dual to that of 2.

 5) We can define two functions $\varphi(f) = Gf \circ \eta_x$ and $\theta(g) = \varepsilon_y \circ Fg$.
    We checked in 1 (and 3) that these functions are natural in both arguments;
    now we will see that they are inverses of each other using naturality
    and the triangle identities
    \[\begin{aligned}
    \varphi(\theta(g)) &=
    G\varepsilon_a \circ GFg \circ \eta_x =
    G\varepsilon_a \circ \eta_x \circ g = g; \\
    \theta(\varphi(f)) &=
    \varepsilon \circ FGf \circ F\eta =
    f \circ \varepsilon \circ F\eta = f.
    \end{aligned}\]
#+end_proof

*** Properties of adjoints
**** Uniqueness                                                   :ignore:
#+ATTR_LATEX: :options [Essential uniqueness of adjoints]
#+BEGIN_proposition
Two adjoints to the same functor $F,F' \dashv G$ are naturally isomorphic.
#+END_proposition
#+BEGIN_proof
# TODO
#+END_proof

**** Composition of adjoints                                      :ignore:
# Theorem 1 pag 103
#+ATTR_LATEX: :options [Composition of adjunctions]
#+BEGIN_theorem
Given two adjunctions between categories ${\cal X},{\cal Y}$ and ${\cal Y},{\cal Z}$
respectively,
\[
\varphi \colon \hom(Fx,y) \cong \hom(x,Gy)
\qquad
\theta \colon \hom(F'y,z) \cong \hom(G'y,z)
\]
the composite functors yield a composite adjunction
\[
\theta \cdot \varphi \colon \hom(F'Fx,y) \cong \hom(x,GG'z).
\]
If the unit and counit of $\varphi$ are $\pair{\eta,\varepsilon}$ and the unit and counit
of $\theta$ are $\pair{\eta',\varepsilon'}$; the unit and counit of the composite adjunction
are $\pair{G \eta' F \circ \eta,\ \varepsilon' \circ F' \varepsilon G'}$.
#+END_theorem
#+BEGIN_proof
# TODO
#+END_proof

**** TODO Bicategory of adjoints
*** TODO Examples of adjoints
*** TODO Equivalence of categories
*** TODO Adjoints for preorders (Galois connections)
*** Monads
**** Definition of Monads                                         :ignore:
#+attr_latex: :options [Monad]
#+begin_definition
A *monad* is a functor $T\colon X \to X$ with natural transformations

 * $\eta\colon I \todot T$, called /unit/
 * $\mu \colon T^2 \todot T$, called /multiplication/

such that
\[\begin{tikzcd}
T^3 \rar{T\mu}\dar{\mu T} & T^2\dar{\mu} \\
T^2 \rar{\mu} & T
\end{tikzcd}
\qquad
\begin{tikzcd}
IT \rar{\eta T}\drar[swap]{\cong} & T^2\dar{\mu} & \lar[swap]{T\eta}\dlar{\cong} TI \\
& T & &.
\end{tikzcd}\]
#+end_definition

**** Each adjunction gives rise to a monad                        :ignore:
#+attr_latex: :options [Each adjunction gives rise to a monad]
#+begin_proposition
Given $F \dashv G$, $GF$ is a monad.
#+end_proposition
#+begin_proof
We take the unit of the adjunction as the monad unit. We define the
product as $\mu = G\varepsilon F$. Associativity follows from these diagrams
\[\begin{tikzcd}
FGFG\rar{FG\varepsilon} \dar[swap]{\varepsilon FG} & FG \dar{\varepsilon} \\
FG\rar{\varepsilon} & I
\end{tikzcd}
\qquad
\begin{tikzcd}
GFGFGF\rar{GFG\varepsilon F} \dar[swap]{G\varepsilon FGF} & GFGF \dar{G\varepsilon F} \\
GFGF\rar{G\varepsilon F} & GF &,
\end{tikzcd}\]

where the first is commutative by Proposition [[prop-interchangelaw]] and the second
is obtained by applying functors $G$ and $F$. Unit laws follow from
the [[*Unit and counit][triangular identities]] after applying $F$ and $G$.
#+end_proof

**** Comonads                                                     :ignore:
#+attr_latex: :options [Comonad]
#+begin_definition
A *comonad* is a functor $L\colon X \to X$ with natural transformations

 * $\varepsilon\colon L\to I$, called /counit/
 * $\delta\colon L \to L^2$, called /comultiplication/

such that
\[\begin{tikzcd}
L\rar{\delta} \dar[swap]{\delta} & L^{2} \dar{L\delta} \\
L^{2}\rar{\delta L} & L^{3}
\end{tikzcd}
\qquad
\begin{tikzcd}
& L \dar{\delta} \dlar[swap]{\cong} \drar{\cong} & \\
IL & 
L^2 \lar{\varepsilon L}\rar[swap]{L \varepsilon} & 
LI
&.
\end{tikzcd}\]
#+end_definition

*** Algebras for a monad
#+attr_latex: :options [T-algebra]
#+begin_definition
For a monad $T$, a $T\textbf{-algebra}$ is an object $x$ with an arrow $h \colon Tx \to x$ called 
/structure map/ making these diagrams commute
\[\begin{tikzcd}
T^{2}x \rar{Th}\dar[swap]{\mu} & Tx \dar{h} \\
Tx\rar{h} & x
\end{tikzcd}
\]
#+end_definition

#+attr_latex: :options [Morphism of T-algebras]
#+begin_definition
A *morphism of T-algebras* is an arrow $f\colon x \to x'$ making the following square
commute
\[\begin{tikzcd}
Tx \dar[swap]{Tf}\rar{h} & Tx \dar{f} \\
Tx' \rar[swap]{h'} & Tx'
\end{tikzcd}\]
#+end_definition

#+attr_latex: :options [Category of T-algebras]
#+begin_proposition
The set of all $T\text{-algebras}$ and their morphisms form a category $X^{T}$.
#+end_proposition
#+begin_proof
Given $f\colon x \to x'$ and $g\colon x'\to x''$, $T\text{-algebra}$ morphisms, their composition
is also a $T\text{-algebra}$ morphism, due to the fact that this diagram

\[\begin{tikzcd}
Tx \rar{h}\dar[swap]{Tf} & 
x \dar{f}\\
Tx' \dar[swap]{Tg} \rar{h'} &
x' \dar{g}\\
Tx'' \rar{h''}&
x''
\end{tikzcd}\]

commutes.
#+end_proof

*** Free algebras for a monad
*** Kleisli categories
#+ATTR_LATEX: :options [Kleisli category]
#+BEGIN_definition
The *Kleisli category* of a monad $T \colon {\cal X} \to {\cal X}$ is written as ${\cal X}_T$ 
and is given by

 * an object $x_T$ for every $x \in {\cal X}$; and

 * an arrow $f^{\flat} \colon x_T \to y_T$ for every $f \colon x \to T y$.

And the composite of two morphisms is defined as
\[
g^{\flat} \circ f^{\flat} = (\mu \circ Tg \circ f)^{\flat}.
\]
#+END_definition

#+ATTR_LATEX: :options [Adjunction on a Kleisli category]
#+BEGIN_theorem
The functors $F_T \colon {\cal X} \to {\cal X}_T$ and $G \colon {\cal X}_T \to {\cal X}$ from and
to the Kleisli category which are defined on objects as $F_T(x) = x_T$ and
$G_T(x_T) = Tx$, and defined on morphisms as

\[\begin{aligned}
F_T \colon& (k \colon x \to y)             &\mapsto&\ (\eta_y \circ k)^{\flat} \colon x_T \to y_T \\
G_T \colon& (f^{\flat} \colon x_T \to y_T) &\mapsto&\ (\eta_y \circ Tf) \colon Tx \to Ty \\
\end{aligned}\]

form an adjunction $\flat \colon \hom(x,Ty) \to \hom(x_T,y_T)$ whose monad
is precisely $T$.
#+END_theorem
#+BEGIN_proof
# TODO
# Theorem 1 pag 147 MacLane
#+END_proof

*** TODO Beck's theorem
#+ATTR_LATEX: :options [Beck's theorem]
#+BEGIN_theorem

#+END_theorem
** TODO Spans
# This implies that the category of spans of finite sets is equivalent
# to the Lawvere theory of commutative monoids, that is, to the category
# of finitely generated free commutative monoids.
** Kan extensions
*** Dinatural transformations
# Extension of the concept of naturality to a special type of
# bifunctors.
# Natural diagonal between functors.

#+ATTR_LATEX: :options [Dinatural transformation]
#+BEGIN_definition
A *dinatural transformation* between functors $F,G \colon {\cal C}^{op} \times {\cal C} \to {\cal B}$
is a family of morphisms $\alpha_x \colon F(x,x) \to G(x,x)$ for each $x \in {\cal C}$,
called /components/, such that for every $f \colon x \to y$,
\[\begin{tikzcd} &
F(x,x) \rar{\alpha_{x}} &
G(x,x) \drar{G(\id,f)} & \\
F(y,x) \urar{F(f,\id)}\drar[swap]{F(\id,f)} & & &
G(x,y) \\
&
F(y,y) \rar{\alpha_{y}} &
G(y,y) \urar[swap]{G(f,\id)} &
\end{tikzcd}\]

commutes
#+END_definition

# TODO: Every natural transformation gives a dinatural transformation.

**** TODO Natural transformation of covariant to contravariant functor :ignore:
**** Extranatural transformations (wedges)                        :ignore:
#+ATTR_LATEX: :options [Wedge]
#+BEGIN_definition
A *wedge* (also called *extranatural transformation*) is a dinatural
transformation from a constant functor.

That is, given a functor $T \colon {\cal C}^{op} \times {\cal C} \to {\cal B}$, a *wedge* $\alpha \colon x \toddot T$ is a
family of morphisms $\alpha_c \colon x \to T(c,c)$ such that

\[\begin{tikzcd}
x        \dar[swap]{\alpha_{c'}} \rar{\alpha_{c}} &
T(c,c)   \dar{T(\id,f)} \\
T(c',c') \rar[swap]{T(f,\id)} &
T(c,c')
\end{tikzcd}\]

commutes for every $f \colon c \to c'$. 
#+END_definition

# Wedges or cowedges?
# #+ATTR_LATEX: :options [Cowedge]
# #+BEGIN_definition
# A *wedge* (also called *extranatural transformation*) is a dinatural
# transformation from a constant functor. We write wedges as $\alpha \colon F \toddot b$.
# #+END_definition

# That is, given a functor $F \colon {\cal C}^{op} \times {\cal C} \to {\cal B}$, a wedge $\alpha \colon F \toddot b$ is a
# family of morphisms $\alpha_x \colon F(x,x) \to b$ such that

# \[\begin{tikzcd}
# F(y,x)\rar{F(\id,f)} \dar[swap]{F(f,\id)} & F(y,y) \dar{\alpha_{y}} \\
# F(x,x)\rar[swap]{\alpha_{x}} & b
# \end{tikzcd}\]

# commutes for every $f \colon x \to y$. 

**** TODO Examples of dinatural transformations                   :ignore:
*** Ends
**** Definition of Ends                                           :ignore:
#+ATTR_LATEX: :options [End]
#+BEGIN_definition
An *end* of $T \colon {\cal C}^{op} \times {\cal C} \to {\cal B}$ is its universal wedge.
That is, a wedge $\alpha \colon e \toddot T$ such that for every other $\beta \colon k \toddot T$,
there exists a unique $u \colon k \to e$ such that $\alpha_c \circ u = \beta_c$ for
each $c \in {\cal C}$.
#+END_definition

Diagramatically,

\[\begin{tikzcd}
k \ar[dashed]{dr}{\exists! u} \ar[bend left]{drr}{\beta_{c}} \ar[swap,bend right]{ddr}{\beta_{c'}} & &\\
&e        \dar[swap]{\alpha_{c'}} \rar{\alpha_{c}} &
T(c,c)   \dar{T(\id,f)} \\
&T(c',c') \rar[swap]{T(f,\id)} &
T(c,c')
\end{tikzcd}\]


We usually call /end/ to the object $e$, and we use the following integral
notation
\[
e = \int_{c \in {\cal C}} T(c,c).
\]

**** Examples of ends 1                                           :ignore:
We present some examples of Ends, taken mainly from cite:thecatsters.

#+ATTR_LATEX: :options [Natural transformations as Ends]
#+BEGIN_exampleth
Natural transformations are ends; they can be written as
\[
\Nat(F,G) = \int_{c\in {\cal C}} \hom(Fc,Gc)\ .
\]

Take $T = \hom(F-,G-) \colon {\cal C}^{op} \times {\cal C} \to \Sets$, where $F,G \colon {\cal C} \to {\cal E}$ are
two arbitrary functors. A wedge $x \toddot T$ would be defined by a set $x \in \Sets$
and a family of functions $\omega_c \colon x \to \hom(Fc,Gc)$ such that
\[\begin{tikzcd}
x        \dar[swap]{\omega_{c'}} \rar{\omega_{c}} &
\hom(Fc,Gc)   \dar{Gf \circ -} \\
\hom(Fc',Gc') \rar[swap]{- \circ Ff} &
\hom(Fc,Gc')
\end{tikzcd}\]
commutes for each $f \colon c \to c'$. That is, for each element of the
set, $a \in x$, we get $\omega_c(a) \colon Fc \to Gc$ such that $Gf \circ \omega_c(a) = \omega_{c'}(a) \circ Ff$;
in other words, the following square is natural
\[\begin{tikzcd}
Fc \rar{\omega_c(a)}\dar[swap]{Ff} & Gc \dar{Gf} \\
Fc' \rar[swap]{\omega_{c'}(a)} & Gc' \\
\end{tikzcd}\]
and $\omega_c(a)$ is a natural transformation from $F$ to $G$, for each $a \in x$. 

Thus, we have proved that a wedge $x \toddot T$ is a set $x$ mapping to the
set of natural transformations. The universal set with this property
is clearly (it can be seen as the terminal object of the slice
category) the set of natural transformations. That is,
\[
\Nat(F,G) = \int_{c\in {\cal C}} \hom_{{\cal E}}(Fc,Gc)\ .
\]

The particular case
\[
\Nat(\id,\id) = \int_{c\in{\cal C}} \hom_{{\cal C}}(c,c)
\]
is sometimes known as the center of the category.
#+END_exampleth

**** Examples of ends 2                                           :ignore:
#+ATTR_LATEX: :options [Tannakian reconstruction]
#+BEGIN_exampleth
Let $M$ be a monoid in $\Sets$. We consider $M\mhyphen\Sets$, the category of sets
in which $M$ acts; that is, pairs $(S,\sigma)$, where $s$ is an arbitrary
set and $\sigma \colon M \times S \to S$ is an action of the monoid on the set.
Morphisms are of the form $f \colon (S,\sigma) \to (R,\rho)$ where $f \circ \sigma = \rho \circ f$.

There is
a forgetful functor $U \colon M\mhyphen\Sets \to \Sets$ that simply forgets the $M\mhyphen\text{action}$
on the set; and we can recover the monoid from this functor as
\[
\int_{(s,\sigma) \in M\mhyphen\Sets} \hom(U(s,\sigma), U(s,\sigma)) =
\Nat(U,U) = M.
\]
This is a very particular instance of the idea of Tannakian reconstruction,
as described in cite:joyal91.

The forgetful functor $U \colon M\mhyphen\Sets \to \Sets$ is a representable functor;
represented by $(M,\mu) \in M\mhyphen\Sets$. There exists a function
\[
g \colon \hom_{M\mhyphen\Sets}((M,\mu), (S,\sigma)) \to S = U(S,\sigma)
\]
sending $g(f) = f(e)$. Note that $f$ is completely determined by $f(e)$; as
$f(m) = f(\mu(m,e)) = \sigma(m,f(e))$; and this $g$ is a bijection. We have
proved that $U \cong \hom((M,\mu),-)$.

Now, using this fact and the dual Yoneda lemma,
\[\begin{aligned}
\Nat(U,U) &\cong \Nat(\hom((M,\mu),-), \hom((M,\mu),-)) \\
          &= \hom_{M\mhyphen\Sets}((M,\mu),(M,\mu)) = M;
\end{aligned}\]
and the components of the natural transformation are simply
multiplication by $m$ for some fixed $m \in M$. Diagramatically, for
any given $\psi \in \Nat(U,U)$,
\[\begin{tikzcd}[row sep={4mm,between origins},column sep={8mm,between origins}]
(M,\mu) \arrow[rrrr, "\psi_{(S,\sigma)}"] \arrow[ddd, "{\sigma(-,s)}", swap] 
    & & & & (M,\mu) \arrow[ddd, "{\sigma(-,s)}"] \\  
    & e \arrow[rr, |->] \arrow[d, |->] & & m \arrow[d, |->] & \\[1cm]
    & s \arrow[rr, |->] & & \sigma(m,s) \\
(S,\sigma) \arrow[rrrr, "\psi_{(S,\sigma)}", swap] & & & & (S,\sigma)\\  
\end{tikzcd}\]
it must be the case that $\psi_{(S,\sigma)}(s) = ms$.
#+END_exampleth

**** Examples of ends 3                                           :ignore:
#+ATTR_LATEX: :options [Reconstruction of the center]
#+BEGIN_exampleth
A variant on the previous example is to use identity functors
to recover
\[
\int_{(s,\sigma) \in M\mhyphen\Sets} \hom((s,\sigma),(s,\sigma)) = \Nat(\id,\id) = {\cal Z}(M)
\]
the center of the monoid.

If $\psi \in \Nat(\id,\id)$, we can apply the forgetful functor to get
$U\psi \in \Nat(U,U)$. By the previous example, the natural transformation
must be of the form $\psi_{(S,\sigma)}(s) = \sigma(m,s)$ for each $(S,\sigma)$, given some
fixed $m \in M$.

However, further restrictions apply in this case. In particular,
$\psi_{(M,\mu)}$ must be a morphism in $M\mhyphen\Sets$, therefore, for any $n \in M$,
\[
nm = \mu(n,\mu(m,e)) = \mu(m,\mu(n,e)) = mn,
\]
and $m \in {\cal Z}(M)$. The fact that any $m \in {\cal Z}(M)$ defines a natural
transformation follows from the axioms of the $M\mhyphen\Sets$ structure.
#+END_exampleth

*** Coends
*** TODO Profunctors, distributors
# https://ncatlab.org/nlab/show/profunctor
**** Composition
**** Heteromorphisms
**** https://ncatlab.org/nlab/show/anafunctor
*** Kan extensions
** TODO Monoidal categories
*** TODO String diagrams
# Peter Selinger - A survey of graphical languages https://arxiv.org/pdf/0908.3347.pdf
** n-categories
# TODO: Bicategories are weak 2-categories
# TODO: 2-categories are strict 2-categories
*** Strict 2-categories
**** 2-categories: introduction                                   :ignore:
# TODO: The category of two categories
In a 2-category, instead of having sets of morphisms; we have a
category of morphisms between any two objects. It has objects,
called */0-cells/*; morphisms between objects, called */1-cells/*; and
morphisms between morphisms, called */2-cells/*.
\[\begin{tikzcd}
x
&
x \rar{f}&y 
&
x\phantom{y} 
\arrow[bend left=50]{r}[name=U,below]{}{f}
\arrow[bend right=50]{r}[name=D]{}[swap]{g}
& 
\phantom{x}y \arrow[Rightarrow,from=U,to=D]{}{\alpha}
\end{tikzcd}\]
As, morphisms between two objects form a category, morphisms between
morphisms can be composed vertically in their category; 
\[\begin{tikzcd}
x\phantom{y}
\arrow[bend left=80]{rr}[name=U2,below]{}{f}
\arrow[]{rr}[name=C2,below]{}[name=C1,above]{}[above=2mm, left=3mm]{g}
\arrow[bend right=80]{rr}[name=D2]{}[swap]{h}
&& 
\phantom{x}y
\arrow[Rightarrow,from=U2,to=C1]{}{\alpha}
\arrow[Rightarrow,from=C2,to=D2]{}{\beta}
&
x\phantom{y} 
\arrow[bend left=50]{r}[name=U,below]{}{f}
\arrow[bend right=50]{r}[name=D]{}[swap]{h}
& 
\phantom{x}y \arrow[Rightarrow,from=U,to=D]{}{\beta \circ \alpha}
\end{tikzcd}\]
but the 2-category will also provide a horizontal composition of
morphisms between two objects
\[\begin{tikzcd}
\makebox[0pt][l]{$x$}\phantom{y} 
\arrow[bend left=50]{r}[name=U,below]{}{f}
\arrow[bend right=50]{r}[name=D]{}[swap]{f'}
& 
\phantom{}y
\arrow[Rightarrow,from=U,to=D]{}{\alpha}
\arrow[bend left=50]{r}[name=U,below]{}{g}
\arrow[bend right=50]{r}[name=D]{}[swap]{g'}
& 
\makebox[0pt][l]{$z$}\phantom{y}
\arrow[Rightarrow,from=U,to=D]{}{\beta}
&
x
\arrow[bend left=50]{rr}[name=U,below]{}{g \circ f}
\arrow[bend right=50]{rr}[name=D]{}[swap]{g' \circ f'}
&&
z
\arrow[Rightarrow,from=U,to=D]{}{\beta \ast \alpha}
\end{tikzcd}\]
which, as we will formalize, will act functorially with respect to
vertical composition. An interchange law which will act as a
generalization of Proposition <<prop-interchangelaw>> will follow
from functoriality. In fact, the category of small categories, with
functors and natural transformations between them will be an example
of 2-category.

**** 2-category: definition                                       :ignore:
#+ATTR_LATEX: :options [2-category]
#+BEGIN_definition
A *2-category* is given by

  * a collection of objects;
  * a category $T(a,b)$ for every pair of objects $a,b$, called
    /vertical category/ between $a,b$;
  * a identity arrow $U_a \colon 1 \to T(a,a)$ for every object $a$;
  * a horizontal composition functor $K_{a,b,c}\colon T(b,c) \times T(a,b) \to T(a,c)$
    for every triple of objects $a,b,c$;

where we usually write the action of $K$ on morphisms as $K(\beta,\alpha) = \beta \ast \alpha$.
The functor $K_{a,b,c}$ must be associative and $U_a$ must provide a left and
right identity.
#+END_definition

Associativity means that the two following composite functors are
strictly equal,

\[\begin{tikzcd}[row sep=huge, column sep=tiny]
&
T(c,d) \times T(b,c) \times T(a,b)
\ar[bend left=60]{dd}[name=U,left]{}{}
\ar[bend right=60]{dd}[name=D]{}[swap]{}
\drar{K_{a,b,c} \times \id}
\dlar[swap]{K_{b,c,d} \times \id} & \\
T(b,d) \times T(a,b) \drar[swap]{K_{a,b,d}} &&
T(c,d) \times T(a,c) \dlar{K_{a,c,d}} \\
& T(a,d) & 
\arrow[phantom,from=U,to=D]{}{=}
\end{tikzcd}\]

This is a particular case of an *enriched category*.

**** Interchange law                                              :ignore:
#+ATTR_LATEX: :options [Interchange law]
#+BEGIN_proposition
Given any $\alpha,\alpha',\beta,\beta'$ composable 2-cells,
\[
(\beta' \circ \beta) \ast (\alpha' \circ \alpha) =
(\beta' \ast \alpha') \circ (\beta \ast \alpha).
\]
#+END_proposition
#+BEGIN_proof
Unfolding the definition in the product category and applying
the functoriality of $K$, we get
\[\begin{aligned}
(\beta' \circ \beta) \ast (\alpha' \circ \alpha) &=
K(\beta' \circ \beta, \alpha' \circ \alpha)  = K((\beta' , \alpha') \circ (\beta , \alpha)) \\
&= K(\beta', \alpha') \circ K(\beta, \alpha) = (\beta' \ast \alpha') \circ (\beta \ast \alpha).
\end{aligned}\]
#+END_proof

**** TODO 2-cat                                                   :ignore:
# Describe 2-cat as a 3-cat
**** TODO Eckmann-Hilton                                          :ignore:
**** TODO Double categories                                       :ignore:
*** Weak 2-categories
In the definition of strict 2-categories, we have postulated that the
horizontal composition of 2-cells should be associative, but we could
relax this condition and write a similar definition in which the
horizontal composition is not associative but
/associative up to isomorphism/. This new definition gives us the
notion of *bicategories*, which are also called *2-categories*.

**** Formal definition: bicategory                                :ignore:
#+ATTR_LATEX: :options [Bicategory]
#+BEGIN_definition
A *bicategory* is given by

  * a collection of objects;
  * a category $B(a,b)$ for every pair of objects $a,b$, called
    /vertical category/ between $a,b$;
  * a identity arrow $U_a \colon 1 \to T(a,a)$ for every object $a$;
  * a horizontal composition functor $\ast \colon T(b,c) \times T(a,b) \to T(a,c)$
    for every triple of objects $a,b,c$;

such that, whenever we find the following configuration,
\[\begin{tikzcd}[]
a\phantom{b}
\ar[bend left=60]{r}[name=U1,below]{}{}
\ar[bend right=60]{r}[name=U2,above]{}{} &
b\phantom{a}
\ar[bend left=60]{r}[name=V1,below]{}{}
\ar[bend right=60]{r}[name=V2,above]{}{} &
c\phantom{b}
\ar[bend left=60]{r}[name=W1,below]{}{}
\ar[bend right=60]{r}[name=W2,above]{}{} &
d\phantom{c}
\arrow[Rightarrow,from=U1,to=U2]{}{}
\arrow[Rightarrow,from=V1,to=V2]{}{}
\arrow[Rightarrow,from=W1,to=W2]{}{}
\end{tikzcd}\]
associativity holds up to natural isomorphism, that is, there exists
an isomorphism $\alpha$ between the following composite functors
\[\begin{tikzcd}[row sep=huge, column sep=tiny]
&
B(c,d) \times B(b,c) \times B(a,b)
\ar[bend left=60]{dd}[name=U,left]{\phantom{as}}{}
\ar[bend right=60]{dd}[name=D,right]{\phantom{as}}[swap]{}
\drar{\ast \times \id}
\dlar[swap]{\ast \times \id} & \\
B(b,d) \times B(a,b) \drar[swap]{\ast} &&
B(c,d) \times B(a,c) \dlar{\ast} \\
& B(a,d) & 
\arrow[Rightarrow,from=D,to=U]{}{\alpha}
\end{tikzcd}\]
#+END_definition

**** TODO Coherence theorem for bicategories                      :ignore:
# https://ncatlab.org/nlab/show/pentagon+identity
# https://ncatlab.org/nlab/show/bicategory

*** Simplicial sets
# [Goerss], [nLab], [Jardine] and [Riehl]
This section follows cite:goerss09 and cite:riehl11.

# https://golem.ph.utexas.edu/category/2017/08/simplicial_sets_vs_simplicial.html

**** Simplicial sets                                              :ignore:
#+ATTR_LATEX: :options [Simplex category]
#+BEGIN_definition
The *simplex category* $\Delta$ is the category of finite ordinals of the
form
\[
[n] = (0 < \dots < n),
\]
for any $n \in \mathbb{N}$, and order-preserving maps between them.
#+END_definition

All the morphisms simplex category are generated by composition of
morphisms of the following two families:

 * *coface maps*, maps of the form $d^k \colon [n-1] \to [n]$ defined as the
   only injective map not having $k$ in its image; and
 * *codegeneracy maps*, maps of the form $s^k \colon [n+1] \to [n]$ defined as
   the only surjective map such that $k$ has two preimages.

From the definition of these functions, the following equations follow
\begin{aligned}
d^id^j = d^{j+1}d^i, & \quad & \text{for each }\ i \leq j, \\
s^js^i = s^is^{j+1}, & \quad & \text{for each }\ i \leq j, \\
\end{aligned}
\[
s^jd^i =  \left\{
	\begin{array}{ll}
		d^is^{j-1}  & \mbox{if } i < j, \\
		1 & \mbox{if } i = j, \mbox{ or } j+1, \\
                d^{i-1}s^j & \mbox{otherwise}.
	\end{array} \right.
\]

#+ATTR_LATEX: :options [Representation of a morphism]
#+BEGIN_lemma
Any morphism in $\Delta$ can be written as
\[
f = d^{i_k}\dots d^{i_1}s^{j_1} \dots s^{j_k} : [n] \to [n'],
\]
where $0 \leq i_1 < \dots < i_k < n'$ and $0 \leq j_1 < \dots < j_k < n$.
#+END_lemma
#+BEGIN_proof
# TODO
#+END_proof


#+ATTR_LATEX: :options [Simplicial set]
#+BEGIN_definition
A *simplicial set* is a contravariant functor from $\Delta$ to the
category of sets. The category of *simplicial sets*, $\sSets{}$, is the functor
category $\Sets^{\Delta^{op}}$.
#+END_definition

If $X \in \sSets$ is a simplicial set, we call

 * $X_n$ to the image of $[n]$ under the functor $X$;
 * $d_k$ to the *face maps*, the images of the coface maps under the
   contravariant functor $X$;
 * $s_k$ to the *degeneracy maps*, the images of the codegeneracy maps
   under the contravariant functor $X$.

The following identities follow from the previous identities in the
category and the contravariance of the functor
\begin{aligned}
d_jd_i = d_id_{j+1}, & \quad & \text{for each }\ i \leq j, \\
s_is_j = s_{j+1}s_i, & \quad & \text{for each }\ i \leq j, \\
\end{aligned}
\[
d_is_j =  \left\{
	\begin{array}{ll}
		s_{j-1}d_i  & \mbox{if } i < j, \\
		1 & \mbox{if } i = j,\mbox{ or } j+1, \\
                s_jd_{i-1} & \mbox{otherwise}.
	\end{array} \right.
\]

**** Nerve                                                        :ignore:
#+ATTR_LATEX: :options [Nerve of a category]
#+BEGIN_definition
The *nerve* of a category ${\cal C}$ is a simplicial set $N({\cal C}) \in \sSets$ given
by the functor category $N({\cal C})_n = {\cal C}^{[n]}$, where $[n]$ is the ordinal $(0 < \dots < n)$
interpreted as a category, that is, $N({\cal C})_n$ is the set of paths of
composable morphisms of length $n$.
#+END_definition

# Face map

#+ATTR_LATEX: :options [Nerve functor]
#+BEGIN_lemma
The *nerve functor* $N \colon \Cat \to \sSets$ is fully faithful.
#+END_lemma

**** Standard n-simplex
#+ATTR_LATEX: :options [Standard n-simplex]
#+BEGIN_definition
The *standard n-simplex*, $\Delta^n$, is defined as the nerve of $[n]$,
$\Delta^n = N([n])$. In fact, it can be seen as the covariant functor
\[
\Delta^n = \hom_{\Delta}(-,[n]).
\]
#+END_definition

# TODO check
By Yoneda Lemma, natural transformations between standard n-simplices
correspond to maps between ordinals
\[
\Nat(\hom(-,[n]), \hom(-,[m])) \cong \hom([n],[m]).
\]
Thus, coface and codegeracy maps can be defined between n-simplices.
In particular, the i-th face of $\Delta^n$, written as $\partial_i \Delta^n$ is defined as the image of
$d^i : \Delta^{n-1} \to \Delta^n$.


**** Boundaries                                                   :ignore:
The boundary is the smallest subsimplicial set of $\Delta^n$
containing all of his faces, that is, the union of the faces 
$\partial_0\Delta^n, \dots, \partial_n\Delta^n$.

From the simplicial identities, it follows that
\[\begin{tikzcd}
\Delta^{n-2}\rar{d^{j-1}} \dar[swap]{d^{i}} & \Delta^{n-1} \dar{d^{i}} \\
\Delta^{n-1}\rar[swap]{d^{j}} & \Delta^{n}
\end{tikzcd}\]
and this is a pullback square. This allows us to define the
boundary of a simplicial set as a coequalizer.

#+ATTR_LATEX: :options [Boundary]
#+BEGIN_definition
The *boundary* of the n-simplex, $\partial \Delta^n$ is the subobject of $\Delta^n$ generated
by its $(n-1)\mhyphen\mathrm{simplices}$. Formally, it is the coequalizer given by the
following diagram
\[\begin{tikzcd}
\bigsqcup_{0 \leq i < j \leq n} \Delta^{n-2} \rar[yshift=0.5ex] \rar[yshift=-0.5ex] &
\bigsqcup_{0 \leq i \leq n} \Delta^{n-1} \rar&
\partial \Delta^n.
\end{tikzcd}\]
#+END_definition

**** Horns                                                        :ignore:
The simplicial horn is the union of all the faces of an n-simplex except
for one. Formally, it can be described as a coequalizer, as we did with
the boundary.

#+ATTR_LATEX: :options [Horns]
#+BEGIN_definition
The *n-horn* $\Lambda^n_k \subset \partial \Delta^n$ for $0 \leq k \leq n$ is defined as the coequalizer
\[\begin{tikzcd}
\bigsqcup_{0 \leq i < j \leq n} \Delta^{n-2} \rar[yshift=0.5ex] \rar[yshift=-0.5ex] &
\bigsqcup_{i \neq k} \Delta^{n-1} \rar&
\Lambda^n_k.
\end{tikzcd}\]
#+END_definition

The horns $\Lambda_k^n$ for $0 < k < n$ are called *inner horns* while $\Lambda_0^n$ and $\Lambda_n^n$
are *outer horns*. For instance, in dimension 2, we have two outer horns
$\Lambda^2_0, \Lambda^2_2$ and one inner horn $\Lambda^2_1$ that can be drawn as
\[\begin{tikzcd}[column sep=tiny]
& \cdot &                    & & \cdot \drar &      & & \cdot \ar{dr} &\\
\cdot \urar \ar{rr} && \cdot & \cdot \urar && \cdot & \cdot \ar{rr} && \cdot
\end{tikzcd}\]
with the inner horn being the one in the middle.

Note that the inner horn can be extended by composition to a
simplex. This property of certain horns is what will define Kan
complexes and \infty-categories.

**** Kan complex                                                  :ignore:
#+ATTR_LATEX: :options [Kan complex]
#+BEGIN_definition
A *Kan complex* is a simplicial set $X \in \sSets$ such that any horn $\Lambda^n_k \to X$
can be extended to an n-simplex $\Delta^n \to X$. That is, the following diagram
has a solution, which needs not to be unique.
\[\begin{tikzcd}
\Lambda^n_k \rar \dar[hook] & X \\
\Delta^n \ar[dashed, swap]{ur}{\exists} & \\
\end{tikzcd}\]
#+END_definition



*** (\infty,1)-categories
# https://ncatlab.org/nlab/show/%28n%2Cr%29-category
# https://ncatlab.org/nlab/show/higher+category+theory
# https://arxiv.org/pdf/1007.2925.pdf Short course on infty-categories

There is no consensus on the definition of a *(\infty,1)-category*
yet, but whenever we talk of (\infty,1)-categories, we expect them to
have

 - a collection of objects;

 - morphisms between objects, invertible 2-morphisms between
   morphisms, invertible 3-morphisms between 2-morphisms, invertible
   morphisms between 4-morphisms, and so forth;

 - composition of morphisms, with suitably associative and identity
   laws.

In general, a *(n,k)-category* has morphisms, 2-morphisms, \dots and
n-morphisms, and all morphisms above the k-th dimension are
invertible. This section will follow cite:groth10.
# We follow [Groth] and 

**** \infty-categories :ignore:
#+ATTR_LATEX: :options [\infty-category]
#+BEGIN_definition
An *\infty-category* is a simplicial set ${\cal C} \in \sSets$ such that every
/inner/ horn $\Lambda^n_k \to {\cal C}$ can be extended to an n-simplex $\Delta^n \to {\cal C}$.
#+END_definition

# Spaces and categories give rise to \infty-categories

# Source, target and identities as coface and codegeneracy maps

*** \infty-groupoids
# http://www.cs.nott.ac.uk/~psztxa/publ/weakomega2.pdf
# https://arxiv.org/pdf/1709.09519.pdf
# https://arxiv.org/pdf/1007.2925.pdf Groth, course on infty-categories

#+ATTR_LATEX: :options [Infinity-groupoids]
#+BEGIN_definition
An *\infty-groupoid* is a \infty-category in which the homotopy category
is a groupoid.
#+END_definition

#+ATTR_LATEX: :options [Characterization of groupoids]
#+BEGIN_theorem
An *\infty-category* is an \infty-groupoid if and only if it is a Kan
complex.
#+END_theorem

**** TODO Fundamental \infty-groupoid                             :ignore:
# Grothendieck homotopy hypothesis

*** TODO Higher-order categories and homotopy
# From chapter 2 of HoTT
*** TODO Weak \infty-groupoids

* Categorical logic (abstract)                                       :ignore:
#+LATEX: \ctparttext{\color{black}\begin{center}
This section is based on cite:maclane94.

Topos theory arises independently with Grothendieck and sheaf theory,
Lawvere and the axiomatization of set theory and Paul Cohen with the
forcing techniques with allowed to construct new models of ZFC.
#+LATEX: \end{center}}

* Categorical logic
# [[https://www.youtube.com/watch?v=zUPBEQe4Ti8][Internal Languages for Higher Toposes - Michael Shulman - YouTube]]

** Lawvere theories
This section follows cite:bauer17.

*** Motivation for algebraic theories
We will develop an unified approach to the study of algebraic
structures based on constants, operations and equations; such as
groups, modules or rings.

**** First definition of algebraic theory                         :ignore:
Our *algebraic theories* are usually given by

  * a /signature/, a family of sets $\left\{ \Sigma_k \right\}_{k \in \mathbb{N}}$ whose elements are called
    /k-ary operations/. The /terms/ of a signature are defined inductively,
    being variables or k-ary operations applied to k-tuples of terms;

  * and a set of /axioms/, which are equations between terms.

**** Examples: theory of groups, theory of fields                 :ignore:
For example, the theory of groups is given by a signature conaining

 * a /binary operation/ called $\cdot$,
 * a /unary operation/ written as $^{-1}$, and
 * a /nullary operation/ or a /constant/ called $e$.

Satisfiying the following axioms

 * $(x \cdot y) \cdot z = x \cdot (y \cdot z)$,

 * $x \cdot e = x$,

 * $e \cdot x = x$,

 * $x \cdot x^{-1} = e$, and

 * $x^{-1} \cdot x = e$.

Note how quantifiers are not needed here, as we are interpreting
each $x,y,z$ as free variables and the universal quantification is
therefore implicit.
Theories in which the operations are not defined for every possible
term, cannot be expressed in this way. Fields, in which the inverse of
$0$ is not defined, are not expressable in this form; this fact will
be proved formally in Example [[example-fieldsarenotalgebraic]].

\\

**** Interpretations of algebraic theories                        :ignore:
A theory can be *interpreted* on a suitable category ${\cal C}$ as

  * an object of the category, $A \in {\cal C}$;
  * with morphisms $If \colon A^k \to A$ for every k-ary operation $f$.

Any interpretation of the theory induces an interpretation for every
term on a context. That is, a term $t$ can be given in the
variable context $x_1,\dots,x_n$ if all variables that
appear in $t$ appear in $x_1,\dots,x_n$. We write that as
\[
x_1,x_2,\dots,x_n \mid t;
\]
and the *interpretation of the term* $x_1,\dots,x_n \mid t$ *on that context* is a morphism
$I(x_1,\dots,x_n \mid t) \colon A^n \to A}$ defined inductively knowing that

  * the interpretation of the i-th variable is the i-th
    projection
    \[I(x_1,\dots,x_n \mid x_i) = \pi_i : A^n \to A,\]

  * the interpretation of an operation over a term is the
    interpretation of the morphism componsed with the componentwise
    interpretation of subterms
    \[
    I(f \pair{t_1,\dots,t_k}) =
    If \circ \pair{It_1,\dots,It_k}
    \colon A^n \to A.
    \]
    where we implicitely assume the context to be $x_1,\dots,x_n$.

The interpretation of a particular variable depends therefore on the
context. We say that an interpretation *satisfies* an equation $\Gamma \mid u=v$
in a particular given context if the interpretation of both terms of
the equation is the same on that context, $I(\Gamma \mid u) = I(\Gamma \mid v)$.

We usually would like to find interpretations where all the axioms of the
theory were satisfied. These are called *models of the algebraic theory*.

***** TODO [#C] Example, groups                                  :ignore:
# Maybe this example could be written once we have developed
# the categorical definition.

# For instance, the term $(x \cdot y) \cdot z$ in our previously discussed theory of
# groups would be intepreted in a category with finite products and
# a terminal object $1$ as the morphism
# \[\begin{tikzcd}
# A \cong A \times 1 \rar{\pair{\id,!}} & A \times A \rar & A
# \end{tikzcd}\]
# and the axiom $(x \cdot y) \cdot z = x \cdot (y \cdot z)$ would be represented by the following commutative
# square:
**** Notion of representation-free theories                       :ignore:
The problem with this notion of algebraic theory is that it is not
representation-free; it is not independent of the choice of constants,
operations or axioms. There may be multiple formulations of the same
theory, with different but equivalent axioms. For instance,
cite:mccune91 discusses many single-equation axiomatizations of groups, 
such as
\[
x\ /\
\big(((x/x)/y)/z)\ /\ ((x/x)/x)/z\big)
= y
\]

with the binary operation $/$, related to the usual multiplication as $x / y = x \cdot y^{-1}$.

Our solution to this problem will be to capture all the algebraic
information of a theory -- all operations, constants and axioms --
into a category. Differently presented but equivalent theories will
give rise to the same category. This category will have /contexts/
$[x_1,\dots,x_n]$ as objects. A morphism from $[x_1,\dots,x_n]$ to
$[x_1,\dots,x_m]$ will be a tuple of terms
\[ 
\pair{t_1,\dots,t_k}
\colon [x_1,\dots,x_n] \to [x_1,\dots,x_m]
\]
such that every $t_k$ is given in the context $[x_1,\dots,x_n]$. 
Composition is defined componentwise as substitution of the terms of the 
first morphism into the variables of the second one, that is,
\[
\pair{s_1,\dots,s_n} = \pair{u_1,\dots,u_n} \circ \pair{t_1,\dots,t_m},
\]
where
\[ 
s_i = u_i[t_1,\dots,t_m / x_1,\dots,x_m].
\]

Two morphisms in this category $\pair{t_1,\dots,t_n}$ and $\pair{s_1,\dots,s_n}$ are equal if the axioms of
the theory imply the componentwise equality of its terms, that is, $t_i = s_i$.

This interpretation will lead us to our definition of *algebraic*
*theory* as a category with finite products.

Every model $M$ in the previous sense could be seen as a functor from
this category to a given category ${\cal C}$ preserving finite products. Once the
image of $M[x_1] = A$ is chosen, the functor is determined on objects by
\[
M[x_1,\dots,x_n] = A^{k}
\]
and once it is defined for the basic operations, it is inductively determined
on morphisms as

  * $M\pair{x_i} = \pi_i \colon A^k \to A$, for any morphism $\pair{x_i}$;
  * $M\pair{t_1,\dots,t_m} = \pair{Mt_1,\dots,Mt_m} \colon A^m \to A$, the 
    componentwise interpretation of subterms;
  * $M\pair{f\pair{t_1,\dots,t_m}} = Mf \circ \pair{Mt_1,\dots,Mt_{m}} \colon (M\mathbb{A})^m\to M\mathbb{A}$.

The fact that $M$ is a well-defined functor follows from the
assumption that it is a model.

*** Algebraic theories as categories
**** Algebraic theory                                             :ignore:
#+ATTR_LATEX: :options [Lawvere algebraic theory]
#+BEGIN_definition
An *algebraic theory* is a category $\mathbb{A}$ with finite products and objects
forming a sequence $A^0,A^1,A^2,\dots$ such that $A^m \times A^n = A^{m+n}$ for
any $m,n$.
#+END_definition

From this definition, it follows that $A^0$ must be the terminal object.

**** Models as functors                                           :ignore:
#+ATTR_LATEX: :options [Model]
#+BEGIN_definition
A *model* of an algebraic theory $\mathbb{A}$ in a category ${\cal C}$ is a functor
$M \colon \mathbb{A} \to {\cal C}$ preserving all finite products.
#+END_definition

#+ATTR_LATEX: :options [Category of models of a theory]
#+BEGIN_definition
The *category of models* $\mathtt{Mod}_{{\cal C}}(\mathbb{A})$ is the full subcategory of functor
category ${\cal C}^{\mathbb{A}}$ given by the functors preserving all finite products.
Morphisms between models of a theory in a category are natural transformations.
#+END_definition

#+ATTR_LATEX: :options [Algebraic category]
#+BEGIN_definition
An *algebraic category* is category equivalent to a category of the form $\mathtt{Mod}_{{\cal C}}(\mathbb{A})$,
where $\mathbb{A}$ is an algebraic theory.
#+END_definition

**** TODO Example: groups                                         :ignore:
#+ATTR_LATEX: :options [Theory of groups]
#+BEGIN_exampleth
# Groups as algebraic theory
# Category of groups as category of models in Set
# Abelian groups as models in the category Grp
#+END_exampleth

# Hopf algebras
# https://www.youtube.com/watch?v=p3kkm5dYH-w

**** Example: fields                                              :ignore:
#+ATTR_LATEX: :options [Fields have no algebraic theory]
#+BEGIN_exampleth
<<example-fieldsarenotalgebraic>>
The category $\mathtt{Fields}$ is not an algebraic category.
Any algebraic category $\mathtt{Mod}_{{\cal C}}(\mathbb{A})$ has a terminal object given by the
constant functor $\Delta_1 : \mathbb{A} \to {\cal C}$ to $1$, the terminal object of ${\cal C}$. Note that
${\cal C}$ must have a terminal object for a model to it to exist, as
models must preserve all finite products. We know that $\Delta_1$ is a
terminal object because, in general, it is the terminal object of the
category of functors ${\cal C}^{\mathbb{A}}$.
However, $\mathtt{Fields}$ has no terminal object.
#+END_exampleth

**** TODO More examples
*** Completeness for algebraic theories
When defining interpretation of algebraic theories as categories we
should ensure the property of */semantic completeness/*. We already
know that, if an equation can be proved from the axioms, it will be
valid in all models; but we will also like to prove that, if every
model of the theory satisfies a particular equation, it can actually
be proved from the axioms of the theory.

#+ATTR_LATEX: :options [Completeness for algebraic theories]
#+BEGIN_theorem
Given $\mathbb{A}$ an algebraic theory, there exists a category ${\cal A}$ with
a model $U \in \mathtt{Mod}_{{\cal A}}(\mathbb{A})$ such that, for every terms $u,v$,
\[
U \text{ satisfies } u = v 
\iff
\mathbb{A} \text{ proves } u = v.
\] 
This is called the *universal model* for $\mathbb{A}$. This theorem
asserts that categorical semantics of algebraic theories are complete.
#+END_theorem
#+BEGIN_proof
Simply taking $\mathbb{A}$ with the identity functor, we have an universal
model for $\mathbb{A}$.
#+END_proof

Note that this universal model needs not to be set-theoretic; but,
even in this situation, we can always find a universal model in a
presheaf category via the Yoneda embedding.

#+ATTR_LATEX: :options [Yoneda embedding as a universal model]
#+BEGIN_proposition
The Yoneda embedding $y \colon \mathbb{A} \to \widehat{\mathbb{A}}$ is a universal model for $\mathbb{A}$.
#+END_proposition
#+BEGIN_proof
It preserves finite products because it preserves all limits, hence
it is a model. As it is a faithful functor, we know that any equation
proved in the model is an eqation proved by the theory.
#+END_proof

** Cartesian closed categories
*** Exponential
#+ATTR_LATEX: :options [Exponential]
#+BEGIN_definition
An *exponential* of $A$ and $B$ in a category with binary products is an
object $B^A$ with a morphism $e : B^A \times A \to B$ called /evaluation morphism/,
such that, for any $f : C \times A \to B$ exists a unique $\widetilde{f} : C \to B^A$ such that
the following diagram commutes
\[\begin{tikzcd}
B^A & B^A \times A \drar{e} & \\
C \uar[dashed]{\exists! \widetilde f}  & C \times A \rar[swap]{f}\uar{\widetilde f \times id} & B 
\end{tikzcd}\]
#+END_definition

An object $A$ for which the exponentiation $-^A$ is always defined is called
*exponentiable*.

#+ATTR_LATEX: :options [Exponentials as adjoints]
#+BEGIN_proposition
<<prop-exponentialadjoints>>
An object $A$ in a category with binary products ${\cal C}$ is exponentiable if and only if
the functor $(- \times A) : {\cal C} \to {\cal C}$ has a right adjoint.
#+END_proposition
#+BEGIN_proof
# TODO
# Given by the counit
#+END_proof

*** Cartesian category
#+ATTR_LATEX: :options [Cartesian category]
#+BEGIN_definition
A *cartesian category* is a category with all finite products.
#+END_definition
#+ATTR_LATEX: :options [Cartesian closed category]
#+BEGIN_definition
A *cartesian closed category* is a category with all finite products
and exponentials.
#+END_definition

The definition of cartesian closed category can be written in terms of
existence of adjoints.

#+ATTR_LATEX: :options [Cartesian closed categories and adjoints]
#+BEGIN_proposition
Any category ${\cal C}$ is cartesian closed if and only if there exist
right adjoints for the following functors

  * $! \colon {\cal C} \to 1$, the unique functor to the terminal category;
  * $\Delta \colon {\cal C} \to {\cal C} \times {\cal C}$, the diagonal functor;
  * $(- \times A) \colon {\cal C} \to {\cal C}$, the product functor, for each $A \in {\cal C}$.
#+END_proposition
#+BEGIN_proof
# TODO
We know that the product functor of an object has a right adjoint if and
only if the object is exponentiable, as we saw in Proposition [[prop-exponentialadjoints]].
#+END_proof

#+ATTR_LATEX: :options [Category of small cartesian closed categories]
#+BEGIN_definition
We call $\mathtt{Ccc}$ to the category of small cartesian closed
categories with functors preserving finite products and
exponentials as morphisms. These functors are called
*cartesian closed functors*.
#+END_definition

*** Frames and locales
#+ATTR_LATEX: :options [Completeness for posets]
#+BEGIN_proposition
Complete posets are cocomplete. Cocomplete posets are complete.
#+END_proposition
#+BEGIN_proof
# TODO
#+END_proof

#+ATTR_LATEX: :options [Frames]
#+BEGIN_definition
*Frames* are complete cartesian closed posets.
#+END_definition

# TODO: Equivalently A frame is a poset with distributive law.

#+ATTR_LATEX: :options [Frame morphism]
#+BEGIN_definition
A *frame morphism* is a function between frames preserving finite
infima and arbitrary suprema.
#+END_definition

# TODO: Locales

# TODO: Locales as non-pointed topological spaces, from An informal
# introduction to topos theory

** Heyting algebras
In this section, we develop the notion of a *Heyting algebra* and show
its differences with a Boolean algebra.

There is a correlation between classical propositional calculus and
the Boolean algebra of the subsets of a given set. If we interpret
a proposition $p$ as a subset of a given universal set $P \subset U$ and
fix an element $u \in U$, propositions can be translated to $u \in P$, 
logical connectives can be translated as

\[\begin{tabular}{cc|cc}
logic &  & &  subsets \\
\hline
$P \land Q$ & and & intersection & $P \cap Q$  \\
$P \lor Q$ & or & union  & $P \cup Q$ \\
$\neg P$ & not & complement & $\overline{P}$ \\
$P \to Q$ & implication & complement union & $\overline{P} \cup Q$
\end{tabular}\]

using crucially that $\neg P \land Q \equiv P \to Q$.

In the same way that Boolean algebras correspond to classical
propositional logic, Heyting algebras correspond to intuitionistic
propositional calculus. Its model on a set-like theory is not the subsets
of a given set, but instead, only the /open/ sets of a given
topological space

\[\begin{tabular}{cc|cc}
logic &  & & open sets \\
\hline
$P \land Q$ & and & intersection & $P \cap Q$  \\
$P \lor Q$ & or & union  & $P \cup Q$ \\
$\neg P$ & not & interior of the complement & $\mathrm{int}\left(\overline{P}\right)$ \\
$P \to Q$ & implication & interior of complement and consequent & $\mathrm{int}(\overline{P} \cup Q)$
\end{tabular}\]

where $\mathrm{int}$ is the topological interior of a set.
# TODO: This definition of the implication does not match with the used
# on Moerdijk. It is equivalent but different.

*** Lattices and Boolean algebras
**** Lattices, definition                                         :ignore:
#+attr_latex: :options [Lattice]
#+begin_definition
A *lattice* is a partially ordered set with all binary products and
coproducts. It is a *bounded lattice* if it has all finite products
and coproducts.
#+end_definition

We will usually work with bounded lattices and simply call them
/lattices/. A bounded lattice can be defined then by the following
bidirectional inference rules

\begin{prooftree}
\AXC{$0 \leq x \leq 1$}
\AXC{$z \leq x$}
\AXC{$z \leq y$}
\doubleLine
\BIC{$z \leq x \land y$}
\AXC{$x \leq z$}
\AXC{$y \leq z$}
\doubleLine
\BIC{$x \lor y \leq z$}
\noLine
\TIC{$$}
\end{prooftree}


meaning that it has a terminal and a final object, in order to have
all finte products and coproducts, and all binary products and
coproducts.

#+ATTR_LATEX: :options [Lattice homomorphism]
#+BEGIN_definition
A lattice homomorphism is a function between lattices preserving
finite products and coproducts. That is, a function $f$ such that

  * $f(0) = 0$, $f(1) = 1$,
  * $f(x \wedge y) = f(x) \land f(y)$,
  * $f(x \vee y) = f(x) \vee f(y)$;

and the category of lattices and lattice homomorphisms is denoted
by $\mathtt{Lat}$.
#+END_definition

A bounded lattice can also be defined as a set with $0,1$ and two binary
operations $\land,\lor$ satisfying

  * $1 \land x = x$, and $0 \lor x = x$;
  * $x \land x = x$, and $x \lor x = x$;
  * $x \land (y \lor x) = x = (x \land y) \lor x$;
  * $x \land y = y \land x$ and $x\lor y = y \lor x$.

This perspective allows us also to define a lattice object in any
category as an object $L$ with morphisms

\[
\land \colon L \times L \to L,
\quad
\lor \colon L \times L \to L,
\quad
0,1 \colon \mathrm{I} \to L,
\]

where $\mathrm{I}$ is the terminal object of the category; and commutative
diagrams encoding the previous equations.

**** Distributive lattices                                        :ignore:
#+attr_latex: :options [Distributive lattice]
#+begin_definition
A *distributive lattice* is a lattice where
\[
x \land (y \lor z) = (x \land y) \lor (x \land z),
\]

holds for all $x,y,z$.
#+end_definition
# TODO: This implies the dual distributive law

**** Complements                                                  :ignore:
#+attr_latex: :options [Complement]
#+begin_definition
A *complement* of $a$ in a bounded lattice is an element $\overline{a}$ such that
\[
a \land \overline{a} = 0, \qquad
a \lor \overline{a} = 1.
\]
#+end_definition

#+attr_latex: :options [The complement in distributive lattices is unique]
#+begin_proposition
If a complement of an element exists in a distributive lattice, it is unique.
#+end_proposition
# TODO: Commutativity is needed on this proof, but it is not stated elsewhere
#+begin_proof
Given $a$ with two complements $x,y$, we have that
\[
x = x \land (a \lor y) = (x \land a) \lor (x \land y) =
(y \land a) \lor (x \land y) = y \lor (x \land a) = y.
\]
#+end_proof

**** Boolean algebras                                             :ignore:
#+attr_latex: :options [Boolean algebra]
#+begin_definition
A *Boolean algebra* is a distributive bounded lattice in which every element
has a complement.
#+end_definition

Boolean algebras satisfy certain known properties such as the DeMorgan laws
and the double negation elimination rule.

# TODO: Every Boolean algebra is the algebra of subsets of some set.
# This is a result by M.H.Stone, cited in pag 50 Moerdijk.

*** Heyting algebras
**** Definition of Heyting algebras                               :ignore:
#+attr_latex: :options [Heyting algebra]
#+begin_definition
A *Heyting algebra*, also called *Brouwerian lattice*, is a bounded lattice
which is cartesian closed as a category; that is, for every pair of elements
$x,y$, the exponential $y^x$ exists.
#+end_definition

The exponential in Heyting algebras is usually written as $x \Rightarrow y$ and is 
characterized by its adjunction with the product
\[
z \leq (x \Rightarrow y) \text{ if and only if } z \land x \leq y,
\]
which can be expressed logically as
\begin{prooftree}
\AXC{$z \land x \leq y$}
\UIC{$z \leq x \Rightarrow y$}
\end{prooftree}

#+ATTR_LATEX: :options [Heyting algebra homomorphism]
#+BEGIN_definition
A *Heyting algebra homomorphism* is a lattice homomorphism between
Heyting algebras which does preserve implication. The category of
Heyting algebras is written as $\mathtt{Heyt}$.
#+END_definition

# TODO: Any complete distributive lattice is a Heyting algebra

**** Boolean algebras are Heyting algebras                        :ignore:
#+attr_latex: :options [Boolean algebras are Heyting algebras]
#+begin_proposition
Every Boolean algebra is a Heyting algebra with exponentials given by
\[
(x \Rightarrow y) = \overline{x} \land y.
\]
#+end_proposition
#+begin_proof
We will prove that
\[
z \leq (\overline{x} \lor y) \text{ if and only if } z \land x \leq y.
\]
If $z \leq (\overline{x} \lor y)$,
\[
z \land x \leq
(\overline{x} \lor y) \land x \leq
(\overline{x} \land x) \lor (y \land x) \leq
y \land x \leq
y;
\]
and if $z \land x \leq y$,
\[
z = 
z \land 1 =
z \land (\overline{x} \lor x) =
(z \land \overline{x}) \lor (z \land x) \leq 
(z \land \overline{x}) \lor y \leq
z \lor y.
\]
#+end_proof

**** Negation on Heyting algebras                                 :ignore:
#+attr_latex: :options [Negation]
#+begin_definition
The *negation* of $x$ in a Heyting algebra is defined as
\[
\neg x = (x \Rightarrow 0).
\]
#+end_definition

In general, we only have that $\neg\neg x \leq x$. An element for which
$\neg\neg x = x$ is called a *regular* element.

# TODO: Regular elements form a Boolean algebra.

# TODO: Frames and Heyting algebras. What are the regular elements?

**** Properties of a Heyting algebra                              :ignore:
#+begin_proposition
In any Heyting algebra,

  1) $x \leq \neg \neg x$,
  2) $x \leq y$ implies $\neg y \leq \neg x$,
  3) $\neg x = \neg\neg\neg x$,
  4) $\neg\neg (x \land y) = \neg \neg x \land \neg \neg y$,
  5) $(x \impl x) = 1$,
  6) $x \land (x \impl y) = x \land y$,
  7) $y \land (x \impl y) = y$,
  8) $x \impl (y \land z) = (x \impl y) \land (x \impl z)$.

Any bounded lattice $L$ with an operation satisfying the last four properties
is a Heyting algebra with this operation as implication.
#+end_proposition
#+begin_proof
We can prove the inequalities using the definition of implication.

  1) By definition, $x \land (x \Rightarrow \bot) \leq \bot$.
  2) Again, by definition, $\neg y \land x \leq \neg y \land y \leq \bot$.
  3) Is a consequence of the first two inequalities.
  4) We know that $x \land y \leq x,y$, and therefore $\neg\neg (x\land y) \leq \neg \neg x \land \neg \neg y$.
     We can prove $\neg\neg x \land \neg\neg y \leq \neg\neg (x \land y)$ using the definition of
     negation to get $\neg\neg x \land \neg\neg y \land \neg (x \land y) \leq \bot$, and then by reversing
     the definition of implication $\neg\neg y \land \neg (x \land y) \leq \neg\neg\neg x = x$. Applying
     the same reasoning to $y$, we finally get $x \land y \land \neg (x \land y) \leq \bot$.
  5) Follows from $x \land 1 \leq x$.
  6) Using the evaluation morphism, we know that $x \land (x \impl y) \leq y \leq x \land y$.
  7) Using the definition of implication $y = y \land y \leq y \land (x \Rightarrow y)$.
  8) The exponential $x \Rightarrow -$ is a right adjoint and it preserves products.
#+end_proof

**** Characterization of Boolean algebras                         :ignore:
#+attr_latex: :options [Complements are negations in Heyting algebras]
#+begin_proposition
If an element has a complement on a Heyting algebra, it must be $\neg x$.
#+end_proposition
#+begin_proof
Let $a$ a complement of $x$. By definition, $x \land a = \bot$ and therefore $a \leq \neg x$.
The reverse inequality can be proven using the lattice properties as
\[
\neg x = \neg x \land (x \lor a) = \neg x \land a.
\]
#+end_proof

#+attr_latex: :options [Characterization of Boolean algebras]
#+begin_proposition
A Heyting algebra is Boolean if and only if $\neg\neg x = x$ for every $x$; 
and if and only if $x \lor \neg x = 1$ for every $x$.
#+end_proposition
#+begin_proof
In a Boolean algebra the complement is unique and $\neg\neg x = x$. Now, 
if $\neg\neg y = y$ for every $y$,
\[
x \lor \neg x = \neg\neg (x \lor \neg x) = \neg (\neg x \land \neg\neg x) = \top;
\]
and then, as $x \lor \neg x = \bot$, $\neg x$ must be the complement of $x$. We have
used the fact that $\neg (x \lor y) = (\neg x) \land (\neg y)$ in any Heyting algebra.
#+end_proof

*** Intuitionistic propositional calculus
#+ATTR_LATEX: :options [Existence of free Heyting algebras]
#+BEGIN_proposition
# TODO
#+END_proposition

#+ATTR_LATEX: :options [Intuitionistic propositional calculus]
#+BEGIN_definition
The *Intuitionistic Propositional Calculus* (IPC) is the free Heyting
algebra over an infinite countable set $\{p_0,p_1,p_2,\dots\}$ of elements 
which are usually called /atomic propositions/.
#+END_definition

**** Classical propositional calculus
#+ATTR_LATEX: :options [Classical propositional calculus]
#+BEGIN_definition
The *Classical Propositional Calculus* (CPC) is the free Boolean
algebra over an infinite countable set $\{p_0,p_1,p_2,\dots\}$.
#+END_definition

# TODO: Law of excluded middle and reductio ad absurdum.

*** Quantifiers as adjoints
# Maybe this could be written before, in the adjoints section.
#+begin_definition
Given a relation between sets $S \subseteq X \times Y$, the functors $\forall_p, \exists_p \colon {\cal P}(X \times Y) \to {\cal P}(Y)$
are defined as

  * $\forall_p S = \left\{ y \mid \forall x: \pair{x,y} \in S \right\}$, and
  * $\exists_p S = \left\{ y \mid \exists x: \pair{x,y} \in S \right\}$.
#+end_definition

#+begin_theorem
The functors $\exists_p,\forall_p$ are the left and right adjoints to the inverse image of the
projection functor, $p^{\ast} \colon {\cal P}(Y) \to {\cal P}(X \times Y)$.
#+end_theorem
#+begin_proof
# TODO: Proof
#+end_proof

#+begin_theorem
Given any function on sets $f \colon Z \to Y$, the inverse image functor $f^{\ast} \colon {\cal P}Y \to {\cal P}Z$
has left and right adjoints, called $\exists_f$ and $\forall_f$.
#+end_theorem

** Simply-typed \lambda-theories
*** Simply-typed \lambda-theories
#+ATTR_LATEX: :options [Simply typed lambda theory]
#+BEGIN_definition
A simply-typed \lambda-theory is given by

  * a set of /basic types/.
  * a set of /basic constants/ with their types.
  * a set of /equations/ on those terms, of the form $\Gamma \mid u = t : A$,
    where $\Gamma$ is the variable context.
#+END_definition

# TODO Example: Groups
# TODO Any algebraic theory is a lambda theory
# TODO Theory of a reflexive type

*** Interpretation of \lambda-theories
#+ATTR_LATEX: :options [Interpretation of a lambda theory]
#+BEGIN_definition
An *interpretation* of a \lambda-calculus $\mathbb{T}$ in a cartesian
closed category ${\cal C}$ is given by

  1. an object $\intr{A} \in \mathbb{C}$ for every basic type $A$ in $\mathbb{T}$.
  2. a morphism $\intr{c} \colon 1 \to \intr{A}$ for every constant $c : A$.

The interpretation can be extended to all types using the terminal
object, binary products and exponentials

 * $\intr{1} = 1$;
 * $\intr{A \times B} = \intr{A} \times \intr{B}$;
 * $\intr{A \impl B} = \intr{B}^{\intr{A}}$.

Every context $\Gamma = x_1:A_1, \dots,x_n:A_n$ can be interpreted as the
object $$
#+END_definition

*** Syntactic categories
#+ATTR_LATEX: :options [Syntactic category]
#+BEGIN_definition
The *syntactic category* of a \lambda-theory, ${\cal S}(\mathbb{T})$, is given by

 * the types of the theory as objects; and
 * the terms in context $x : A \mid t :B$

#+END_definition

#+ATTR_LATEX: :options [Syntactic categories are cartesian closed]
#+BEGIN_proposition
The syntactic category of a \lambda-theory is cartesian closed.
#+END_proposition
#+BEGIN_proof
# TODO
#+END_proof


#+ATTR_LATEX: :options [Model of a \lambda-theory]
#+BEGIN_definition
A *model* of a \lambda-theory $\mathbb{T}$ is a functor $M \colon {\cal S}(\mathbb{T}) \to {\cal C}$ preserving
finite products and exponentials.
#+END_definition

# TODO: Canonical interpretation

*** Translations, category of lambda-theories
#+ATTR_LATEX: :options [Translation]
#+BEGIN_definition
A *translation* between \lambda-theories $\tau \colon \mathbb{T} \to \mathbb{U}$ is a model of $\mathbb{T}$
in the syntactic category ${\cal S}(U)$.
#+END_definition

#+ATTR_LATEX: :options [Translation]
#+BEGIN_definition
A *translation* between \lambda-theories $\tau \colon \mathbb{T} \to \mathbb{U}$ is given by

  1. a type $\tau A$ in $\mathbb{U}$ for each type $A$ in $\mathbb{T}$; in such a way that
     \[
     \tau 1 = 1,\qquad
     \tau(A \times B) = \tau A \times \tau B,\qquad
     \tau(A \to B) = \tau A \to \tau B.
     \]

  2. a term $\tau c : \tau A$ in $\mathbb{T}$ for each term $c : A$ in $\mathbb{T}$; in such a way that
     \[\begin{aligned}
     \tau(\texttt{fst}\ t) = \texttt{fst}\ (\tau t), &&
     \tau(\texttt{snd}\ t) = \texttt{snd}\ (\tau t), &&
     \tau\pair{u,v} = \pair{\tau u, \tau v}, \\
     \tau (tu) = (\tau t)(\tau u), &&
     \tau (\lambda x:A. t) = \lambda x:\tau A. \tau t,
     \end{aligned}\]

A translation is also required to preserve all equations. That is, if
$\Gamma \mid t = u : A$ can be proved in $\mathbb{T}$, $\tau \Gamma \mid \tau t = \tau u : \tau A$ should be provable
in $\mathbb{U}$.
#+END_definition


#+ATTR_LATEX: :options [Category of lambda-theories]
#+BEGIN_definition
The category $\lambda\mhyphen\mathtt{Thr}$ has \lambda-theories as objects and translations
between them as morphisms.
#+END_definition


#+ATTR_LATEX: :options [Isomorphism of types]
#+BEGIN_definition
Two types in a \lambda-theory $\mathbb{T}$ are isomorphic if there exist
terms $x:A \mid t:B$ and $y:B \mid u:A$ such that
\[
x:A\mid u[t/y] = x : A, \quad\text{ and }\quad y : B \mid t[u/x] = y : B,
\]
are provable in $\mathbb{T}$.
#+END_definition

#+ATTR_LATEX: :options [Equivalence of theories]
#+BEGIN_definition
Two \lambda-theories $\mathbb{T},\mathbb{U}$ are equivalent if there are a pair of translations
$\tau : \mathbb{T} \to \mathbb{U}$ and $\sigma : \mathbb{U} \to \mathbb{T}$ such that
\[
\sigma(\tau A) \cong A, \quad\text{ and }\quad \tau(\sigma B) = B;
\]
for any given types $A,B$.
#+END_definition

*** Internal language of a category
#+ATTR_LATEX: :options [Internal language]
#+BEGIN_definition
The *internal language* of a small cartesian closed category ${\cal C}$
is a \lambda-theory given by

 1. a /basic type/ $\intl{A }$ for every object $A \in {\cal C}$;

 2. a /constant/ $\intl{f} \colon \intl{A} \to \intl{B}$ for every morphism $f : A \to B$;

 3. the /identity/ axiom 
    \[
    x : \intl{A} \mid \intl{\id}\ x = x : \intl{A}
    \] 
    for every $A$;

 4. the /composition/ axiom
    \[
    x : \intl{A} \mid \intl{g \circ f}\ x = \intl{g}(\intl{f} x) : \intl{C}
    \]
    for every pair of composable morphisms $f : A \to B$ 
    and $g : B \to C$;

 5. and /constants/
    \[\begin{aligned}
    \texttt{T} &: \texttt{1} \to \intl{1} \\
    \texttt{P}_{A,B} &: \intl{A} \times \intl{B} \to \intl{A \times B} \\
    \texttt{E}_{A,B} &: (\intl{A} \to \intl{B}) \to \intl{B^{A}}
    \end{aligned}\]
    for any $A,B \in {\cal C}$.

Satisfying that
\[\begin{aligned}
u : \intl{1}                 &\mid \mathtt{T}\ast = u : \intl{1} \\
z : \intl{A \times B}        &\mid \mathtt{P}_{A,B}\pair{\intl{\pi_0} z, \intl{\pi_1} z} = z : \intl{A \times B} \\
w : \intl{A} \times \intl{B} &\mid \pair{\intl{\pi_0}(\mathtt{P}_{A,B}w), \intl{\pi_1}(\mathtt{P}_{A,B}w)} = w : \intl{A} \times \intl{B} \\
f : \intl{B^A}               &\mid \mathtt{E}_{A,B}(\lambda x^{\scriptsize\intl{A}}. \intl{\mathtt{ev}_{A,B}}\ (\mathtt{P}_{A,B}\pair{f,x})) = f : \intl{B^A} \\
g : \intl{A} \to \intl{B}    &\mid \lambda x^{\scriptsize\intl{A}}.\intl{\mathtt{ev_{A,B}}} (\mathtt{P}_{A,B}\pair{\mathtt{E}_{A,B}g,x}) = g : \intl{A} \to \intl{B} \\
\end{aligned}\]
#+END_definition

These axioms ensure the isomorphism between the terminal, product and
exponential types of the category and the language; that is,

 * $\intl{1} \cong 1$,
 * $\intl{A \times B} \cong \intl{A} \times \intl{B}$,
 * $\intl{B^A} \cong \intl{A} \to \intl{B}$.

**** TODO Equivalence of CC categories
*** TODO Bicartesian closed categories
** TODO First-order logic (?)
** Locally closed cartesian categories
# Clive Newstead - Locally closed cartesian categories and ML-type theory.
# Contar ML aquí y luego contarlo solo desde Agda

In the same way that cartesian closed categories provide a
categorical interpretation of the simply typed \lambda-calculus,
locally closed cartesian categories will provide an interpretation
of Martin-Löf type theories.

*** Locally closed cartesian category
**** The pullback functor                                         :ignore:
#+ATTR_LATEX: :options [The pullback functor]
#+BEGIN_definition
Given a function $f \colon A \to B$ in any category ${\cal C}$ with all pullbacks, the
*pullback functor* $f^{\ast} \colon {\cal C}/B \to {\cal C}/A$ is defined for any object $y \colon Y \to B$
as the object $f^{\ast}y \colon (f^{\ast}Y) \to A$ such that
\[\begin{tikzcd}
(f^{\ast}Y) \rar{} \dar[swap]{f^{\ast}y} & Y \dar{y} \\
A\rar{f} & B
\end{tikzcd}\]
is a pullback square. The functor is defined on any morphism
$\alpha \colon y \to y'$ between two objects $y \colon Y \to B$, $y' \colon Y' \to B$ as
the only morphism making the following diagram commute
\[\begin{tikzcd}
f^{\ast}Y \ar{rr}\ar[dashed]{dr}{\exists! f^{\ast}\alpha} \ar[bend right,swap]{ddr}{f^{\ast}y} &&
Y \dar{\alpha}\ar[bend left=60]{dd}{y} \\
& f^{\ast}Y' \dar[swap]{f^{\ast}y'} \rar & Y' \dar{y'} \\
& A \rar[swap]{f} & B \\
\end{tikzcd}\]
where $x$ and $x'$ form pullback squares with $y$ and $y'$. Note that the pullback
functor is only defined up to isomorphism in objects and well-defined on
morphisms by virtue of the universal property of pullbacks.
#+END_definition

**** Categorical dependent sum                                    :ignore:
#+ATTR_LATEX: :options [Left adjoint of the pullback functor]
#+BEGIN_proposition
Any pullback functor $f^{\ast} \colon A \to B$ has a left adjoint $\Sigma_{f}$.
#+END_proposition
#+BEGIN_proof
# TODO

\[\hom(\Sigma_f x, y) \cong \hom(x, f^{\ast} y)\]

\[\begin{tikzcd}
X \ar[bend right,swap]{ddr}{x} \ar[dashed]{dr}\ar[dashed, bend left]{drr} & & \\
& f^{\ast}Y \rar\dar{f^{\ast}y} & Y \dar{y} \\
& A \rar & B
\end{tikzcd}\]
#+END_proof

**** Categorical dependent product                                :ignore:
#+ATTR_LATEX: :options [Right adjoint of the pullback functor]
#+BEGIN_definition
A category ${\cal C}$ is locally cartesian closed if and only if the pullback
functor $f^{\ast} \colon {\cal C}/B \to {\cal C}/A$ has a right adjoint, called
$\Pi_f \colon {\cal C}/A \to {\cal C}/B$.
#+END_definition

*** TODO Extensional Martin-Löf Type Theory
# http://www.math.mcgill.ca/rags/LCCC/LCCC.pdf
# The Biequivalence of Locally Cartesian Closed Categories and Martin-Löf Type Theories
# http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.646.9598&rep=rep1&type=pdf 

# Extensional theory with universes
**** Dependent sum                                                :ignore:
Introduction rule

\begin{prooftree}
\AXC{$\Gamma \vdash a \colon A$}
\AXC{$\Gamma \vdash b \colon B[a/x]$}
\BIC{$\Gamma \vdash \pair{a,b} \colon \sum_{x:A} B$}
\end{prooftree}

elimination rules

\begin{prooftree}
\AXC{$\Gamma \vdash c \colon \sum_{x:A} B$}
\UIC{$\Gamma \vdash \pi_1(c) \colon A$}
\AXC{$\Gamma \vdash c \colon \sum_{x:A} B$}
\UIC{$\Gamma \vdash \pi_2(c) \colon B[\pi_1(c) / x]$}
\noLine
\BIC{$$}
\end{prooftree}

** Topoi
*** Subobject classifier
#+attr_latex: :options [Subobject classifier]
#+begin_definition
A *subobject classifier* is an object $\Omega$ with a monomorphism $\mathrm{true} \colon 1 \to \Omega$
such that, for every monomorphism $S \to X$, there exists a unique $\phi$ such that
\[\begin{tikzcd}
S\rar{} \dar[swap]{} & 1 \dar{\mathrm{true}} \\
X\rar[dashed]{\phi} & \Omega
\end{tikzcd}\]
forms a pullback square.
#+end_definition

*** Definition of a topos
#+attr_latex: :options [Topos]
#+begin_definition
An *elementary topos* (plural /topoi/) is a cartesian closed category
with all finite limits and a subobject classifier.
#+end_definition

*** Reasoning in a topos
# From the nLab: topos
#
# Any result in ordinary mathematics whose proof is finitist and
# constructive automatically holds in any topos. If you remove the
# restriction that the proof be finitist, then the result holds in any
# topos with a natural numbers object; if you remove the restrictions
# that the proof be constructive, then the result holds in any boolean
# topos. On the other hand, if you add the restriction that the proof be
# predicative in the weaker sense used by constructivists, then the
# result may fail in some toposes but holds in any Π-pretopos; if you
# add the restriction that the proof be predicative in a stronger sense,
# then the result holds in any Heyting pretopos.

*** TODO (\infty,1)-toposes
** TODO Table of correspondences
[[https://ncatlab.org/nlab/show/relation+between+type+theory+and+category+theory][relation between type theory and category theory in nLab]]

|------------------------------------+------------------------------------------------|
| Type theory                        | Category theory                                |
|------------------------------------+------------------------------------------------|
| Simply typed lambda calculus       | Cartesian closed categories                    |
| Extensional dependent type theory  | Locally cartesian closed categories            |
| Martin-Löf intensional type theory | Locally cartesian closed (\infty,1)-categories |
| Homotopy type theory               | Elementary (\infty,1)-topos                    |
|------------------------------------+------------------------------------------------|

* Type theory (abstract)                                             :ignore: 
#+LATEX: \ctparttext{\color{black}\begin{center}
This section is mainly based on cite:hottbook and [Course on Hott, Harper].
#+LATEX: \end{center}}

* Type theory
** Martin-Löf Type Theory
Type theory can be seen as an intuitionistic foundation of mathematics.

*Martin-Löf Type Theory* (MLTT) can be used as a constructive and computational
foundation of mathematics. It is not directly based on first-order predicate
logic, but only interpreted in it via the Curry-Howard isomorphism. It comes
in two flavours, intensional and extensional.

*Intensional type theory* (ITT) is an intuitionistic type theory serving as
the core to many other type theories.

*Extensional type theory* (ETT) extends Intensional Type Theory with
equality of reflection and uniqueness of identity proofs. Types behave
like sets when we interpret them in this setting, and therefore, it
can be seen as an intuitionistic theory of sets.


The principal difference between types and sets is that 
In Type theory, membership of an element to a type, as in $a : A$, is a
/judgement/ of the theory instead of a /proposition/. In practice, this
means that that we can write things like $\forall a. a \in A \to (a,a) \in A \times A$,
as $a \in A$ *is* a proposition; but we cannot write $\prod_{a : A} (a,a) : A \times A$.
A type declaration is not part of the language.

This section follows [Course on Hott, Harper] and [Hott].


We introduce ITT with Naturals, Sigma, Pi, Identity types and Universes.
[Martin-Löf 73]

*** Contexts and judgments
As we did when we described simply typed \lambda-calculus; we will present
a type theory using contexts, substitutions and some inference rules. We
consider three different kinds of judgments for this formal system, namely,

  * $\Gamma\ \mathtt{ctx}$, expressing that $\Gamma$ is a context;
  * $\Gamma \vdash a : A$, expressing that, from the context $\Gamma$, it follows that $a$ is
    of type $A$; and
  * $\Gamma \vdash a \equiv a' : A$, expressing that, from the context $\Gamma$, it follows that
    $a$ is /definitionally equal/ to $a'$. It is important not to confuse this
    notion of equality with the notion of /propositional equality/ we will
    describe later.

*Contexts*, in particular, will be given by a (possibly empty) list of type
declarations
\[
x_1{:}A_1,\ x_2{:}A_2,\ \dots,\ x_n{:}A_n,
\]
and we will consider as valid the judgement saying that the empty context
is a context ($\cdot\ \mathtt{ctx}$), and the judgement saying that, given a context
$x_1{:}A_1,\ x_2{:}A_2,\ \dots,\e x_{n-1}{:}A_{n-1}$ and any type $A_n$, we can take a new
unused variable to form a new context
\[x_1{:}A_1,\ x_2{:}A_2,\ \dots,\e x_{n-1}{:}A_{n-1}, x_n{:}A_n\ \mathtt{ctx}.\]

**** TODO Admissible rules: weakening and substitution            :ignore:

*** Type universes
If we want to blur the difference between types and terms, we will need
type declarations for types. Types whose elements are types are called
/*universes*/. We would like, then, to define a single universe of all
types ${\cal U}$ containing even itself as an element, ${\cal U} : {\cal U}$; but this leads
to paradoxes. In particular, we can encode a particular version of Russell's
paradox.
# In appendix ?, a mechanized proof in Agda of this fact is shown.

To avoid having to deal with paradoxes, we will postulate a
*/hierarchy of cumulative universes/*; that is, we will consider
a list of inclusions between countably many type universes
\[
{\cal U}_0 : {\cal U}_1 : {\cal U}_2 : \dots
\]
and such that $a : {\cal U}_i$ implies $a : {\cal U}_{i+1}$. The relevant inference rules
are
\begin{prooftree}
\AXC{$$}
\UIC{$\Gamma \vdash {\cal U}_i : {\cal U}_{i+1}$}
\AXC{$\Gamma \vdash A : {\cal U}_i$}
\UIC{$\Gamma \vdash A : {\cal U}_{i+1}$}
\noLine
\BIC{$$}
\end{prooftree}
for each $i$ in the natural numbers.

# Type universes a la Russell, they are different in other presentations of ITT

Universe indices, however, are usually implicitly assumed when writing
ITT; as we will mostly work with types from some fixed universe of the
hierarchy ${\cal U}$ and only refer to higher universes when necessary. This
should not lead to confusion: a derivation will be only valid if we
can assign indices to the universes consistently, even if we do not
explicitly write them.

*** Defining types
Types will be defined by the formation and elimination rules we can
apply to them.  We will follow a general pattern for specifying the
typing rules of ITT. They will be classified into

  * *formation rules*, indicating how to create new types of a
    particular kind;
  * *introduction rules*, indicating how to create new terms of
    a particular type;
  * *elimination rules*, indicating how to use elements of a
    particular type;
  * *\beta-reductions*, indicating how elimination acts on terms;
  * *\eta-reductions*, defining some kind uniqueness principle.

Depending in how the uniqueness principle is defined, we classify types
into negative and positive types.

  * Uniqueness of *negative types* states that every element of
    the type can be reconstructed by applying eliminators to it
    and then applying a constructor. Products are an example of
    negative types.

  * Uniqueness of *positive types* states that every funtion from
    the type is determined by some data. Coproducts are an example
    of positive types.

*** Dependent function types
**** Dependent function types: introduction                       :ignore:
*Dependent function types*, or *\Pi-types*, are the generalized version
of the function types in STLC. The elements of $\prod_{x:A} B(x)$ are functions
with the type $A$ as domain and a changing codomain $B(x)$, depending on
the specific element to which the function is applied. This type is often
written also as $\Pi(x:A), B(x)$ to resemble the universal quantifier; under
the /propositions as types/ interpretation, it would correspond to the proof
that a proposition $B$ holds for any $x : A$, that is, $\forall (x : A), B(x)$.

**** Dependent function types: definition                         :ignore:
#+ATTR_LATEX: :options [Dependent function type]
#+BEGIN_definition
The following rules apply for the dependent function type:

  * its formation rule sinthetizes the fact that the codomain
    type can depend on the argument to which the function is applied,
    \begin{prooftree}
    \RightLabel{$\Pi$\textsc{-form}}
    \AXC{$\Gamma \vdash A : {\cal U}_i$}
    \AXC{$\Gamma, x:A \vdash B : U_i$}
    \BIC{$\Gamma \vdash \prod_{x:A}B : {\cal U}_i$}
    \end{prooftree}
    
  * its introduction rule its a generalized version of \lambda-abstraction,
\begin{prooftree}
\RightLabel{$\Pi$\textsc{-intro}}
\AXC{$\Gamma, x:A \vdash b:B$}
\UIC{$\Gamma \vdash \lambda x.b : \prod_{x:A}B$}
\end{prooftree}

  * and its elimination rule generalizes function application,
\begin{prooftree}
\RightLabel{$\Pi$\textsc{-elim}}
\AXC{$\Gamma \vdash f : \prod_{x:A} B$}
\AXC{$\Gamma \vdash a : A$}
\BIC{$\Gamma \vdash f(a) : B[a/x]$}
\end{prooftree}

  * its \beta-rule is similar to STLC's \beta-rule, with the difference that
    it has to specify a substitution on the type of the codomain
    \begin{prooftree}
    \RightLabel{$\Pi$\textsc{-comp}}
    \AXC{$\Gamma, x:A \vdash b:B$}
    \AXC{$\Gamma \vdash a:A$}
    \BIC{$\Gamma \vdash (\lambda x.b)\ a \equiv b[a/x] : B[a/x]$}
    \end{prooftree}

  * and its \eta-rule is also similar to STLC's one
    \begin{prooftree}
    \RightLabel{$\Pi$\textsc{-uniq}}
    \AXC{$\Gamma \vdash f : \prod_{x:A} B$}
    \UIC{$\Gamma \vdash f \equiv \lambda x. f(x) : \prod_{x:A} B$}
    \end{prooftree}
#+END_definition

In the particular case in which $x$ does not appear in $B$ so that $B$ is constant
and does not depend on $A$, we obtain our usual *function type*. In ITT, we
define the function type as a particular case of the dependent function type,
$A \to B :\equiv \prod_{x:A} B$.

\\

**** Polymorphism                                                 :ignore:
*Polymorphic functions* can be defined in terms of dependent function
types and universes. We can see a polymorphic term as a dependent function
taking a type as its first argument. For example, the identity
function $\mathrm{id} : \prod_{A:{\cal U}} A \to A$ can be defined as $\mathrm{id} :\equiv \lambda (A : {\cal U}). \lambda (x : A). x$.
When applied, the type should be passed as an argument, but it is
a common convention to omit arguments when they are obvious from the
context and only refer to them when typechecking.

**** Composition and implicit arguments                           :ignore:
A more complex example of polymorphic function is /function composition/,
which is also an example of higher-order function. Its type signature
takes three types as arguments and two functions between these types,
\[
\circ : \prod_{A : {\cal U}_i} \prod_{B : {\cal U}_i} \prod_{C : {\cal U}_i} (A \to B) \to (B \to C) \to A \to C
\]
and it is defined as
\[
\circ :\equiv \lambda (A : {\cal U}_i). \lambda (B : {\cal U}_i) . \lambda (C : {\cal U}_i).\ 
\lambda g. \lambda f. \lambda x.\ g(f(x)).
\]
However, we face a notational nuisance here: to explicitly write down
all the types each time we want to notate a function would be very
wordy. Instead, we will allow arguments to be determined
implicitely. Implicit arguments must be inferred by the context and
its supression is not formalized as part of our type theory. At the
same time, we will use infix notation whenever it is easier to read.

Thus, we will write $f \circ g$ instead of $\circ (A,B,C,f,g)$. 

# Sometimes, when defining a function, it will be also useful to write
# it not as a lambda expression but as a function with parentheses.

*** Dependent pair types
**** Dependent pair types: introduction                           :ignore:
*Dependent pair types*, or *\Sigma-types*, can be seen as a
generalized version of the product type, but they can be also
particularized in the union type. The elements of $\sum_{x:A}B(x)$ are
pairs where the first element is of type $A$ and the second element
is of type $B(x)$, where $x$ is the first one; that is, the type of
the second component depends on the first component. This type is
often written as $\Sigma (x:A), B(x)$ and it corresponds to the
intuitionistic existential quantifier under the /propositions as types/
interpretation. That is, the proof of $\exists (x:A), B(x)$ must be seen as
a pair given by an element $x$ and a proof of $B(x)$.

**** Dependent pair types: definition                             :ignore:
#+ATTR_LATEX: :options [Dependent pair type]
#+BEGIN_definition
The following rules apply for the dependent pair type:

 * its formation rule is similar to that of the product
   \begin{prooftree}
   \RightLabel{$\Sigma$\textsc{-form}}
   \AXC{$\Gamma \vdash A \colon {\cal U}_i$}
   \AXC{$\Gamma, x:A \vdash B \colon {\cal U}_i$}
   \BIC{$\Gamma \vdash \sum_{x:A} B : {\cal U}_i$}
   \end{prooftree}

 * a pair can be constructed by its two components
   \begin{prooftree}
   \RightLabel{$\Sigma$\textsc{-intro}}
   \AXC{$\Gamma, x:A \vdash B : {\cal U}_i$}
   \AXC{$\Gamma \vdash a : A$}
   \AXC{$\Gamma \vdash b : B[a/x]$}
   \TIC{$\Gamma \vdash (a,b) : \sum_{x:A} B$}
   \end{prooftree}

 * its elimination rule tell us that we can use a pair using its two
   elements, $x,y$, to create a new one
   \begin{prooftree}
   \RightLabel{$\Sigma$\textsc{-elim}}
   \AXC{$\Gamma, z:\sum_{x:A}B \vdash C : {\cal U}_i$}
   \AXC{$\Gamma, x:A, y:B \vdash g : C[(x,y)/z]$}
   \AXC{$\Gamma \vdash p : \sum_{x:A} B$}
   \TIC{$\Gamma \vdash \mathrm{ind}_{\sum} ([z].C, [x].[y].g, p) : C[p/z]$}
   \end{prooftree}

 * and its \beta-rule substitutes the two elements of the pair onto
   an element
   \begin{prooftree}
   \AXC{$\Gamma, z:\sum_{x:A}B \vdash C : {\cal U}_i$}
   \AXC{$\Gamma, x:A,y:B \vdash g : C[(x,y)/z]$}
   \AXC{$\Gamma \vdash a:A$}
   \noLine
   \UIC{$\Gamma \vdash b : B[a/x]$}
   \RightLabel{$\Sigma$\textsc{-comp}}
   \TIC{$\Gamma \vdash \mathrm{ind}_{\sum} ([z].C, [x].[y].g, (a,b)) \equiv g[a/x][b/y] : C[(a,b)/z]$}
   \end{prooftree}
#+END_definition

Note that we do not postulate an \eta-rule for the dependent pair
type. We will show later that it follows from these rules.

\\

**** Projections                                                  :ignore:
The elimination rule says that we can define a function from the
dependent pair type $\prod_{x:A} B$ to an arbitrary type $C$ by
assuming some $x:A$ and $y : B(x)$ and constructing a term of type $C$
with them.

For instance, we can declare of the first projection to be
$\proj_1 :\left(\sum_{x:A} B(x) \right) \to A$ and define it as $\proj_1((a,b)) :\equiv a$.
The second projection is slightly more complex, as it must
be a dependent function
\[
\proj_2 : \prod_{p: \sum_{x:A} B(x)} B(\proj_1(p))
\]
whose type depends on the first projection. It can be again
defined as $\proj_2((a,b)) :\equiv b$.

\\

**** Type-theoretic axiom of choice                               :ignore:
An interesting example arises when we consider a function that builds
a function from a term $R$ that can be regarded as a proof-relevant
binary relation, in that any term of type $R(x,y)$ is a witness of the
relation between some particular $x$ and $y$.
\[
\mathtt{ac} : \left( \prod_{x:A}\sum_{y:B} R(x,y) \right)
\to \left( \sum_{f:A \to B}\prod_{x:A} R(x,f(x)) \right) 
\]
Under the propositions as types interpretation, this type represents
the *axiom of choice*; that is, if for all $x \in A$ there exists a $y \in B$
such that $R(x,y)$, then there exists a function $f : A \to B$ such that
$R(x,f(x))$ for all $x \in A$.

The actual *axiom of choice* can be shown to be independent of
Zermelo-Fraenkel set theory. However, maybe surprisingly, this type
theoretic version can be directly proved from the axioms of our
system. The function $\mathtt{ac}$ can be constructed as
\[
\mathtt{ac} :\equiv \lambda g . \left( \proj_1 \circ g, \proj_2 \circ g \right),
\]
and we can check that, in fact, if $g : \prod_{x:A}\sum_{y:B} R(x,y)$, then

 * $\proj_1 \circ g$ is of type $A \to B$, and
 * $\proj_2 \circ g$ is of type $\prod_{x:A} R(x,\proj_1(g(x)))$;

effectively proving that $\mathtt{ac}(g)$ has type $\sum_{f : A \to B}\prod_{x:A}R(x,f(x))$.

Why the type-theoretic version of the axiom of choice is, instead, a
*theorem of choice*? The crucial notion here is that no choice is
actually involved. The dependent pair $\Sigma$ differs from the classical
existential quantification $\exists$ in that it is constructive, that is,
any witness of $\sum_{a:A}B$ has to actually provide an element of $A$ such
that $B(a)$; while a proof of $\exists a \in A, B$ does not need to point to any
particular element of $A$.

\\

**** Magmas                                                       :ignore:
Algebraic structures can be also described as dependent pairs. For
instance, we can define a *magma* to be a type $A$ endowed with a binary
operation $A \to A \to A$. This can be encoded in a type of magmas as
\[
\mathtt{Magma} :\equiv \sum_{A : {\cal U}} (A \to A \to A).
\]
After we introduce identity types, we will be able to add constraints
to our structures that will allow us to define more complex algebraic
structures such as groups or categories.

*** Unit, empty, coproduct and boolean types
**** Empty type                                                   :ignore:
The *empty* type, $0 \colon {\cal U}$, has no inhabitants and a function $f \colon 0 \to C$
is defined without having to give any equation.

#+ATTR_LATEX: :options [Empty type]
#+BEGIN_definition
The following rules apply for the empty type:

 * it can be formed without any assumption,
   \begin{prooftree}
   \RightLabel{$0$\textsc{-form}}
   \AXC{$$}
   \UIC{$\Gamma \vdash 0 : {\cal U}_i$}
   \end{prooftree}
   
 * and it can be used without any restriction,
   \begin{prooftree}
   \RightLabel{$0$\textsc{-elim}}
   \AXC{$\Gamma, x:0 \vdash C : {\cal U}_i$}
   \AXC{$\Gamma \vdash a : 0$}
   \BIC{$\Gamma \vdash \mathrm{ind}_0([x].C, a) : C[a/x]$}
   \end{prooftree}
#+END_definition

Note that the empty type has no introduction and \beta-rules. We
cannot compute with the empty type and it should not be possible to
create an element of that type.

\\

**** Unit type                                                    :ignore:
The *unit* type, $1 : {\cal U}$ has only one inhabitant. To define a function
$f : 1 \to C$ amounts to choose on element of type $C$.

#+ATTR_LATEX: :options [Unit type]
#+BEGIN_definition
The following rules apply for the unit type:

 * it can be formed without any assumption
   \begin{prooftree}
   \RightLabel{$1$\textsc{-form}}
   \AXC{$$}
   \UIC{$\Gamma \vdash 1 : {\cal U}_i$}
   \end{prooftree}

 * and its only instance can be also introduced without any assumption,
   \begin{prooftree}
   \RightLabel{$1$\textsc{-intro}}
   \AXC{$$}
   \UIC{$\Gamma \vdash \star : 1$}
   \end{prooftree}

 * it can be used to choose an instance of any type
   \begin{prooftree}
   \RightLabel{$1$\textsc{-elim}}
   \AXC{$\Gamma, x:1 \vdash C : {\cal U}_i$}
   \AXC{$\Gamma \vdash c : C[\star / x]$}
   \AXC{$\Gamma \vdash a : 1$}
   \TIC{$\Gamma \vdash \mathrm{ind}_1([x].C,c,a) : C[a/x]$}
   \end{prooftree}

 * and its beta rule computes the element we had chosen
\begin{prooftree}
\RightLabel{$1$\textsc{-comp}}
\AXC{$\Gamma, x:1 \vdash C : {\cal U}_{i}$}
\AXC{$\Gamma \vdash c : C[\star / x]$}
\BIC{$\Gamma \vdash \mathrm{ind}_1([x].C,c,\star) \equiv c : C[\star / x]$}
\end{prooftree}
#+END_definition

# Uniqueness
Uniqueness need not to be postulated, it can be proved using these
axioms.

**** Coproducts: definition                                       :ignore:
#+ATTR_LATEX: :options [Coproduct type]
#+BEGIN_definition
The following rules apply for coproduct types:

 * there is a coproduct type for any two types,
   \begin{prooftree}
   \RightLabel{$+$\textsc{-form}}
   \AXC{$\Gamma \vdash A : {\cal U}_i$}
   \AXC{$\Gamma \vdash B : {\cal U}_i$}
   \BIC{$\Gamma \vdash A + B : {\cal U}_i$}
   \end{prooftree}

 * a term can be introduced in two different ways
   \begin{prooftree}
   \RightLabel{$+$\textsc{-intro1}}
   \AXC{$\Gamma \vdash A : {\cal U}_i$}
   \AXC{$\Gamma \vdash B : {\cal U}_i$}
   \AXC{$\Gamma \vdash a : A$}
   \TIC{$\Gamma \vdash \mathtt{inl}(a) : A + B$}
   \end{prooftree}
   
   \begin{prooftree}
   \RightLabel{$+$\textsc{-intro2}}
   \AXC{$\Gamma \vdash A : {\cal U}_i$}
   \AXC{$\Gamma \vdash B : {\cal U}_i$}
   \AXC{$\Gamma \vdash b : B$}
   \TIC{$\Gamma \vdash \mathtt{inr}(b) : A + B$}
   \end{prooftree}

 * and its elimination rule has to consider both cases
   \begin{prooftree}
   \AXC{$\Gamma, z {:} (A+B) \vdash C : {\cal U}_i$}
   \AXC{$\Gamma, x {:} A \vdash c : C[ \mathtt{inl}(x)/z ]$}
   \noLine
   \UIC{$\Gamma, y {:} B \vdash d : C[ \mathtt{inr}(y)/z ]$}
   \AXC{$\Gamma \vdash e : A + B$}
   \RightLabel{$+$\textsc{-elim}}
   \TIC{$\Gamma \vdash \mathtt{ind}_{A+B}([z].C, [x].c, [y].d, e) : C[e/z]$}
   \end{prooftree}

 * in particular, we have two \beta-rules, each one for each different
   way in which a term can be introduced
   \begin{prooftree}
   \AXC{$\Gamma, z {:} (A+B) \vdash C : {\cal U}_i$}
   \AXC{$\Gamma, x {:} A \vdash c : C[ \mathtt{inl}(x)/z ]$}
   \noLine
   \UIC{$\Gamma, y {:} B \vdash d : C[ \mathtt{inr}(y)/z ]$}
   \AXC{$\Gamma \vdash a : A$}
   \RightLabel{$+$\textsc{-comp1}}
   \TIC{$\Gamma \vdash \mathtt{ind}_{A+B}([z].C, [x].c, [y].d, \mathtt{inl}(a)) \equiv c[a/x] : C[ \mathtt{inl}(a) /z]$}
   \end{prooftree}

   \begin{prooftree}
   \AXC{$\Gamma, z {:} (A+B) \vdash C : {\cal U}_i$}
   \AXC{$\Gamma, x {:} A \vdash c : C[ \mathtt{inl}(x)/z ]$}
   \noLine
   \UIC{$\Gamma, y {:} B \vdash d : C[ \mathtt{inr}(y)/z ]$}
   \AXC{$\Gamma \vdash b : B$}
   \RightLabel{$+$\textsc{-comp2}}
   \TIC{$\Gamma \vdash \mathtt{ind}_{A+B}([z].C, [x].c, [y].d, \mathtt{inr}(b)) \equiv d[b/x] : C[ \mathtt{inr}(b) /z]$}
   \end{prooftree}
#+END_definition

**** Booleans                                                     :ignore:
# Define booleans as 1+1

*** Natural numbers
**** Natural numbers: introduction                                :ignore:
Although we will see later that it can be seen as a particular case of
inductive datatypes, a *natural numbers* type can be directly
defined in the core of our type theory. Its introduction rules will match
Peano's axioms and its elimination and \beta-rules will provide us with
the notion of induction.

**** Natural numbers: definition                                  :ignore:
#+ATTR_LATEX: :options [Natural numbers]
#+BEGIN_definition
The following rules apply for natural numbers:

 * the natural numbers type can be formed without any assumption,
   \begin{prooftree}
   \RightLabel{$\mathbb{N}$\textsc{-form}}
   \AXC{$$}
   \UIC{$\Gamma \vdash \mathbb{N} : {\cal U}_i$}
   \end{prooftree}

 * a natural number can be introduced in two ways, as zero or as the
   successor of a natural number
   \begin{prooftree}
   \RightLabel{$\mathbb{N}$\textsc{-intro1}}
   \AXC{$$}
   \UIC{$\Gamma \vdash 0 : \mathbb{N}$}
   \AXC{$\Gamma \vdash n : \mathbb{N}$}
   \RightLabel{$\mathbb{N}$\textsc{-intro2}}
   \UIC{$\Gamma \vdash  \mathtt{succ}(n) : \mathbb{N}$}
   \noLine
   \BIC{$$}
   \end{prooftree}

 * we can apply induction on natural numbers
   \begin{prooftree}
   \AXC{$\Gamma, x{:} \mathbb{N} \vdash C : {\cal U}_i$}
   \AXC{$\Gamma \vdash c_0 : C[0/x]$}
   \noLine
   \UIC{$\Gamma, x{:} \mathbb{N}, y {:} C \vdash c_s : C[ \mathtt{succ}(x)/x]$}
   \AXC{$\Gamma \vdash n : \mathbb{N}$}
   \RightLabel{$\mathbb{N}$\textsc{-elim}}
   \TIC{$\Gamma \vdash \mathtt{ind}_{\mathbb{N}}([x].C, c_0, [x].[y].c_s, n) : C[n/x]$}
   \end{prooftree}

 * and it must be interpreted recursively for zero or a successor with the
   following two \beta-rules
   \begin{prooftree}
   \AXC{$\Gamma, x{:} \mathbb{N} \vdash C : {\cal U}_i$}
   \AXC{$\Gamma \vdash c_0 : C[0/x]$}
   \noLine
   \UIC{$\Gamma, x{:} \mathbb{N}, y {:} C \vdash c_s : C[ \mathtt{succ}(x)/x]$}
   \RightLabel{$\mathbb{N}$\textsc{-comp1}}
   \BIC{$\Gamma \vdash \mathtt{ind}_{\mathbb{N}}([x].C, c_0, [x].[y].c_s, 0) \equiv c_0 : C[0/x]$}
   \end{prooftree}

   \begin{prooftree}
   \AXC{$\Gamma, x{:} \mathbb{N} \vdash C : {\cal U}_i$}
   \AXC{$\Gamma \vdash c_0 : C[0/x]$}
   \noLine
   \UIC{$\Gamma, x{:} \mathbb{N}, y {:} C \vdash c_s : C[ \mathtt{succ}(x)/x]$}
   \RightLabel{$\mathbb{N}$\textsc{-comp2}}
   \BIC{$\Gamma \vdash \mathtt{ind}_{\mathbb{N}}([x].C, c_0, [x].[y].c_s, \mathtt{succ}\ n) \equiv$}
   \noLine
   \UIC{$c_s[n/x][\mathrm{ind}_{\mathbb{N}}([x].C, c_0, [x].[y].c_s, n)/y] : C[\mathtt{succ}(n)/x]$}
   \end{prooftree}
#+END_definition

**** Pattern matching                                             :ignore:
Any function on natural numbers can be defined in two
equivalent ways, either using the induction principle or its defining
equations.

 * If we use the induction principle, a function from the naturals
   to the type $C$ must be defined as
   \[
   f :\equiv \mathtt{ind}_{\mathbb{N}}(C, c_0, c_s)
   \]
   where $c_s$ can depend on an element of type $C$. Two defining equations
   capturing the definition could be written in this case as
   \[\begin{aligned}
   f(0) :\equiv c_0, \\
   f( \mathtt{succ}(n)) :\equiv c_s,
   \end{aligned}\]
   where $c_s$ would depend on an element of type $C$ that can be interpreted
   as being $f(n)$, that is, a recursive call to the function.

 * Conversely, we could define a function by its two defining
   equations. If we had
   \[\begin{aligned}
   f(0) :\equiv \Phi_0, \\
   f( \mathtt{succ}(n)) :\equiv \Phi_s,
   \end{aligned}\]   
   where $\Phi_s : C$ depends on $f(n)$, we could substitute $f(n)$ by a variable
   $r$ and equivalently write the definition as
   \[
   f :\equiv \mathtt{ind}_{\mathbb{N}}(C,\Phi_0, \lambda n.\lambda r. \Phi_{s}).
   \]

This can be done in general with types whose induction principle can be
split in multiple cases. We will call *pattern matching* to this style
of defining functions and it will be used from now on because of its
simplicity. However, we must be careful when doing this, as it could
create the false impression that arbitrary recursion is allowed in our
definitions. An equation such as $f(\mathtt{succ}(n)) :\equiv f(\mathtt{succ}(\mathtt{succ}(n)))$,
for example, is not valid.

\\

**** Addition                                                     :ignore:
Our first example will be the definition of addition on natural
numbers. It will be an infix function $+ : \mathbb{N} \to \mathbb{N} \to \mathbb{N}$ given by
\[\begin{aligned}
0 + m &:\equiv m, \\
(\mathtt{succ}\ n) + m &:\equiv \mathtt{succ}(n + m),
\end{aligned}\]
where we are applying the induction principle to the first argument.
In other words,
\[
{+} :\equiv \lambda (n{:}\mathbb{N}). \lambda (m{:}\mathbb{N}). \mathtt{ind}_{\mathbb{N}}(\mathbb{N}, m, [s].[r]. \mathtt{succ}\ r, n).
\]

\\

**** Extensional equality of 2+2=4                                :ignore:
Under this definition it can be shown that, for example, $2 + 2 :\equiv 4$.
However, if we wanted to prove the fact that addition is
commutative, we would have no way of expressing this in terms of an
extensional equality. We would need a notion of propositional
equality, that is, an equality $=$ that could be used as a type in an
expression, in order to write something like

\[
\mathtt{comm_{+}} : \prod_{n : \mathbb{N}} \prod_{m : \mathbb{N}} (n + m = m + n).
\]

*** Identity types
**** Identity types: introduction                                 :ignore:
Under a /propositions as types/ interpretation, the proposition that
two elements of a type are equal, should correspond to a type; and
therefore, equality should correspond to a family of types,
\[
{=} : \prod_{A : \cal U} A \to A \to {\cal U}.
\]
We will write the equality type between $a,b : A$ interchangeably as
$a =_A b$ or $\mathrm{Id}_A(a,b)$.

**** Identity types: definition                                   :ignore:
#+ATTR_LATEX: :options [Identity types]
#+BEGIN_definition
The following rules apply for identity types:

 * an identity type is defined between any two elements of
   the same type
   \begin{prooftree}
   \RightLabel{$=$\textsc{-form}}
   \AXC{$\Gamma \vdash A : {\cal U}_i$}
   \AXC{$\Gamma \vdash a : A$}
   \AXC{$\Gamma \vdash b : A$}
   \TIC{$\Gamma \vdash a =_A b : {\cal U}_i$}
   \end{prooftree}

 * the only way to introduce an identity is using the principle
   of *reflexivity*, that is, there exists an element $\mathtt{refl}$ of
   type $a =_A a$ for each $a : A$,
   \begin{prooftree}
   \RightLabel{$=$\textsc{-intro}}
   \AXC{$\Gamma \vdash A : {\cal U}_i$}
   \AXC{$\Gamma \vdash a : A$}
   \BIC{$\Gamma \vdash \mathrm{refl} : a =_A a$}
   \end{prooftree}

 * its elimination rule of $p : x =_A y$ allows us to create an instance of a
   particular type, or prove a proposition, simply assuming that $x$ and $y$
   are in fact the same element and $p = \mathrm{refl}_{z}$,
   \begin{prooftree}
   \AXC{$\Gamma, x{:}A, y{:}A, p{:} (x=_A y) \vdash C : {\cal U}_i$}
   \noLine
   \UIC{$\Gamma, z{:}A \vdash c : C[z/x][z/y][ \mathrm{refl} / p ]$}
   \AXC{$\Gamma \vdash a : A$}
   \noLine
   \UIC{$\Gamma \vdash b : A$}
   \AXC{$\Gamma \vdash q : a =_A b$}
   \RightLabel{$=$\textsc{-elim}}
   \TIC{$\Gamma \vdash \mathtt{ind}_{=_A}([x].[y].[p].C, [z].c, a,b,q) : C[a,b,q / x,y,p]$}
   \end{prooftree}

 * and its \beta-rule simply substitutes two equal instances by the same
   one
   \begin{prooftree}
   \AXC{$\Gamma, x {:} A, y {:} A, p {:} x=_A y \vdash C : {\cal U}_i$}
   \noLine
   \UIC{$\Gamma, z {:} A \vdash c : C[z,z, \mathtt{refl}_z / x,y,p]$}
   \AXC{$\Gamma \vdash a : A$}
   \RightLabel{$=$\textsc{-comp}}
   \BIC{$\Gamma \vdash \mathtt{ind}_{=_A}([x].[y].[p].C, [z].c, a, a, \mathtt{refl}_a) \equiv c[a/z] : C[a/x][a/y][\mathtt{refl}_a/p]$}
   \end{prooftree}
#+END_definition

**** Reflexivity                                                  :ignore:
Note that the introduction rule on types is a dependent function
called *reflexivity* that provides us with a basic way of constructing
an element of type $(a =_A b) : {\cal U}$,
\[
\mathtt{refl} :
\prod_{a : A} (a =_A a).
\]
In particular, any two definitionally equal terms, such as $2 + 2 :\equiv 4$,
are also propositionally equal. In fact, $\mathtt{refl}_4 : 2 + 2 = 4$ is a well-typed
statement, precisely because of the fact that $2 + 2$ and $4$ are definitionally
equal.

\\

**** Path induction                                               :ignore:
The elimination principle and the \beta-rule for identity types are
usually called the *path induction* principle. It states that, given a
family of types parametrized over the identity type,
\[
C : \prod_{x,y:A} (x =_A y) \to {\cal U},
\]
every function defined over the $\mathtt{refl}$ element, $c : \prod_{z:A} C(z,z, \mathtt{refl}_z)$,
can be extended to any equality term as
\[
f : \prod_{x:A}\prod_{y:A}\prod_{p : x=y} C(x,y,p),
\]
and it follows from the \beta-rule that $f(x,x, \mathtt{refl}_x) :\equiv c(x)$, that is,
that it is in fact an extension.

This *path induction* principle could be wrapped into a function
\[
\mathtt{ind}_{=} :
\prod_{\left(C : \prod_{x:A}\prod_{y:A} (x = y) \to {\cal U}\right)}
\left( \prod_{x:A} C(x,x, \mathtt{refl_x}) \to \prod_{x:A}\prod_{y:A}\prod_{p : x=y} C(x,y,p) \right)
\]
and the \beta-rule could be replaced by $\mathtt{ind}_{=}(C,c,x,x, \mathtt{refl}_x) :\equiv c(x)$.

**** Indiscernability of identicals follows from path induction   :ignore:
We will use path induction to prove a the principle of
*indiscernability of identicals*, that is, the fact that,
for every family of types $C : A \to {\cal U}$, there is a function
between the type associated to propositionally equal elements,
that is,
\[
f : \prod_{x:A} \prod_{y:A} \prod_{p : x=y} C(x) \to C(y),
\]
such that, when applied to reflexivity, it outputs the identity
function, $f(x,x, \mathtt{refl}_x) :\equiv \mathtt{id}_{C(x)}$.

We can prove this principle for any $C : A \to {\cal U}$ by defining
\[
f :\equiv
\mathtt{ind}_{=}
\left(\lambda x. \lambda y. \lambda p. C(x) \to C(y), \mathtt{id}_{C(x)}\right),
\]
and the fact that $f(x,x, \mathtt{refl}_x) :\equiv \mathtt{id}_{C(x)}$ follows from the \beta-rule.

# We will later prove that this function is an equivalence

*** Inductive types
Inductive types provide the intuitive notion of a free type generated
by a finite set of constructors. That is, the only elements of an
inductive type should be those that can be obtained by applying a
fixed finite set of constructors. Thus, we can define functions over
inductive types using certain induction principles that define a
specific action for each one of the possible constructors.

# Example: naturals

**** W-types                                                      :ignore:
# https://mazzo.li/epilogue/index.html%3Fp=324.html

# Following notes by Muller and Cavallo
# http://www.cs.cmu.edu/~rwh/courses/hott/notes/notes_week10.pdf

The *W-type* (or /Brower ordinal/) $\wtype_{a:A} B(a)$, also written as $\wt (a:A),B(a)$,
can be thought as an inductive type whose set of constructors is
indexed by the type $A$, such that the constructor indexed by the
particular element $a$ has arguments indexed by $B(a)$. In other
words, it is a well-founded tree where every node is labeled by
an element of $a$ and has a set of child nodes indexed by the type
$B(a)$.

**** Rules for W-types                                            :ignore:
\begin{prooftree}
\RightLabel{$\wt$\textsc{-form}}
\AXC{$\Gamma \vdash A : {\cal U}$}
\AXC{$\Gamma, x:A \vdash B : {\cal U}$}
\BIC{$\Gamma \vdash \wtype_{x:A} B : {\cal U}$}
\end{prooftree}

\begin{prooftree}
\RightLabel{$\wt$\textsc{-intro}}
\AXC{$\Gamma \vdash a : A$}
\AXC{$\Gamma, x{:} B(a) \vdash \wtype_{x:A} B$}
\BIC{$\Gamma \vdash \mathtt{sup}[a]([x].w) : \wtype_{x:A} B$}
\end{prooftree}

\begin{prooftree}
\AXC{$\Gamma, z : \wtype_{x:A} B \vdash P : {\cal U} $}
\noLine
\UIC{$\Gamma, a{:}A, p: B(a) \to \wtype_{x:A} B, h : \prod_{b : B(a)} P(p(b)) \vdash M : P( \mathtt{sup}(p) )$}
\RightLabel{$\wt$\textsc{-elim}}
\UIC{$\Gamma, z{:} \wtype_{x:A} B \vdash \mathtt{ind}_{\wt}(a,p,[h].M) : P(z)$}
\end{prooftree}

# Computation rule only holds propositionally.

**** W-types as initial algebras                                  :ignore:
The relevant endofunctor must be polynomial.

Not all endofunctors have initial algebras, but all polynomial functors do have
initial algebras.


Each $\wt\mhyphen\mathrm{type}$ determines a functor on types given by
\[
F(X) = \sum_{a:A} B(a) \to X.
\]
Functors of this form are called *polynomial functors*, as they can be
written as $\sum_{a:A} X^{B(a)}$ and thought as a generalized polynomial function.

In fact, this functor is an F-algebra with the function
\[
\lambda (a,w). \mathrm{sup}[a](w) : F(X) \to X.
\]
# It is an equivalence and this is an homotopy-initial algebra.

**** TODO Two equivalently defined inductive functions are in fact equal
** Programming in Martin-Löf Type theory
# Cite Connor's course in Agda
# Cite Altenkirch's notes on HoTT

Martin-Löf type theory can also be regarded as a programming language in
which is possible to formally specify and prove theorems about the
code itself. It is similar to the Calculus of Constructions that we 
mentioned earlier.

Formal proofs in MLTT can be written in this programming language and
checked by a machine. Some programming languages offering this complex
type system are

 * *Agda* implements a variant of MLTT and can be used as a programming
   language or a proof assistant;

 * *Coq* cite:coq04 was developed in 1984 in INRIA; it implements Calculus of
   Constructions and was used, for example, to check a proof of the
   Four Color Theorem;
   # http://www.ams.org/notices/200811/tx081101382p.pdf
   # https://coq.inria.fr/about-coq

 * *Idris* implements a different version of identity types than that in
   MLTT and aims to be a useful programming language instead of a proof
   assistant;

 * *NuPRL* cite:nuprl86 implements Extensional Type Theory;

 * *Cubical* cite:cohen16 provides an implementation of Cubical Type
   Theory;

 * *RedPRL* is still in beta and provides an implementation of Cubical
   Type Theory in the NuPRL style.

In this text, we will use Agda as our proof assistant and we will mechanize
proofs in Martin-Löf type theory using it. We will introduce Agda using
ideas from cite:Conor17.

*** Martin-Löf type theory in Agda
In this chapter, we will describe how to program in MLTT using Agda as
proof assistant. An Agda file consists of type declarations and
definitions stated with ~=~, as in the following example.

#+latex: \ExecuteMetaData[latex/Ctlc.tex]{example}

This code exemplifies multiple idiosyncratic features of Agda when compared
to our previous presentation of MLTT.

 * The type of types, ${\cal U}$, is written as =Set=; however, it has nothing
   to do with our usual /set-theoretical sets/. The hierarchy of universes
   in Agda is written as
   \[
   \mathtt{Set} :
   \mathtt{Set}_1 :
   \mathtt{Set}_{2} :
   \dots
   \]
   We will see later that it is possible to use compiler flags to
   postulate the collapse of this hierarchy into a single
   $\mathtt{Set} : \mathtt{Set}$ and derive a contradiction from this
   assumption.

   If we want a definition to work at all levels of the universe
   hierarchy, we need to explicitly write =∀{l}...Set l= instead
   of simply write =Set=.

 * Dependent function are implicitly built into the language.  In
   general, the dependent function type $\prod_{a:A} B$ is written as
   ~(a : A) → B~. In particular, we write the type
   $\mathtt{id} : \prod_{A : {\cal U}} A \to A$ as =(A : Set) → A → A=.

 * Agda allows a shorthand for nested dependent function types of
   the form $\prod_{a:A} \prod_{a':A} B$. We can write them as =(a a' : A) → B=,
   instead of having to write =(a : A) → (a' : A) → B=.

 * Implicit arguments are declared between curly braces, ={ ... }=.
   Whenever we call a function with implicit arguments, the type
   checker will infer them from the context if we decide not to
   overwrite them explicitely.

 * Infix operators can be defined using underscores ~_~.

**** Data and records                                             :ignore:
We have seen that Agda directly provides function types, but
it also provides two mechanisms to define new types:
/*data declarations*/ and /*record types*/.

 * *Data declarations* can be thought as inductive types. They build the
   free type (the initial algebra) over a set of constructors; and
   they are better suited for defining positive types such as the void
   type, coproducts or natural numbers.

 * *Record types* can be thought as generalized dependent n-tuples in
   which every element of the tuple is labeled and depends on the
   previous ones. They are better suited for defining negative
   types such as the unit type or product types. A constructor can
   be provided for them.

The complete type constructors of MLTT and the majority of types that
we will use in this text can be defined in terms of these.

**** Code: types and proofs                                       :ignore:
#+latex: \ExecuteMetaData[latex/Ctlc.tex]{mltt}

Once we have defined these types, we can start profiting from the
propositions as types interpretation to write and check mathematical
proofs in Agda.

Functions can be defined by a (possibly empty) list of declarations
that exhaustively pattern match on all the possible constructors of
the type.

#+latex: \ExecuteMetaData[latex/Ctlc.tex]{mlttproof}

In particular, we can check our previous proof of the Theorem of
Choice defining the two projections from a dependent pair type.

#+latex: \ExecuteMetaData[latex/Ctlc.tex]{theoremchoice}

The path induction principle follows from the definition of equality
as an inductive type and can be used to prove indiscernability of
identicals.

#+latex: \ExecuteMetaData[latex/Ctlc.tex]{indiscernability}

*** Type in type
# Russell's paradox code
# Girard paradox in agda: https://github.com/nzl-nott/PhD-of-nzl/blob/master/Exercise/Ex4/girard.agda
# Hurkens' paradox: https://github.com/agda/agda/blob/master/test/Succeed/Hurkens.agda
# Hurkens: https://www.cs.cmu.edu/~kw/scans/hurkens95tlca.pdf
# Burali-Forti paradox

#+latex: \ExecuteMetaData[latex/Russell.tex]{russell}

** Homotopy Type Theory I: Univalence
# HoTT is the internal language of (infty,1)-toposes

# TODO: Check the unicode error output!

The basic idea of *Homotopy Type Theory* is to think of types as
\infty-groupoids. Between two elements of a type $a,b:A$ we can define their
identity type $a =_A b$; between two proofs of equality $p,q : a =_A b$ we
can define again a new identity type $p =_{a=b} q$; between any two proofs
of equality between equalities, we could define again a new identity type,
and so on. All of this \infty-groupoid structure arises from the path
induction principle.

Homotopy Type Theory (HoTT) adds the univalence axiom and higher
inductive types to intensional type theory (ITT).

This presentation of Homotopy Type Theory will follow cite:hottbook.

*** Types as \infty-groupoids
**** Types as groupoids                                           :ignore:
Types and identity types between their elements are groupoids.
# Higher Identity types give them a structure of higher groupoids.

#+latex: \ExecuteMetaData[latex/Ctlc.tex]{groupoids}

**** Functions as functors                                        :ignore:
Functions can be seen as functors between groupoids.

#+latex: \ExecuteMetaData[latex/Ctlc.tex]{groupoid-functors}

*** Univalence axiom
*** Naturals and Naturals'
*** Transport
#+begin_quote
Finding a type-theoretic description of
this behavior (that is, introduction, elimination and computation rules which comport
with Gentzen’s Inversion Principle) is an open problem.
#+end_quote
*** Higher inductive types
# as free \infty-groupoids

** Homotopy Type Theory II: Mathematics
*** TODO Categories
*** TODO Homotopy theory
\begin{tikzcd}[column sep=large]
  \tikz\draw[{[-]},yshift=0.5ex] (0,0)node[below]{$0$} -- (2,0)node[below]{$1$};
  \arrow[r, "\textrm{gluing}"]
& \tikz\draw (0,0) circle (1cm)  (0.8,0) -- (1.2,0) node[right]{$\mathbb Z$};
\end{tikzcd}
** TODO Constructive mathematics
*** TODO Axiom of choice implies excluded middle
# In Agda or Coq!?

*** TODO Type theory as a foundation of mathematics

** TODO Cubical type theory
# A hands-on introduction to cubical type theory
* Conclusions
* Appendices
bibliographystyle:alpha
bibliography:Bibliography.bib

** TODO A note on structural induction
http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.184.4173
** TODO Mikrokosmos complete code
** TODO Mikrokosmos user's guide
** TODO Quotes
# These quotes can be placed on the partial abstracts or in the general one.
